[["index.html", "STAT 253: Statistical Machine Learning Welcome!", " STAT 253: Statistical Machine Learning Welcome! Image source This is the class manual for Statistical Machine Learning (STAT 253) at Macalester College for Spring 2023, taught by Professor James Normington. The content largely draws upon our class textbook, An Introduction to Statistical Learning with Applications in R. In addition, much of this site’s content was created by Professor Alicia Johnson, Professor Brianna Heggeseth, and Professor Leslie Myint. This bookdown website was constructed by Professor Brianna Heggeseth. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["schedule-syllabus.html", "Schedule &amp; Syllabus", " Schedule &amp; Syllabus The schedule below is a tentative outline of our plans for the module. Here is the syllabus, current as of Jan. 23, 2023. Before each class period, please watch the indicated videos and check on your understanding by actively reviewing the associated Learning Objectives. The readings listed below are optional but serve as a nice complement to the videos and class activities. Readings refer to chapters/sections in the Introduction to Statistical Learning (ISLR) textbook (available online here). Week 1: 1/19 - 1/20 Day(s) Topic Readings 1/19 Introductions ISLR: Chap 1, Chap 2 - Section 2.1 (Skip 2.1.2, 2.1.3 for now.) Week 2: 1/23 - 1/27 Day(s) Topic Videos/Readings Slides 1/24 Evaluating Regression Models Evaluating Regression Models R: Introduction to TidyModels ISLR: 2.2 PDF 1/26 Overfitting Overfitting R: Pre-processing and recipes PDF Homework 1 due Sunday, Feb 5th at 11:59pm CST Week 3: 1/30 - 2/3 Day(s) Topic Videos/Readings Slides 1/31 Cross-validation Cross-validation R: Training, testing, and cross-validation ISLR: 5.1 PDF 2/2 Subset Selection Variable Subset Selection R: Subset Selection ISLR: 6.1 PDF Week 4: 2/6 - 2/10 Day(s) Topic Videos/Readings Slides 2/7 LASSO (Shrinkage/Regularization) LASSO (Shrinkage/Regularization) ISLR: 6.2 PDF 2/9 Quiz 1 Sec 01 (9:40a) will cover Subset Selection after quiz Sec 02 (1:20p) may leave after quiz Quiz 1 study guide and details "],["learning-objectives.html", "Learning Objectives", " Learning Objectives Learning objectives for our course topics are listed below. Use these to guide your synthesis of video and reading material. Introduction to Statistical Machine Learning Formulate research questions that align with regression, classification, or unsupervised learning tasks Evaluating Regression Models Create and interpret residuals vs. fitted, residuals vs. predictor plots to identify improvements in modeling and address ethical concerns Interpret MSE, RMSE, MAE, and R-squared in a contextually meaningful way Overfitting and cross-validation Explain why training/in-sample model evaluation metrics can provide a misleading view of true test/out-of-sample performance Accurately describe all steps of cross-validation to estimate the test/out-of-sample version of a model evaluation metric Explain what role CV has in a predictive modeling analysis and its connection to overfitting Explain the pros/cons of higher vs. lower k in k-fold CV in terms of sample size and computing time Subset selection Clearly describe the forward and backward stepwise selection algorithm and why they are examples of greedy algorithms Compare best subset and stepwise algorithms in terms of optimality of output and computational time LASSO (shrinkage/regularization) Explain how ordinary and penalized least squares are similar and different with regard to (1) the form of the objective function and (2) the goal of variable selection Explain how the lambda tuning parameter affects model performance and how this is related to overfitting KNN Regression and the Bias-Variance Tradeoff Clearly describe / implement by hand the KNN algorithm for making a regression prediction Explain how the number of neighbors relates to the bias-variance tradeoff Explain the difference between parametric and nonparametric methods Explain how the curse of dimensionality relates to the performance of KNN (not in the video–will be discussed in class) Modeling Nonlinearity: Polynomial Regression and Splines Explain the advantages of splines over global transformations and other types of piecewise polynomials Explain how splines are constructed by drawing connections to variable transformations and least squares Explain how the number of knots relates to the bias-variance tradeoff Local Regression and Generalized Additive Models Clearly describe the local regression algorithm for making a prediction Explain how bandwidth (span) relate to the bias-variance tradeoff Describe some different formulations for a GAM (how the arbitrary functions are represented) Explain how to make a prediction from a GAM Interpret the output from a GAM Logistic regression Use a logistic regression model to make hard (class) and soft (probability) predictions Interpret non-intercept coefficients from logistic regression models in the data context Evaluating classification models Calculate (by hand from confusion matrices) and contextually interpret overall accuracy, sensitivity, and specificity Construct and interpret plots of predicted probabilities across classes Explain how a ROC curve is constructed and the rationale behind AUC as an evaluation metric Appropriately use and interpret the no-information rate to evaluate accuracy metrics Decision trees Clearly describe the recursive binary splitting algorithm for tree building for both regression and classification Compute the weighted average Gini index to measure the quality of a classification tree split Compute the sum of squared residuals to measure the quality of a regression tree split Explain how recursive binary splitting is a greedy algorithm Explain how different tree parameters relate to the bias-variance tradeoff Bagging and random forests Explain the rationale for bagging Explain the rationale for selecting a random subset of predictors at each split (random forests) Explain how the size of the random subset of predictors at each split relates to the bias-variance tradeoff Explain the rationale for and implement out-of-bag error estimation for both regression and classification Explain the rationale behind the random forest variable importance measure and why it is biased towards quantitative predictors (in class) K-means clustering Clearly describe / implement by hand the k-means algorithm Describe the rationale for how clustering algorithms work in terms of within-cluster variation Describe the tradeoff of more vs. less clusters in terms of interpretability Implement strategies for interpreting / contextualizing the clusters Hierarchical clustering Clearly describe / implement by hand the hierarchical clustering algorithm Compare and contrast k-means and hierarchical clustering in their outputs and algorithms Interpret cuts of the dendrogram for single and complete linkage Describe the rationale for how clustering algorithms work in terms of within-cluster variation Describe the tradeoff of more vs. less clusters in terms of interpretability Implement strategies for interpreting / contextualizing the clusters "],["r-and-rstudio-setup.html", "R and RStudio Setup Troubleshooting", " R and RStudio Setup Before class on Tuesday, Jan 24, you should follow these instructions to set up the software that we’ll be using throughout the semester. Even if you’ve already downloaded both R and RStudio, you’ll want to re-download to make sure that you have the most current versions. Highly recommended: Change the default file download location for your internet browser. Generally by default, internet browsers automatically save all files to the Downloads folder on your computer. This does not encourage good file organization practices. It is highly recommended that you change this option so that your browser asks you where to save each file before downloading it. This page has information on how to do this for the most common browsers. Required: Download R and RStudio FIRST: Download R here. You will see three links “Download R for …” Choose the link that corresponds to your computer. As of December 30, 2022, the latest version of R is 4.2.2. SECOND: Download RStudio here. Click the button under step 2 to install the version of RStudio recommended for your computer. As of December 30, 2022, the latest version of RStudio is 2022.12.0+353. Suggested: Watch this video made by Professor Lisa Lendway that describes essential configuration options for RStudio. Required: Install required packages. An R package is an extra bit of functionality that will help us in our data analysis efforts in a variety of ways. Open RStudio and click inside the Console pane (by default, the bottom left pane). Copy and paste the following command into the Console. You should see the text below appear to the right of the &gt;, which is called the R prompt. After you paste, hit Enter. install.packages(c(&quot;ggplot2&quot;, &quot;dplyr&quot;, &quot;readr&quot;, &quot;rmarkdown&quot;, &quot;broom&quot;, &quot;caret&quot;)) You will see a lot of text from status messages appearing in the Console as the packages are being installed. Wait until you see the &gt; again. Enter the command library(ggplot2) and hit enter. If you see the message Error in library(ggplot2) : there is no package called ggplot2, then there was a problem installing this package. Jump down to the Troubleshooting section below. (Any other messages that appear are fine, and a lack of any messages is also fine.) Repeat the above step for the commands: library(dplyr) library(readr) library(rmarkdown) library(broom) library(caret) Quit RStudio. You’re done setting up! Optional: For a refresher on RStudio features, watch this video. It also shows you how to customize the layout and color scheme of RStudio. Troubleshooting Problem: You are on a Mac and getting the following error: Error: package or namespace load failed for ‘ggplot2’ in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]): there is no package called ‘rlang’ Here’s how to fix it: First install the suite of Command Line Tools for Mac using the instructions here. Next enter install.packages(\"rlang\") in the Console. Finally check that entering library(ggplot2) gives no errors. You should be good now for dplyr, readr, and rmarkdown. "],["introductions.html", "Topic 1 Introductions Envisioning a Community of Learners Explorations", " Topic 1 Introductions Slides from today are available here. Envisioning a Community of Learners Directions: In your groups, please first introduce yourselves in whatever way you feel appropriate (e.g., preferred name, pronouns, how you’re feeling at the moment, things you’re looking forward to). When everyone is ready, discuss the 3 prompts below and record thoughts in this Google Doc. The instructor will summarize responses from both sections to create a resource that everyone can use. Prompts: Wonderful leaders in the Macalester community have complied the following set of agreements to consider when working in small groups. Which of the following do you think are most important to keep in mind for our time together in this course and why? Confidentiality: Take the lesson, not the story Use “I” statements; Speak your truth W.A.I.T. (Why Am I Talking/Why Aren’t I Talking) Listen for understanding Unpack assumptions Extend and receive grace Understand impact vs. intention Breathe and lean into discomfort Accept non-closure What strategies have you found work well for you to succeed both in and out of class in these times? What are some things that have contributed to positive experiences in your courses that you would like to see again this module? What has contributed to negative experiences that you would like to prevent? Explorations Each of the data contexts below prompts a broad research goal. For each, sharpen the focus of that goal by coming up with more targeted research questions that: Can be studied with a regression exploration Can be studied with a classification exploration Can be studied with an unsupervised learning exploration Are there harms that you anticipate arising from the collection of data or its analysis? Context 1: The New York Times is trying to better understand the popularity of its different articles with the hope of using this understanding to improve hiring of writers and improve their website layout. Context 2: The Minnesota Department of Health is trying to better understand the different health trajectories of people who have contracted a certain illness to improve funding for relevant health services. Context 3: The campaign management team for a political candidate is trying to better understand voter turnout in different regions of the country to run a better campaign in the next election. "],["evaluating-regression-models.html", "Topic 2 Evaluating Regression Models Learning Goals Exercises", " Topic 2 Evaluating Regression Models Learning Goals Create and interpret residuals vs. fitted, residuals vs. predictor plots to identify improvements in modeling and address ethical concerns Interpret MSE, RMSE, MAE, and R-squared in a contextually meaningful way We’ll also start to develop some new ideas relating to our next topics. Slides from today are available here. Exercises You can download a template RMarkdown file to start from here. Context We’ll be working with a dataset containing physical measurements on 80 adult males. These measurements include body fat percentage estimates as well as body circumference measurements. fatBrozek: Percent body fat using Brozek’s equation: 457/Density - 414.2 fatSiri: Percent body fat using Siri’s equation: 495/Density - 450 density: Density determined from underwater weighing (gm/cm^3). age: Age (years) weight: Weight (lbs) height: Height (inches) neck: Neck circumference (cm) chest: Chest circumference (cm) abdomen: Abdomen circumference (cm) hip: Hip circumference (cm) thigh: Thigh circumference (cm) knee: Knee circumference (cm) ankle: Ankle circumference (cm) biceps: Biceps (extended) circumference (cm) forearm: Forearm circumference (cm) wrist: Wrist circumference (cm) It takes a lot of effort to estimate body fat percentage accurately through underwater weighing. The goal is to build the best predictive model for fatSiri using just circumference measurements, which are more easily attainable. (We won’t use fatBrozek or density as predictors because they’re other outcome variables.) library(readr) library(ggplot2) library(dplyr) library(broom) library(tidymodels) tidymodels_prefer() bodyfat &lt;- read_csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) # Remove the fatBrozek and density variables bodyfat &lt;- bodyfat %&gt;% select(-fatBrozek, -density) Class investigations We’ll work through this section together to review concepts and code. You’ll then work on the remainder of the exercises in your groups. # Exploratory plots # Univariate distribution of outcome ggplot(bodyfat, aes(???)) + geom_???() # Scatterplot of fatSiri vs. weight ggplot(bodyfat, aes(???)) + geom_???() Let’s fit a linear regression model to predict body fat percentage from weight. mod1 &lt;- lm(??? ~ ???, data = bodyfat) We can use the augment() function from the broom package to augment our original data with useful information from the model fitting. In particular, residuals are stored in the .fitted column, and fitted (predicted) values for the cases supplied in newdata are stored in the .fitted column. mod1_output &lt;- broom::augment(mod1, newdata = bodyfat) head(mod1_output) We can use the augment() output to compute error metrics, and glance() to obtain R^2: mod1 %&gt;% augment() %&gt;% summarize( mse = mean((fatSiri - .fitted)^2), rmse = sqrt(mse), mae = mean(abs(fatSiri - .fitted)) ) # R-squared - interpretation? (unit-less) mod1 %&gt;% glance() %&gt;% select(r.squared) …and to create residual plots: # Univariate plot of residuals mod1 %&gt;% augment() %&gt;% ggplot(aes(x = ???)) + geom_histogram() + theme_minimal() # Fitted vs. Residual plot mod1 %&gt;% augment() %&gt;% ggplot(aes(x = ???, y = ???)) + geom_point() + geom_smooth() + geom_hline(yintercept = 0) + theme_minimal() # Predictor vs. Residual plot mod1 %&gt;% augment() %&gt;% left_join(bodyfat) %&gt;% #Merge the remaining variables into data set ggplot(aes(x = height, y = ???)) + #note patterns in residual/error with height geom_point() + geom_smooth() + geom_hline(yintercept = 0) + theme_minimal() Exercise 1 First decide on what you think would be a good model by picking variables based on context. Fit this model, calling it mod_initial. (Remember that you can include several predictors with a + in the model formula - like y ~ x1+x2.) # Code to fit initial model Use residual plot explorations to check if you need to update your model. # Residual plot explorations Fit your updated model, and call it model_updated. # Code to fit updated model Exercise 2 Compute and contextually interpret relevant evaluation metrics for your model. # Code to compute evaluation metrics Exercise 3 Now that you’ve selected your best model, deploy it in the real world by applying it to a new set of 172 adult males. You’ll need to update the newdata argument of the broom::augment() code to use bodyfat_test instead of bodyfat. bodyfat_test &lt;- read_csv(&quot;https://www.dropbox.com/s/7gizws208u0oywq/bodyfat_test.csv?dl=1&quot;) Compare your evaluation metrics from Exercise 2 the metrics here. What do you notice? (Note: this observation is just based on your one particular fitted model. You’ll make some more comprehensive observations in the next exercise.) In general, do you think that models with more or less variables would perform better in this comparison? Explain. Exercise 4 The code below systematically looks at the same comparison that you made in Exercise 3 but for every possible linear regression model formed from inclusion/exclusion of the predictors (without transformations or interactions). Run the code to make a plot of the results of this systematic investigation. (Feel free to inspect the code if you’re curious, but otherwise, don’t worry about understanding it fully.) What do you notice? What do you wonder? get_maes &lt;- function(mod, train_data, test_data) { mod_output_train &lt;- broom::augment(mod, newdata = train_data) mod_output_test &lt;- broom::augment(mod, newdata = test_data) train_mae &lt;- mean(abs(mod_output_train$.resid)) test_mae &lt;- mean(abs(mod_output_test$.resid)) c(train_mae, test_mae) } possible_predictors &lt;- setdiff(colnames(bodyfat), c(&quot;fatSiri&quot;, &quot;hipin&quot;)) results &lt;- bind_rows(lapply(1:13, function(i) { combos &lt;- combn(possible_predictors, i) bind_rows(lapply(seq_len(ncol(combos)), function(j) { formula &lt;- paste(&quot;fatSiri ~&quot;, paste(combos[,j], collapse = &quot;+&quot;)) mod &lt;- lm(as.formula(formula), data = bodyfat) maes &lt;- get_maes(mod = mod, train_data = bodyfat, test_data = bodyfat_test) tibble( form = formula, train_mae = maes[1], test_mae = maes[2], num_predictors = i ) })) })) # Relabel the num_predictors variable results &lt;- results %&gt;% mutate(num_predictors = paste(&quot;# predictors:&quot;, num_predictors)) %&gt;% mutate(num_predictors = factor(num_predictors, levels = paste(&quot;# predictors:&quot;, 1:13))) # Plot results ggplot(results, aes(x = train_mae, y = test_mae, color = num_predictors)) + geom_point() + coord_cartesian(xlim = c(0,7.5), ylim = c(0,7.5)) + geom_abline(slope = 1, intercept = 0) + facet_wrap(~num_predictors) + guides(color = FALSE) + labs(x = &quot;MAE on original data&quot;, y = &quot;MAE on new data on 172 men&quot;) + theme_classic() "],["overfitting.html", "Topic 3 Overfitting Learning Goals The tidymodels package Exercises", " Topic 3 Overfitting Learning Goals Explain why training/in-sample model evaluation metrics can provide a misleading view of true test/out-of-sample performance Implement testing and training sets in R using the tidymodels package Slides from today are available here. The tidymodels package (If you have not already installed the tidymodels package, install it with install.packages(\"tidymodels\").) Over this course, we will looking at a broad but linked set of specialized tools applied in statistical machine learning. Specialized tools generally require specialized code. Each tool has been developed separately and coded in a unique way. In order to facilitate and streamline the user experience, there have been attempts at creating a uniform interface, such as the caret R package. The developers of the caret package are no longer maintaining those packages. They are working on a newer package, called tidymodels. In this class, we will use the tidymodels package, which uses the tidyverse syntax you learned in Stat 155. The tidymodels package is a relatively new package and continues to be developed as we speak. This means that I’m learning with you and in a month or two, there may be improved functionality. As Prof. Heggeseth introduced in the R code videos, we have a general workflow structure that includes a model specification and a recipe (formula + preprocessing steps). # Load the package library(tidymodels) tidymodels_prefer() # Set the seed for the random number generator set.seed(123) # Specify Model model_spec &lt;- linear_reg() %&gt;% # type of model set_engine(engine = ____) #%&gt;% # algorithm to fit the model set_args(__) %&gt;% # hyperparameters/tuning parameters are needed for some models set_mode(__) # regression or classification # Specify Recipe (if you have preprocessing steps) rec &lt;- recipe(formula, data) %&gt;% step_{FILLIN}() %&gt;% # e.g., step_filter() to subset the rows step_{FILLIN}() # e.g., step_lincomb() to remove all predictors which are perfect linear combinations of another # Create Workflow (Model + Recipe) model_wf &lt;- workflow() %&gt;% add_recipe(rec) %&gt;% #or add_formula() add_model(model_spec) We can fit that workflow to training data. # Fit Model to training data (without a recipe) fit_model &lt;- fit(model_spec, formula, data_train) # Fit Model &amp; Recipe to training data fit_model &lt;- fit(model_wf, data_train) And then we can evaluate that fit model on testing data (new data that has not been used to fit the model). # Evaluate on testing data model_output &lt;- fit_model %&gt;% predict(new_data = data_test) %&gt;% # this function will apply recipe to new_data and do prediction bind_cols(data_test) reg_metrics &lt;- metric_set(rmse, rsq, mae) model_output %&gt;% reg_metrics(truth = __, estimate = .pred) The power of tidymodels is that it allows us to streamline the vast world of machine learning techniques into one common syntax. On top of \"lm\", there are many other different machine learning methods that we can use. In the exercises below, you’ll need to adapt the code above to fit a linear regression model (engine = \"lm\"). Exercises You can download a template RMarkdown file to start from here. Context We’ll be working with a dataset containing physical measurements on 80 adult males. These measurements include body fat percentage estimates as well as body circumference measurements. fatBrozek: Percent body fat using Brozek’s equation: 457/Density - 414.2 fatSiri: Percent body fat using Siri’s equation: 495/Density - 450 density: Density determined from underwater weighing (gm/cm^3). age: Age (years) weight: Weight (lbs) height: Height (inches) neck: Neck circumference (cm) chest: Chest circumference (cm) abdomen: Abdomen circumference (cm) hip: Hip circumference (cm) thigh: Thigh circumference (cm) knee: Knee circumference (cm) ankle: Ankle circumference (cm) biceps: Biceps (extended) circumference (cm) forearm: Forearm circumference (cm) wrist: Wrist circumference (cm) It takes a lot of effort to estimate body fat percentage accurately through underwater weighing. The goal is to build the best predictive model for fatSiri using just circumference measurements, which are more easily attainable. (We won’t use fatBrozek or density as predictors because they’re other outcome variables.) library(readr) library(ggplot2) library(dplyr) library(broom) library(tidymodels) bodyfat_train &lt;- read_csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) # Remove the fatBrozek, density, and hipin variables bodyfat_train &lt;- bodyfat_train %&gt;% select(-fatBrozek, -density, -hipin) Exercise 1: 5 models Consider the 5 models below: lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &#39;lm&#39;) %&gt;% set_mode(&#39;regression&#39;) mod1 &lt;- fit(lm_spec, fatSiri ~ age+weight+neck+abdomen+thigh+forearm, data = bodyfat_train) mod2 &lt;- fit(lm_spec, fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps, data = bodyfat_train) mod3 &lt;- fit(lm_spec, fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps+chest+hip, data = bodyfat_train) mod4 &lt;- fit(lm_spec, fatSiri ~ ., # The . means all predictors data = bodyfat_train) bf_recipe &lt;- recipe(fatSiri ~ ., data = bodyfat_train) %&gt;% step_normalize(all_numeric_predictors()) bf_wf &lt;- workflow() %&gt;% add_recipe(bf_recipe) %&gt;% add_model(lm_spec) mod5 &lt;- fit(bf_wf, data = bodyfat_train) STAT 155 review: Look at the tidy() of mod1. Contextually interpret the coefficient for the weight predictor. Is anything surprising? Why might this be? Explain how mod5 is different than mod4. You may want to look at bf_recipe %&gt;% prep(bodyfat_train) %&gt;% juice() to see the preprocessed training data. Which model will have the lowest training RMSE, and why? Explain before calculating (that is part d). Compute the training RMSE for models 1 through 5 to check your answer for part c. Write a sentence interpreting one of values of RMSE in context. Below is a coding example to get you started. reg_metrics = metric_set(rmse) mod1 %&gt;% predict(new_data = bodyfat_train) %&gt;% reg_metrics(truth = bodyfat_train$fatSiri, estimate = .pred) Which model do you think is the “best”? You may calculate MAE and R squared as well to justify your answer. f, Which model do you think will perform worst on new test data? Why? Exercise 2: Evaluating the Test Data Now that you’ve thought about how well the models might perform on test data, deploy it in the real world by applying it to a new set of 172 adult males. You’ll need to update the new_data to use bodyfat_test instead of bodyfat_train. bodyfat_test &lt;- read_csv(&quot;https://www.dropbox.com/s/7gizws208u0oywq/bodyfat_test.csv?dl=1&quot;) Calculate the test RMSE, MAE, and R squared for all five of the models. Here is some code to get you started. # Use fit/trained models and evaluate on test data reg_metrics = metric_set(rmse, mae, rsq) mod1 %&gt;% predict(new_data = bodyfat_test) %&gt;% reg_metrics(truth = bodyfat_test$fatSiri, estimate = .pred) Look back to Exercise 1 and see which model you thought was “best” based on the training data. Is that the “best” model in terms of predicting on new data? Explain. In “real life” we only have one data set. To get a sense of predictive performance on new test data, we could split our data into two groups. Discuss pros and cons of ways you might split the data. How big should the training set be? How big should the testing set be? Exercise 3: Overfitting If you have time, consider the following relationship. Imagine a set of predictions that is overfit to this training data. You are not limited to lines. Draw a picture of that function of predictions on a piece of paper. set.seed(123) data_train &lt;- tibble::tibble( x = runif(15,0,7), y = x^2 + rnorm(15,sd = 7) ) data_train %&gt;% ggplot(aes(x = x, y = y)) + geom_point() + theme_classic() "],["cross-validation.html", "Topic 4 Cross-validation Learning Goals Exercises", " Topic 4 Cross-validation Learning Goals Explain why training/in-sample model evaluation metrics can provide a misleading view of true test/out-of-sample performance Accurately describe all steps of cross-validation to estimate the test/out-of-sample version of a model evaluation metric Explain what role CV has in a predictive modeling analysis and its connection to overfitting Explain the pros/cons of higher vs. lower k in k-fold CV in terms of sample size and computing time Implement cross-validation in R using the tidymodels package Slides from today are available here. Exercises You can download a template RMarkdown file to start from here. Context We’ll be working with a dataset containing physical measurements on 80 adult males. These measurements include body fat percentage estimates as well as body circumference measurements. fatBrozek: Percent body fat using Brozek’s equation: 457/Density - 414.2 fatSiri: Percent body fat using Siri’s equation: 495/Density - 450 density: Density determined from underwater weighing (gm/cm^3). age: Age (years) weight: Weight (lbs) height: Height (inches) neck: Neck circumference (cm) chest: Chest circumference (cm) abdomen: Abdomen circumference (cm) hip: Hip circumference (cm) thigh: Thigh circumference (cm) knee: Knee circumference (cm) ankle: Ankle circumference (cm) biceps: Biceps (extended) circumference (cm) forearm: Forearm circumference (cm) wrist: Wrist circumference (cm) It takes a lot of effort to estimate body fat percentage accurately through underwater weighing. The goal is to build the best predictive model for fatSiri using just circumference measurements, which are more easily attainable. (We won’t use fatBrozek or density as predictors because they’re other outcome variables.) library(readr) library(ggplot2) library(dplyr) library(broom) library(tidymodels) bodyfat_train &lt;- read_csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) # Remove the fatBrozek, density, and hipin variables bodyfat_train &lt;- bodyfat_train %&gt;% select(-fatBrozek, -density, -hipin) and consider the first four models we built on Thursday: lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &#39;lm&#39;) %&gt;% set_mode(&#39;regression&#39;) mod1 &lt;- fit(lm_spec, fatSiri ~ age+weight+neck+abdomen+thigh+forearm, data = bodyfat_train) mod2 &lt;- fit(lm_spec, fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps, data = bodyfat_train) mod3 &lt;- fit(lm_spec, fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps+chest+hip, data = bodyfat_train) mod4 &lt;- fit(lm_spec, fatSiri ~ ., # The . means all predictors data = bodyfat_train) Exercise 1: Cross-validation in Concept We are going to repeat what we did last week but use cross-validation to help us evaluate models in terms of the predictive performance. Explain to your table-mates the steps of cross validation (CV) in concept and then how you might use 10-fold CV with these 80 individual data points. Exercise 2: Cross-validation with tidymodels Complete the code below to perform 10-fold cross-validation for mod1 to estimate the test RMSE (\\(\\text{CV}_{(10)}\\)). Do we need to use set.seed()? Why or why not? (Is there a number of folds for which we would not need to set the seed?) # Do we need to use set.seed()? set.seed(2023) bodyfat_cv &lt;- vfold_cv(??, v = 10) model_wf &lt;- workflow() %&gt;% add_formula(??) %&gt;% add_model(lm_spec) mod1_cv &lt;- fit_resamples(model_wf, resamples = bodyfat_cv, metrics = metric_set(rmse, rsq, mae) ) Run the code below, and use this to calculate the 10-fold cross-validated RMSE “by hand” (you can use R code, but apply the formula mathematically). mod1_cv %&gt;% unnest(.metrics) Run the code below, and compare your answer to part b. mod1_cv %&gt;% collect_metrics() Exercise 3: Looking at the evaluation metrics Perform 10-fold CV using mod2, mod3, and mod4 by running the code below: model2_wf &lt;- workflow() %&gt;% add_formula(fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps) %&gt;% add_model(lm_spec) mod2_cv &lt;- fit_resamples(model2_wf, resamples = bodyfat_cv, metrics = metric_set(rmse, rsq, mae) ) model3_wf &lt;- workflow() %&gt;% add_formula(fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps+chest+hip) %&gt;% add_model(lm_spec) mod3_cv &lt;- fit_resamples(model3_wf, resamples = bodyfat_cv, metrics = metric_set(rmse, rsq, mae) ) model4_wf &lt;- workflow() %&gt;% add_formula(fatSiri ~ .) %&gt;% add_model(lm_spec) mod4_cv &lt;- fit_resamples(model4_wf, resamples = bodyfat_cv, metrics = metric_set(rmse, rsq, mae) ) mod1_cv %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;rmse&quot;) mod2_cv %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;rmse&quot;) mod3_cv %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;rmse&quot;) mod4_cv %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;rmse&quot;) Look at the completed table below of evaluation metrics for the 4 models. Model Training RMSE \\(\\text{CV}_{(10)}\\) mod1 3.811 4.193 mod2 3.767 4.305 mod3 3.752 4.368 mod4 3.572 4.438 Which model performed the best on the training data? Which model performed best on the test set? Explain why there’s a discrepancy between these 2 answers and why CV, in general, can help reduce the impact overfitting. Exercise 4: Practical issues: choosing \\(k\\) In terms of sample size, what are the pros/cons of low vs. high \\(k\\)? In terms of computational time, what are the pros/cons of low vs. high \\(k\\)? If possible, it is advisable to choose \\(k\\) to be a divisor of the sample size. Why do you think that is? Digging deeper If you have time, consider these exercises to further explore concepts related to today’s ideas. Consider leave-one-out-cross-validation (LOOCV) Would two different seeds make a difference in the results (using set.seed)? Why or why not? Using the information from your_output %&gt;% unnest(.metrics), construct a visualization to examine the variability of RMSE from case to case. What might explain any large values? What does this highlight about the quality of estimation of LOOCV? "],["variable-subset-selection.html", "Topic 5 Variable Subset Selection Learning Goals Exercises", " Topic 5 Variable Subset Selection Learning Goals Clearly describe the forward and backward stepwise selection algorithm and why they are examples of greedy algorithms Compare best subset and stepwise algorithms in terms of optimality of output and computational time Describe how selection algorithms can give a measure of variable importance Exercises You can download a template RMarkdown file to start from here. We’ll continue using the body fat dataset to explore subset selection methods. library(caret) library(ggplot2) library(dplyr) library(readr) bodyfat &lt;- read_csv(&quot;http://www.macalester.edu/~ajohns24/data/bodyfatsub.csv&quot;) # Take out the redundant Density and HeightFt variables bodyfat &lt;- bodyfat %&gt;% select(-Density, -HeightFt) Exercise 1: Backward stepwise selection: by hand In the backward stepwise procedure, we start with the full model, full_model, with all predictors: full_model &lt;- lm(BodyFat ~ Age + Weight + Height + Neck + Chest + Abdomen + Hip + Thigh + Knee + Ankle + Biceps + Forearm + Wrist, data = bodyfat) To practice the backward selection algorithm, step through a few steps of the algorithm using p-values as a selection criterion: Identify which predictor contributes the least to the model. One (problematic) approach is to identify the least significant predictor. Fit a new model which eliminates this predictor. Identify the least significant predictor in this model. Fit a new model which eliminates this predictor. Repeat 1 more time to get the hang of it. (We discussed in the video how the use of p-values for selection is problematic, but for now you’re just getting a handle on the algorithm. You’ll think about the problems with p-values in the next exercise.) Exercise 2: Interpreting the results Examine which predictors remain after the previous exercise. Are you surprised that, for example, Wrist is still in the model but Weight is not? Does this mean that Wrist is a better predictor of body fat percentage than Weight is? What statistical idea is relevant here? Exercise 3: Planning forward selection using CV Using p-values to perform stepwise selection has problems, as was discussed in the video. A better alternative to target predictive accuracy is to evaluate the models using cross-validation. Fully outline the steps required to use cross-validation to perform forward selection. Make sure to provide enough detail such that the stepwise selection and CV algorithms are made clear. Exercise 4: Stepwise selection in caret Run install.packages(\"leaps\") in the Console to install the leaps package before proceeding. Complete the caret code below to perform backward stepwise selection with cross-validation. The following points will help you complete the code: In R model formulas, y ~ . sets y as the outcome and all other predictors in the dataset as predictors. The specific method name for backward selection is \"leapBackward\". The tuneGrid argument is already filled in. It allows us to input tuning parameters into the fitting process. The tuning parameters for subset selection are the number of variables included in the models explored (nvmax). This can vary from 1 to 13 (the maximum number of predictors possible). Use 10-fold CV to estimate test performance of the models. Use \"MAE\" as the evaluation metric to choose how the best of the 1-variable, 2-variable, etc. models will be chosen. (Note: CV is only used to pick among the best 1, 2, 3, …, and 13 variable models. To find the best 1, 2, 3, …, and 13 variable models, training MSE is used. caret uses training MSE because within a subset size, all models have the same number of coefficients, which makes both ordinary R-squared and training MSE ok for comparing models.) set.seed(23) back_step_mod &lt;- train( y ~ x, data = bodyfat, method = ___, tuneGrid = data.frame(nvmax = 1:13), trControl = ___, metric = ___, na.action = na.omit ) Exercise 5: Exploring the results There are a number of ways to examine and use the output of the selection algorithm, which we’ll explore here. (It would be useful to make notes along the way - perhaps on your code note sheet.) Part a Let’s first examine the sequence of models explored. The stars in the table at the bottom indicate the variables included in the 1-variable, 2-variable, etc. models. summary(back_step_mod) Of the 13 models in the sequence, R only prints out the 11 smallest models (since, for reasons we’ll discuss below, it determines the 11 predictor model to be “best”). Which predictor is the last to remain in the model? Second-to-last to remain? How do you think we could use these results to identify which predictors were most/least important in predicting the outcome of body fat percentage? Part b Examine the 10-fold CV MAE for each of the 13 models in the backward stepwise sequence: # Plot metrics for each model in the sequence plot(back_step_mod) # Look at accuracy/error metrics for the different subset sizes back_step_mod$results Which size model has the lowest CV MAE? Which size model would you pick? Why? Part c In our model code, we used selectionFunction = \"best\" by default inside trainControl(). By doing so, we indicated that we wanted to find which model minimizes the CV MAE (i.e., has the “best” MAE). With respect to this criterion: # What tuning parameter gave the best performance? # i.e. What subset size gave the best model? back_step_mod$bestTune # Obtain the coefficients for the best model coef(back_step_mod$finalModel, id = back_step_mod$bestTune$nvmax) # Obtain the coefficients of any size model with at most as many variables as the overall best model (e.g., the 2-predictor model) coef(back_step_mod$finalModel, id = 2) Another sensible choice for the selection function is to not choose the model with the lowest estimated error but to account for the uncertainty in the estimation of that test error by picking the smallest model for which the CV MAE is within one standard error of the minimum CV MAE. What do you think the rationale for this is? (We’ll explore the code for this formally later, or you can try it out below in the Digging Deeper section.) Part d We should end by evaluating our final chosen model. Contextually interpret (with units) the CV MAE for the model. Make residual plots for the chosen model in one of 2 ways: (1) use lm() to fit the model with the chosen predictors or (2) use the following code to create a dataset called back_step_mod_out which contains the original data as well as predicted values and residuals (fitted and resid). back_step_mod_out &lt;- bodyfat %&gt;% mutate( fitted = predict(back_step_mod, newdata = bodyfat), resid = BodyFat - fitted ) Digging deeper As mentioned in Exercise 5c, we have another choice for the selectionFunction used to choose from many possible models. The use of selectionFunction = \"oneSE\" below picks the simplest model for which the CV MAE is within one standard error of the minimum CV MAE. Compare the output you obtain here with your best model from Exercise 5. back_step_mod_1se &lt;- train( BodyFat ~ ., data = bodyfat, method = &quot;leapBackward&quot;, tuneGrid = data.frame(nvmax = 1:13), trControl = trainControl(method = &quot;cv&quot;, number = 10, selectionFunction = &quot;oneSE&quot;), metric = &quot;MAE&quot;, na.action = na.omit ) Forward selection can be implemented in caret with analogous code, except that the method name is \"leapForward\". Try implementing forward selection and comparing your results. "],["lasso-shrinkageregularization.html", "Topic 6 LASSO: Shrinkage/Regularization Learning Goals LASSO models in tidymodels Exercises", " Topic 6 LASSO: Shrinkage/Regularization Learning Goals Explain how ordinary and penalized least squares are similar and different with regard to (1) the form of the objective function and (2) the goal of variable selection Explain why variable scaling is important for the performance of shrinkage methods Explain how the lambda tuning parameter affects model performance and how this is related to overfitting Describe how output from LASSO models can give a measure of variable importance Slides from today are available here. LASSO models in tidymodels To build LASSO models in tidymodels, first load the package and set the seed for the random number generator to ensure reproducible results: library(dplyr) library(readr) library(broom) library(ggplot2) library(tidymodels) tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions set.seed(___) # Pick your favorite number to fill in the parentheses Then adapt the following code: # Lasso Model Spec lm_lasso_spec &lt;- linear_reg() %&gt;% set_args(mixture = 1, penalty = 0) %&gt;% ## mixture = 1 indicates Lasso, we&#39;ll choose penalty later set_engine(engine = &#39;glmnet&#39;) %&gt;% # glmnet does regularization (LASSO, ridge, elastic net) set_mode(&#39;regression&#39;) # Recipe with standardization (!) data_rec &lt;- recipe( ___ ~ ___ , data = ___) %&gt;% step_nzv(all_predictors()) %&gt;% # removes variables with the same value step_novel(all_nominal_predictors()) %&gt;% # important if you have rare categorical variables step_normalize(all_numeric_predictors()) %&gt;% # important standardization step for LASSO step_dummy(all_nominal_predictors()) # creates indicator variables for categorical variables # Workflow (Recipe + Model) lasso_wf &lt;- workflow() %&gt;% add_recipe(data_rec) %&gt;% add_model(lm_lasso_spec) # Fit Model lasso_fit &lt;- lasso_wf %&gt;% fit(data = ___) # Fit to data Examining the LASSO model for each \\(\\lambda\\) The glmnet engine fits models for each \\(\\lambda\\) automatically, so we can visualize the estimates for each penalty value. plot(lasso_fit %&gt;% extract_fit_parsnip() %&gt;% pluck(&#39;fit&#39;), # way to get the original glmnet output xvar = &quot;lambda&quot;) # glmnet fits the model with a variety of lambda penalty values Identifying the “best” LASSO model To identify the best model, we need to tune the model using cross validation. Adapt the following code to tune a Lasso Model to choose Lambda: # Create CV folds data_cv10 &lt;- vfold_cv(___, v = 10) # Lasso Model Spec with tune lm_lasso_spec_tune &lt;- linear_reg() %&gt;% set_args(mixture = 1, penalty = tune()) %&gt;% ## mixture = 1 indicates Lasso set_engine(engine = &#39;glmnet&#39;) %&gt;% #note we are using a different engine set_mode(&#39;regression&#39;) # Workflow (Recipe + Model) lasso_wf_tune &lt;- workflow() %&gt;% add_recipe(data_rec) %&gt;% add_model(lm_lasso_spec_tune) # Tune Model (trying a variety of values of Lambda penalty) penalty_grid &lt;- grid_regular( penalty(range = c(-5, 3)), #log10 transformed 10^-5 to 10^3 levels = 30) tune_res &lt;- tune_grid( # new function for tuning parameters lasso_wf_tune, # workflow resamples = data_cv10, # cv folds metrics = metric_set(rmse, mae), grid = penalty_grid # penalty grid defined above ) # Visualize Model Evaluation Metrics from Tuning autoplot(tune_res) + theme_classic() # Summarize Model Evaluation Metrics (CV) collect_metrics(tune_res) %&gt;% filter(.metric == &#39;rmse&#39;) %&gt;% # or choose mae select(penalty, rmse = mean) best_penalty &lt;- select_best(tune_res, metric = &#39;rmse&#39;) # choose penalty value based on lowest mae or rmse # Fit Final Model final_wf &lt;- finalize_workflow(lasso_wf_tune, best_penalty) # incorporates penalty value to workflow final_fit &lt;- fit(final_wf, data = ___) tidy(final_fit) Exercises You can download a template RMarkdown file to start from here. We’ll use a new data set to explore LASSO modeling. This data comes from the US Department of Energy. You will predict the fuel efficiency of modern cars from characteristics of these cars, like transmission and engine displacement. Fuel efficiency is a numeric value that ranges smoothly from about 15 to 40 miles per gallon. library(dplyr) library(readr) library(broom) library(ggplot2) library(tidymodels) tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions set.seed(123) cars2018 &lt;- read_csv(&quot;https://raw.githubusercontent.com/juliasilge/supervised-ML-case-studies-course/master/data/cars2018.csv&quot;) head(cars2018) # Cleaning cars2018 &lt;- cars2018 %&gt;% select(-model_index) Exercise 1: A least squares model Let’s start by building an ordinary (not penalized) least squares model to review important concepts. We’ll fit a model to predict fuel efficiency measured in miles per gallon (mpg) with all possible predictors. lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &#39;lm&#39;) %&gt;% set_mode(&#39;regression&#39;) full_rec &lt;- recipe(mpg ~ ., data = cars2018) %&gt;% update_role(model, new_role = &#39;ID&#39;) %&gt;% # we want to keep the name of the car model but not as a predictor or outcome step_nzv(all_predictors()) %&gt;% # removes variables with the same value step_normalize(all_numeric_predictors()) %&gt;% # important standardization step for LASSO step_dummy(all_nominal_predictors()) # creates indicator variables for categorical variables full_lm_wf &lt;- workflow() %&gt;% add_recipe(full_rec) %&gt;% add_model(lm_spec) full_model &lt;- fit(full_lm_wf, data = cars2018) full_model %&gt;% tidy() Use tidymodels to perform 10-fold cross-validation to estimate test MAE for this model. How do you think the estimated test error would change with fewer predictors? This model fit with ordinary least squares corresponds to a special case of penalized least squares. What is the value of \\(\\lambda\\) in this special case? As \\(\\lambda\\) increases, what would you expect to happen to the number of predictors that remain in the model? Exercise 2: Fitting a LASSO model in tidymodels Adapt our general LASSO code to fit a set of LASSO models with the following parameters: Use 10-fold CV. Use mean absolute error (MAE) to select a final model. Select the simplest model for which the metric is within one standard error of the best metric. Use a sequence of 100 \\(\\lambda\\) values from 0 to 10. Before running the code, enter install.packages(\"glmnet\") in the Console. Save the CV-fit models from tune_grid() as tune_output. # Fit LASSO models for a grid of lambda values # Tune and fit a LASSO model to the data (with CV) set.seed(2023) # Create CV folds data_cv10 &lt;- vfold_cv(??, v = 10) # Lasso Model Spec with tune lm_lasso_spec_tune &lt;- linear_reg() %&gt;% set_args(mixture = 1, penalty = tune()) %&gt;% ## mixture = 1 indicates Lasso set_engine(engine = &#39;glmnet&#39;) %&gt;% #note we are using a different engine set_mode(&#39;regression&#39;) # Workflow (Recipe + Model) lasso_wf_tune &lt;- workflow() %&gt;% add_recipe(full_rec) %&gt;% # recipe defined above add_model(lm_lasso_spec_tune) # Tune Model (trying a variety of values of Lambda penalty) penalty_grid &lt;- grid_regular( penalty(range = c(??, ??)), #log10 transformed levels = ??) tune_output &lt;- tune_grid( # new function for tuning parameters lasso_wf_tune, # workflow resamples = data_cv10, # cv folds metrics = metric_set(rmse, mae), grid = penalty_grid # penalty grid defined above ) Let’s visualize the model evaluation metrics from tuning. We can use autoplot(). # Visualize Model Evaluation Metrics from Tuning autoplot(tune_output) + theme_classic() Try replicating that MAE plot with the data below using ggplot. Use scale_x_log10(). metrics_output &lt;- collect_metrics(tune_output) %&gt;% filter(.metric == &#39;mae&#39;) Inspect the shape of the plot. The errors go down at the very beginning then start going back up. Based on this, what are the consequences of picking a \\(\\lambda\\) that is too small or too large? (This is an example of a very important idea that we’ll see shortly: the bias-variance tradeoff.) Next, we need to choose the lambda that leads to the best model. We can choose the lambda penalty value that leads to the lowest cv MAE or we can take into account the variation of the cv MAE and choose the largest lambda penalty value that is within 1 standard error of the lowest cv MAE. How might the models that result from these two penalties differ? best_penalty &lt;- select_best(tune_output, metric = &#39;mae&#39;) # choose penalty value based on lowest cv mae best_penalty best_se_penalty &lt;- select_by_one_std_err(tune_output, metric = &#39;mae&#39;, desc(penalty)) # choose largest penalty value within 1 se of the lowest cv mae best_se_penalty Now check your understanding by fitting both “final” models and comparing the coefficients. How are these two models different? # Fit Final Model final_wf &lt;- finalize_workflow(lasso_wf_tune, best_penalty) # incorporates penalty value to workflow final_wf_se &lt;- finalize_workflow(lasso_wf_tune, best_se_penalty) # incorporates penalty value to workflow final_fit &lt;- fit(final_wf, data = cars2018) final_fit_se &lt;- fit(final_wf_se, data = cars2018) tidy(final_fit) tidy(final_fit_se) Exercise 3: Examining output: plot of coefficient paths Once we’ved used cross validation, a useful plot allows us to examine coefficient paths resulting from the final fitted LASSO models: coefficient estimates as a function of \\(\\lambda\\). Before running the code, run install.packages(“stringr”) and install.packages(“purrr”) in the Console. glmnet_output &lt;- final_fit_se %&gt;% extract_fit_parsnip() %&gt;% pluck(&#39;fit&#39;) # get the original glmnet output lambdas &lt;- glmnet_output$lambda coefs_lambdas &lt;- coefficients(glmnet_output, s = lambdas ) %&gt;% as.matrix() %&gt;% t() %&gt;% as.data.frame() %&gt;% mutate(lambda = lambdas ) %&gt;% select(lambda, everything(), -`(Intercept)`) %&gt;% pivot_longer(cols = -lambda, names_to = &quot;term&quot;, values_to = &quot;coef&quot;) %&gt;% mutate(var = purrr::map_chr(stringr::str_split(term,&quot;_&quot;),~.[1])) coefs_lambdas %&gt;% ggplot(aes(x = lambda, y = coef, group = term, color = var)) + geom_line() + geom_vline(xintercept = best_se_penalty %&gt;% pull(penalty), linetype = &#39;dashed&#39;) + theme_classic() + theme(legend.position = &quot;bottom&quot;, legend.text=element_text(size=8)) There’s a lot of information in this plot! Each colored line corresponds to a different predictor. The x-axis reflects the range of different \\(\\lambda\\) values. At each \\(\\lambda\\), the y-axis reflects the coefficient estimates for the predictors in the corresponding LASSO model. The vertical dashed line shows where the best penalty value (using the SE method) based on cross-validated MAE. Very roughly eyeball the coefficient estimates at the smallest value of \\(\\lambda\\). Do they look like they correspond to the coefficient estimates from ordinary least squares in exercise 2? Why do all of the lines head toward y = 0 on the far right of the plot? What variables seem to be more “important” or “persistent” (persistently present in the model) variable? Does this make sense in context? Which predictor seems least “persistent”? Does this make sense in context? Note: If you’re curious about code to automate this visual inspection of variable importance, look at Digging Deeper. Exercise 4: Examining and evaluating the best LASSO model. Take a look at the predictors and coefficients for the “best” LASSO model. Are the predictors that remain in the model sensible? Do the coefficient signs make sense? # Obtain the predictors and coefficients of the &quot;best&quot; model # Filter out the coefficient are 0 final_fit_se %&gt;% tidy() %&gt;% filter(estimate != 0) Evaluate the best LASSO model: Contextually interpret (with units) the CV MAE error for the best model by inspecting tune_output %&gt;% collect_metrics() %&gt;% filter(penalty == (best_se_penalty %&gt;% pull(penalty))). Make residual plots for the model by creating a dataset called lasso_mod_out which contains the original data as well as predicted values and residuals (.pred and .resid). lasso_mod_out &lt;- final_fit_se %&gt;% predict(new_data = cars2018) %&gt;% bind_cols(cars2018) %&gt;% mutate(resid = mpg - .pred) Note: If you’re curious about making plots that show both test error estimates and their uncertainty, look at Digging Deeper. Digging deeper We used the plot of coefficient paths to evaluate the variable importance of our predictors. The code below does this systematically for each predictor so that we don’t have to eyeball. Step through and work out what each part is doing. It may help to look up function documentation with ?function_name in the Console. glmnet_output &lt;- final_fit_se %&gt;% extract_fit_engine() # Create a boolean matrix (predictors x lambdas) of variable exclusion bool_predictor_exclude &lt;- glmnet_output$beta==0 # Loop over each variable var_imp &lt;- sapply(seq_len(nrow(bool_predictor_exclude)), function(row) { this_coeff_path &lt;- bool_predictor_exclude[row,] if(sum(this_coeff_path) == ncol(bool_predictor_exclude)){ return(0)}else{ return(ncol(bool_predictor_exclude) - which.min(this_coeff_path) + 1)} }) # Create a dataset of this information and sort var_imp_data &lt;- tibble( var_name = rownames(bool_predictor_exclude), var_imp = var_imp ) var_imp_data %&gt;% arrange(desc(var_imp)) If you want more practice, the Hitters data in the ISLR package (be sure to to install and load) contains the salaries and performance measures for 322 Major League Baseball players. Use LASSO to determine the “best” predictive model of Salary. "],["homework-1.html", "Homework 1 Project Work Ethics in ML Portfolio Work", " Homework 1 **Submit by Sunday, Feb. 5th at 11:59pm to Moodle. Please turn in a single PDF document containing (1) your responses for the Project Work and Ethics in ML sections and (2) a LINK to the Google Doc with your responses for the Portfolio Work section. Project Work Goal: Find a dataset (or datasets) to use for your final project, and start to get to know the data. Details: Your dataset(s) should allow you to perform a (1) regression, (2) classification, and (3) unsupervised learning analysis. The following resources are good places to start looking for data: Google Dataset Search Kaggle UCI Machine Learning Repository Harvard Dataverse Even if you end up working with a partner on the project (which isn’t required - working alone is fine), please complete this initial work individually. It’s fine if you and a potential/future partner end up using the same dataset and collaborate on the finding of data, but complete the short bit of writing (below) individually. Check in with the instructor early if you need help. Deliverables: Write 1-2 paragraphs (no more than 350 words) summarizing: The information in the dataset(s) and the context behind the data. Use the prompts below to guide your thoughts. (Note: in some situations, there may be incomplete information on the data context. That’s fine. Just do your best to summarize what information is available, and acknowledge the lack of information where relevant.) What are the cases? Broadly describe the variables contained in the data. Who collected the data? When, why, and how? 3 research questions 1 that can be investigated in a regression setting 1 that can be investigated in a classification setting 1 that can be investigated in an unsupervised learning setting Also make sure that you can read the data into R. You don’t need to do any analysis in R yet, but making sure that you can read the data will make the next steps go more smoothly. Ethics in ML Read the article Amazon scraps secret AI recruiting tool that showed bias against women. Write a short (roughly 250 words), thoughtful response about the themes and cautions that the article brings forth. Portfolio Work Setup: In addition to your submission here, you’ll want to collect your Portfolio work in a single document. James shared a Google doc link with you on January 24th where you should keep your Portfolio responses. For each Homework submission, copy your Portfolio responses into the appropriate space. For example, Homework 1 has three prompts: Overfitting, Evaluating Regression Models, and Cross-validation. In your portfolio, copy and paste your responses under the appropriate header. Page maximum: 2 pages of text (pictures don’t count) Organization: Your choice! Use titles and section headings that make sense to you. (It probably makes sense to have a separate section for each method.) Deliverables: Put your responses for this part in a Google Doc, and update the link sharing so that anyone with the link at Macalester College can edit. Include the URL for the Google Doc in your submission. Note: Some prompts below may seem very open-ended. This is intentional. Crafting good responses requires looking back through our material to organize the concepts in a coherent, thematic way, which is extremely useful for your learning. Concepts to address: Overfitting: The video used the analogy of a cat picture model to explain overfitting. Come up with your own analogy to explain overfitting. Evaluating regression models: Describe how residuals are central to the evaluation of regression models. Explain how they arise in quantitative evaluation metrics and how they are used in evaluation plots. Include examples of plots that show desirable and undesirable model behavior (feel free to draw them by hand if you wish) and what steps can be taken to address that undesirable behavior. Cross-validation: In your own words, explain the rationale for cross-validation in relation to overfitting and model evaluation. Describe the algorithm in your own words in at most 2 sentences. "],["r-resources.html", "R Resources Outside resources Example code", " R Resources Outside resources Lisa Lendway’s COMP/STAT 112 website (with code examples and videos) RStudio cheat sheets ggplot2 reference R Programming Wikibook Debugging in R Article Video Colors in R Data import tutorial Free online textbooks R for Data Science Exploratory Data Analysis with R Example code Creating new variables case_when() from the dplyr package is a very versatile function for creating new variables based on existing variables. This can be useful for creating categorical or quantitative variables and for creating indices from multiple variables. # Turn quant_var into a Low/Med/High version data &lt;- data %&gt;% mutate(cat_var = case_when( quant_var &lt; 10 ~ &quot;Low&quot;, quant_var &gt;= 10 &amp; quant_var &lt;= 20 ~ &quot;Med&quot;, quant_var &gt; 20 ~ &quot;High&quot; ) ) # Turn cat_var (A, B, C categories) into another categorical variable # (collapse A and B into one category) data &lt;- data %&gt;% mutate(new_cat_var = case_when( cat_var %in% c(&quot;A&quot;, &quot;B&quot;) ~ &quot;A or B&quot; cat_var==&quot;C&quot; ~ &quot;C&quot; ) ) # Turn a categorical variable (x1) encoded as a numerical 0/1/2 variable into a different quantitative variable # Doing this for multiple variables allows you to create an index data &lt;- data %&gt;% mutate(x1_score = case_when( x1==0 ~ 10, x1==1 ~ 20, x1==2 ~ 50 ) ) # Add together multiple variables with mutate data &lt;- data %&gt;% mutate(index = x1_score + x2_score + x3_score) "],["final-project.html", "Final Project Requirements Grading Rubric", " Final Project Requirements You will be analyzing a dataset using a regression and a classification analysis. An unsupervised learning analysis is no longer required for the project. Collaboration: You may work in teams of up to 3 members. Individual work is fine. The weekly homework assignments will note whether work for that week should be submitted individually or if just one team member should submit work. There will be a required synthesis of the weekly homework investigations at the end of the course. If working on a team, this should be done in groups, rather than individually. Final deliverables: Only one team member has to submit these materials to Moodle. The due date is Thursday, May 4th at 11:59pm CST. Submit a final knitted HTML file (must knit without errors) and corresponding Rmd file containing code for your analysis Include a 10-15 minute video presentation of your project that addresses the items in the Grading Rubric below. (Recording the presentation over Zoom is a good option for creating the video. You can record to your computer or to the cloud.) Upload the video itself to Moodle. If it’s too large, share a link to a recording on the web or in a shared drive. All team members should have an equal speaking role in the presentation. If a video presentation would be difficult for you and your team to make, you may instead submit an annotated set of slides. The speaker notes beneath each slide should contain what you would have said in the video presentation. Grading Rubric Data context (10 points) Clearly describe what the cases in the final clean dataset represent. Broadly describe the variables used in your analyses. Who collected the data? When, why, and how? Answer as much of this as the available information allows. Research questions (10 points) Research question(s) for the regression task make clear the outcome variable and its units. Research question(s) for the classification task make clear the outcome variable and its possible categories. HW3 investigations - Methods (10 points) Describe the models used in your HW3 project work investigations. Describe what you did to evaluate models. Indicate how you estimated quantitative evaluation metrics. Indicate what plots you used to evaluate models. Describe the goals / purpose of the methods used in the overall context of your research investigations. HW3 investigations - Results - Variable Importance (10 points) Summarize results from HW3 Investigations 1 (and 2, if applicable) on variable importance measures. Note: Investigation 2 won’t be applicable to your project if you only have categorical predictors. Summarization should show evidence of acknowledging the data context in thinking about the sensibility of these results. HW3 investigations - Summary (10 points) If it was appropriate to fit a GAM for your investigations (having some quantitative predictors), show plots of estimated functions for each predictor, and provide some general interpretations. Compare the different models tried in HW3 in light of evaluation metrics, plots, variable importance, and data context. Display evaluation metrics for different models in a clean, organized way. This display should include both the estimated metric as well as its standard deviation. (Hint: you should be using caret_mod$results.) Broadly summarize conclusions from looking at these evaluation metrics and their measures of uncertainty. Summarize conclusions from residual plots from initial models (don’t have to display them though). Decide an overall most preferable model. Show and interpret some representative examples of residual plots for your final model. Does the model show acceptable results in terms of any systematic biases? Interpret evaluation metric(s) for the final model in context with units. Does the model show an acceptable amount of error? Classification analysis - Methods (10 points) Indicate 2 different methods used to answer your classification research question. Describe what you did to evaluate the 2 models explored. Indicate how you estimated quantitative evaluation metrics. Describe the goals / purpose of the methods used in the overall context of your research investigations. Classification analysis - Results - Variable Importance (10 points) Summarize results about variable importance measures in your classification analysis. Summarization should show evidence of acknowledging the data context in thinking about the sensibility of these results. Classification analysis - Summary (10 points) Compare the 2 different classification models tried in light of evaluation metrics, variable importance, and data context. Display evaluation metrics for different models in a clean, organized way. This display should include both the estimated metric as well as its standard deviation. (This won’t be available from OOB error estimation. If using OOB, don’t worry about reporting the SD.) Broadly summarize conclusions from looking at these evaluation metrics and their measures of uncertainty. Decide an overall most preferable model. Interpret evaluation metric(s) for the final model in context. Does the model show an acceptable amount of error? If using OOB error estimation, display the test (OOB) confusion matrix, and use it to interpret the strengths and weaknesses of the final model. Code (20 points) Knitted, error-free HTML and corresponding Rmd file submitted Code corresponding to all analyses above is present and correct "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
