[["index.html", "STAT 253: Statistical Machine Learning Welcome!", " STAT 253: Statistical Machine Learning Welcome! Image source This is the class manual for Statistical Machine Learning (STAT 253) at Macalester College for Spring 2023, taught by Professor James Normington. The content largely draws upon our class textbook, An Introduction to Statistical Learning with Applications in R. In addition, much of this site’s content was created by Professor Alicia Johnson, Professor Brianna Heggeseth, and Professor Leslie Myint. This bookdown website was constructed by Professor Brianna Heggeseth. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["schedule-syllabus.html", "Schedule &amp; Syllabus", " Schedule &amp; Syllabus The schedule below is a tentative outline of our plans for the module. Here is the syllabus, current as of Jan. 23, 2023. Before each class period, please watch the indicated videos and check on your understanding by actively reviewing the associated Learning Objectives. The readings listed below are optional but serve as a nice complement to the videos and class activities. Readings refer to chapters/sections in the Introduction to Statistical Learning (ISLR) textbook (available online here). Week 1: 1/19 - 1/20 Day(s) Topic Readings 1/19 Introductions ISLR: Chap 1, Chap 2 - Section 2.1 (Skip 2.1.2, 2.1.3 for now.) Week 2: 1/23 - 1/27 Day(s) Topic Videos/Readings Slides 1/24 Evaluating Regression Models Evaluating Regression Models R: Introduction to TidyModels ISLR: 2.2 PDF 1/26 Overfitting Overfitting R: Pre-processing and recipes PDF Homework 1 due Sunday, Feb 5th at 11:59pm CST Week 3: 1/30 - 2/3 Day(s) Topic Videos/Readings Slides 1/31 Cross-validation Cross-validation R: Training, testing, and cross-validation ISLR: 5.1 PDF 2/2 Subset Selection Variable Subset Selection R: Subset Selection ISLR: 6.1 PDF Week 4: 2/6 - 2/10 Day(s) Topic Videos/Readings Slides 2/7 LASSO (Shrinkage/Regularization) LASSO (Shrinkage/Regularization) ISLR: 6.2 PDF 2/9 Quiz 1 Sec 01 (9:40a) will cover Subset Selection after quiz Sec 02 (1:20p) may leave after quiz Quiz 1 study guide and details Week 5: 2/13 - 2/17 Day(s) Topic Videos/Readings Slides 2/14 KNN Regression and the Bias-Variance Tradeoff KNN Regression and the Bias-Variance Tradeoff ISLR: 2.2.2 for the bias-variance tradeoff; 3.5 for KNN regression PDF 2/16 Modeling Nonlinearity: Polynomial Regression and Splines Modeling Nonlinearity: Polynomial Regression and Splines ISLR: 7.1-7.4 PDF Homework 2 due Monday, 2/27 at 11:59pm Week 6: 2/20 - 2/24 Day(s) Topic Videos/Readings Slides 2/21 Local Regression and Generalized Additive Models Local Regression and Generalized Additive Models ISLR: 7.6-7.7 PDF 2/23 Quiz 2 Homework 2 due Monday, 2/27 at 11:59pm "],["learning-objectives.html", "Learning Objectives", " Learning Objectives Learning objectives for our course topics are listed below. Use these to guide your synthesis of video and reading material. Introduction to Statistical Machine Learning Formulate research questions that align with regression, classification, or unsupervised learning tasks Evaluating Regression Models Create and interpret residuals vs. fitted, residuals vs. predictor plots to identify improvements in modeling and address ethical concerns Interpret MSE, RMSE, MAE, and R-squared in a contextually meaningful way Overfitting and cross-validation Explain why training/in-sample model evaluation metrics can provide a misleading view of true test/out-of-sample performance Accurately describe all steps of cross-validation to estimate the test/out-of-sample version of a model evaluation metric Explain what role CV has in a predictive modeling analysis and its connection to overfitting Explain the pros/cons of higher vs. lower k in k-fold CV in terms of sample size and computing time Subset selection Clearly describe the forward and backward stepwise selection algorithm and why they are examples of greedy algorithms Compare best subset and stepwise algorithms in terms of optimality of output and computational time LASSO (shrinkage/regularization) Explain how ordinary and penalized least squares are similar and different with regard to (1) the form of the objective function and (2) the goal of variable selection Explain how the lambda tuning parameter affects model performance and how this is related to overfitting KNN Regression and the Bias-Variance Tradeoff Clearly describe / implement by hand the KNN algorithm for making a regression prediction Explain how the number of neighbors relates to the bias-variance tradeoff Explain the difference between parametric and nonparametric methods Explain how the curse of dimensionality relates to the performance of KNN (not in the video–will be discussed in class) Modeling Nonlinearity: Polynomial Regression and Splines Explain the advantages of splines over global transformations and other types of piecewise polynomials Explain how splines are constructed by drawing connections to variable transformations and least squares Explain how the number of knots relates to the bias-variance tradeoff Local Regression and Generalized Additive Models Clearly describe the local regression algorithm for making a prediction Explain how bandwidth (span) relate to the bias-variance tradeoff Describe some different formulations for a GAM (how the arbitrary functions are represented) Explain how to make a prediction from a GAM Interpret the output from a GAM Logistic regression Use a logistic regression model to make hard (class) and soft (probability) predictions Interpret non-intercept coefficients from logistic regression models in the data context Evaluating classification models Calculate (by hand from confusion matrices) and contextually interpret overall accuracy, sensitivity, and specificity Construct and interpret plots of predicted probabilities across classes Explain how a ROC curve is constructed and the rationale behind AUC as an evaluation metric Appropriately use and interpret the no-information rate to evaluate accuracy metrics Decision trees Clearly describe the recursive binary splitting algorithm for tree building for both regression and classification Compute the weighted average Gini index to measure the quality of a classification tree split Compute the sum of squared residuals to measure the quality of a regression tree split Explain how recursive binary splitting is a greedy algorithm Explain how different tree parameters relate to the bias-variance tradeoff Bagging and random forests Explain the rationale for bagging Explain the rationale for selecting a random subset of predictors at each split (random forests) Explain how the size of the random subset of predictors at each split relates to the bias-variance tradeoff Explain the rationale for and implement out-of-bag error estimation for both regression and classification Explain the rationale behind the random forest variable importance measure and why it is biased towards quantitative predictors (in class) K-means clustering Clearly describe / implement by hand the k-means algorithm Describe the rationale for how clustering algorithms work in terms of within-cluster variation Describe the tradeoff of more vs. less clusters in terms of interpretability Implement strategies for interpreting / contextualizing the clusters Hierarchical clustering Clearly describe / implement by hand the hierarchical clustering algorithm Compare and contrast k-means and hierarchical clustering in their outputs and algorithms Interpret cuts of the dendrogram for single and complete linkage Describe the rationale for how clustering algorithms work in terms of within-cluster variation Describe the tradeoff of more vs. less clusters in terms of interpretability Implement strategies for interpreting / contextualizing the clusters "],["r-and-rstudio-setup.html", "R and RStudio Setup Troubleshooting", " R and RStudio Setup Before class on Tuesday, Jan 24, you should follow these instructions to set up the software that we’ll be using throughout the semester. Even if you’ve already downloaded both R and RStudio, you’ll want to re-download to make sure that you have the most current versions. Highly recommended: Change the default file download location for your internet browser. Generally by default, internet browsers automatically save all files to the Downloads folder on your computer. This does not encourage good file organization practices. It is highly recommended that you change this option so that your browser asks you where to save each file before downloading it. This page has information on how to do this for the most common browsers. Required: Download R and RStudio FIRST: Download R here. You will see three links “Download R for …” Choose the link that corresponds to your computer. As of December 30, 2022, the latest version of R is 4.2.2. SECOND: Download RStudio here. Click the button under step 2 to install the version of RStudio recommended for your computer. As of December 30, 2022, the latest version of RStudio is 2022.12.0+353. Suggested: Watch this video made by Professor Lisa Lendway that describes essential configuration options for RStudio. Required: Install required packages. An R package is an extra bit of functionality that will help us in our data analysis efforts in a variety of ways. Open RStudio and click inside the Console pane (by default, the bottom left pane). Copy and paste the following command into the Console. You should see the text below appear to the right of the &gt;, which is called the R prompt. After you paste, hit Enter. install.packages(c(&quot;ggplot2&quot;, &quot;dplyr&quot;, &quot;readr&quot;, &quot;rmarkdown&quot;, &quot;broom&quot;, &quot;caret&quot;)) You will see a lot of text from status messages appearing in the Console as the packages are being installed. Wait until you see the &gt; again. Enter the command library(ggplot2) and hit enter. If you see the message Error in library(ggplot2) : there is no package called ggplot2, then there was a problem installing this package. Jump down to the Troubleshooting section below. (Any other messages that appear are fine, and a lack of any messages is also fine.) Repeat the above step for the commands: library(dplyr) library(readr) library(rmarkdown) library(broom) library(caret) Quit RStudio. You’re done setting up! Optional: For a refresher on RStudio features, watch this video. It also shows you how to customize the layout and color scheme of RStudio. Troubleshooting Problem: You are on a Mac and getting the following error: Error: package or namespace load failed for ‘ggplot2’ in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]): there is no package called ‘rlang’ Here’s how to fix it: First install the suite of Command Line Tools for Mac using the instructions here. Next enter install.packages(\"rlang\") in the Console. Finally check that entering library(ggplot2) gives no errors. You should be good now for dplyr, readr, and rmarkdown. "],["introductions.html", "Topic 1 Introductions Envisioning a Community of Learners Explorations", " Topic 1 Introductions Slides from today are available here. Envisioning a Community of Learners Directions: In your groups, please first introduce yourselves in whatever way you feel appropriate (e.g., preferred name, pronouns, how you’re feeling at the moment, things you’re looking forward to). When everyone is ready, discuss the 3 prompts below and record thoughts in this Google Doc. The instructor will summarize responses from both sections to create a resource that everyone can use. Prompts: Wonderful leaders in the Macalester community have complied the following set of agreements to consider when working in small groups. Which of the following do you think are most important to keep in mind for our time together in this course and why? Confidentiality: Take the lesson, not the story Use “I” statements; Speak your truth W.A.I.T. (Why Am I Talking/Why Aren’t I Talking) Listen for understanding Unpack assumptions Extend and receive grace Understand impact vs. intention Breathe and lean into discomfort Accept non-closure What strategies have you found work well for you to succeed both in and out of class in these times? What are some things that have contributed to positive experiences in your courses that you would like to see again this module? What has contributed to negative experiences that you would like to prevent? Explorations Each of the data contexts below prompts a broad research goal. For each, sharpen the focus of that goal by coming up with more targeted research questions that: Can be studied with a regression exploration Can be studied with a classification exploration Can be studied with an unsupervised learning exploration Are there harms that you anticipate arising from the collection of data or its analysis? Context 1: The New York Times is trying to better understand the popularity of its different articles with the hope of using this understanding to improve hiring of writers and improve their website layout. Context 2: The Minnesota Department of Health is trying to better understand the different health trajectories of people who have contracted a certain illness to improve funding for relevant health services. Context 3: The campaign management team for a political candidate is trying to better understand voter turnout in different regions of the country to run a better campaign in the next election. "],["evaluating-regression-models.html", "Topic 2 Evaluating Regression Models Learning Goals Exercises", " Topic 2 Evaluating Regression Models Learning Goals Create and interpret residuals vs. fitted, residuals vs. predictor plots to identify improvements in modeling and address ethical concerns Interpret MSE, RMSE, MAE, and R-squared in a contextually meaningful way We’ll also start to develop some new ideas relating to our next topics. Slides from today are available here. Exercises You can download a template RMarkdown file to start from here. Context We’ll be working with a dataset containing physical measurements on 80 adult males. These measurements include body fat percentage estimates as well as body circumference measurements. fatBrozek: Percent body fat using Brozek’s equation: 457/Density - 414.2 fatSiri: Percent body fat using Siri’s equation: 495/Density - 450 density: Density determined from underwater weighing (gm/cm^3). age: Age (years) weight: Weight (lbs) height: Height (inches) neck: Neck circumference (cm) chest: Chest circumference (cm) abdomen: Abdomen circumference (cm) hip: Hip circumference (cm) thigh: Thigh circumference (cm) knee: Knee circumference (cm) ankle: Ankle circumference (cm) biceps: Biceps (extended) circumference (cm) forearm: Forearm circumference (cm) wrist: Wrist circumference (cm) It takes a lot of effort to estimate body fat percentage accurately through underwater weighing. The goal is to build the best predictive model for fatSiri using just circumference measurements, which are more easily attainable. (We won’t use fatBrozek or density as predictors because they’re other outcome variables.) library(readr) library(ggplot2) library(dplyr) library(broom) library(tidymodels) tidymodels_prefer() bodyfat &lt;- read_csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) # Remove the fatBrozek and density variables bodyfat &lt;- bodyfat %&gt;% select(-fatBrozek, -density) Class investigations We’ll work through this section together to review concepts and code. You’ll then work on the remainder of the exercises in your groups. # Exploratory plots # Univariate distribution of outcome ggplot(bodyfat, aes(???)) + geom_???() # Scatterplot of fatSiri vs. weight ggplot(bodyfat, aes(???)) + geom_???() Let’s fit a linear regression model to predict body fat percentage from weight. mod1 &lt;- lm(??? ~ ???, data = bodyfat) We can use the augment() function from the broom package to augment our original data with useful information from the model fitting. In particular, residuals are stored in the .fitted column, and fitted (predicted) values for the cases supplied in newdata are stored in the .fitted column. mod1_output &lt;- broom::augment(mod1, newdata = bodyfat) head(mod1_output) We can use the augment() output to compute error metrics, and glance() to obtain R^2: mod1 %&gt;% augment() %&gt;% summarize( mse = mean((fatSiri - .fitted)^2), rmse = sqrt(mse), mae = mean(abs(fatSiri - .fitted)) ) # R-squared - interpretation? (unit-less) mod1 %&gt;% glance() %&gt;% select(r.squared) …and to create residual plots: # Univariate plot of residuals mod1 %&gt;% augment() %&gt;% ggplot(aes(x = ???)) + geom_histogram() + theme_minimal() # Fitted vs. Residual plot mod1 %&gt;% augment() %&gt;% ggplot(aes(x = ???, y = ???)) + geom_point() + geom_smooth() + geom_hline(yintercept = 0) + theme_minimal() # Predictor vs. Residual plot mod1 %&gt;% augment() %&gt;% left_join(bodyfat) %&gt;% #Merge the remaining variables into data set ggplot(aes(x = height, y = ???)) + #note patterns in residual/error with height geom_point() + geom_smooth() + geom_hline(yintercept = 0) + theme_minimal() Exercise 1 First decide on what you think would be a good model by picking variables based on context. Fit this model, calling it mod_initial. (Remember that you can include several predictors with a + in the model formula - like y ~ x1+x2.) # Code to fit initial model Use residual plot explorations to check if you need to update your model. # Residual plot explorations Fit your updated model, and call it model_updated. # Code to fit updated model Exercise 2 Compute and contextually interpret relevant evaluation metrics for your model. # Code to compute evaluation metrics Exercise 3 Now that you’ve selected your best model, deploy it in the real world by applying it to a new set of 172 adult males. You’ll need to update the newdata argument of the broom::augment() code to use bodyfat_test instead of bodyfat. bodyfat_test &lt;- read_csv(&quot;https://www.dropbox.com/s/7gizws208u0oywq/bodyfat_test.csv?dl=1&quot;) Compare your evaluation metrics from Exercise 2 the metrics here. What do you notice? (Note: this observation is just based on your one particular fitted model. You’ll make some more comprehensive observations in the next exercise.) In general, do you think that models with more or less variables would perform better in this comparison? Explain. Exercise 4 The code below systematically looks at the same comparison that you made in Exercise 3 but for every possible linear regression model formed from inclusion/exclusion of the predictors (without transformations or interactions). Run the code to make a plot of the results of this systematic investigation. (Feel free to inspect the code if you’re curious, but otherwise, don’t worry about understanding it fully.) What do you notice? What do you wonder? get_maes &lt;- function(mod, train_data, test_data) { mod_output_train &lt;- broom::augment(mod, newdata = train_data) mod_output_test &lt;- broom::augment(mod, newdata = test_data) train_mae &lt;- mean(abs(mod_output_train$.resid)) test_mae &lt;- mean(abs(mod_output_test$.resid)) c(train_mae, test_mae) } possible_predictors &lt;- setdiff(colnames(bodyfat), c(&quot;fatSiri&quot;, &quot;hipin&quot;)) results &lt;- bind_rows(lapply(1:13, function(i) { combos &lt;- combn(possible_predictors, i) bind_rows(lapply(seq_len(ncol(combos)), function(j) { formula &lt;- paste(&quot;fatSiri ~&quot;, paste(combos[,j], collapse = &quot;+&quot;)) mod &lt;- lm(as.formula(formula), data = bodyfat) maes &lt;- get_maes(mod = mod, train_data = bodyfat, test_data = bodyfat_test) tibble( form = formula, train_mae = maes[1], test_mae = maes[2], num_predictors = i ) })) })) # Relabel the num_predictors variable results &lt;- results %&gt;% mutate(num_predictors = paste(&quot;# predictors:&quot;, num_predictors)) %&gt;% mutate(num_predictors = factor(num_predictors, levels = paste(&quot;# predictors:&quot;, 1:13))) # Plot results ggplot(results, aes(x = train_mae, y = test_mae, color = num_predictors)) + geom_point() + coord_cartesian(xlim = c(0,7.5), ylim = c(0,7.5)) + geom_abline(slope = 1, intercept = 0) + facet_wrap(~num_predictors) + guides(color = FALSE) + labs(x = &quot;MAE on original data&quot;, y = &quot;MAE on new data on 172 men&quot;) + theme_classic() "],["overfitting.html", "Topic 3 Overfitting Learning Goals The tidymodels package Exercises", " Topic 3 Overfitting Learning Goals Explain why training/in-sample model evaluation metrics can provide a misleading view of true test/out-of-sample performance Implement testing and training sets in R using the tidymodels package Slides from today are available here. The tidymodels package (If you have not already installed the tidymodels package, install it with install.packages(\"tidymodels\").) Over this course, we will looking at a broad but linked set of specialized tools applied in statistical machine learning. Specialized tools generally require specialized code. Each tool has been developed separately and coded in a unique way. In order to facilitate and streamline the user experience, there have been attempts at creating a uniform interface, such as the caret R package. The developers of the caret package are no longer maintaining those packages. They are working on a newer package, called tidymodels. In this class, we will use the tidymodels package, which uses the tidyverse syntax you learned in Stat 155. The tidymodels package is a relatively new package and continues to be developed as we speak. This means that I’m learning with you and in a month or two, there may be improved functionality. As Prof. Heggeseth introduced in the R code videos, we have a general workflow structure that includes a model specification and a recipe (formula + preprocessing steps). # Load the package library(tidymodels) tidymodels_prefer() # Set the seed for the random number generator set.seed(123) # Specify Model model_spec &lt;- linear_reg() %&gt;% # type of model set_engine(engine = ____) #%&gt;% # algorithm to fit the model set_args(__) %&gt;% # hyperparameters/tuning parameters are needed for some models set_mode(__) # regression or classification # Specify Recipe (if you have preprocessing steps) rec &lt;- recipe(formula, data) %&gt;% step_{FILLIN}() %&gt;% # e.g., step_filter() to subset the rows step_{FILLIN}() # e.g., step_lincomb() to remove all predictors which are perfect linear combinations of another # Create Workflow (Model + Recipe) model_wf &lt;- workflow() %&gt;% add_recipe(rec) %&gt;% #or add_formula() add_model(model_spec) We can fit that workflow to training data. # Fit Model to training data (without a recipe) fit_model &lt;- fit(model_spec, formula, data_train) # Fit Model &amp; Recipe to training data fit_model &lt;- fit(model_wf, data_train) And then we can evaluate that fit model on testing data (new data that has not been used to fit the model). # Evaluate on testing data model_output &lt;- fit_model %&gt;% predict(new_data = data_test) %&gt;% # this function will apply recipe to new_data and do prediction bind_cols(data_test) reg_metrics &lt;- metric_set(rmse, rsq, mae) model_output %&gt;% reg_metrics(truth = __, estimate = .pred) The power of tidymodels is that it allows us to streamline the vast world of machine learning techniques into one common syntax. On top of \"lm\", there are many other different machine learning methods that we can use. In the exercises below, you’ll need to adapt the code above to fit a linear regression model (engine = \"lm\"). Exercises You can download a template RMarkdown file to start from here. Context We’ll be working with a dataset containing physical measurements on 80 adult males. These measurements include body fat percentage estimates as well as body circumference measurements. fatBrozek: Percent body fat using Brozek’s equation: 457/Density - 414.2 fatSiri: Percent body fat using Siri’s equation: 495/Density - 450 density: Density determined from underwater weighing (gm/cm^3). age: Age (years) weight: Weight (lbs) height: Height (inches) neck: Neck circumference (cm) chest: Chest circumference (cm) abdomen: Abdomen circumference (cm) hip: Hip circumference (cm) thigh: Thigh circumference (cm) knee: Knee circumference (cm) ankle: Ankle circumference (cm) biceps: Biceps (extended) circumference (cm) forearm: Forearm circumference (cm) wrist: Wrist circumference (cm) It takes a lot of effort to estimate body fat percentage accurately through underwater weighing. The goal is to build the best predictive model for fatSiri using just circumference measurements, which are more easily attainable. (We won’t use fatBrozek or density as predictors because they’re other outcome variables.) library(readr) library(ggplot2) library(dplyr) library(broom) library(tidymodels) bodyfat_train &lt;- read_csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) # Remove the fatBrozek, density, and hipin variables bodyfat_train &lt;- bodyfat_train %&gt;% select(-fatBrozek, -density, -hipin) Exercise 1: 5 models Consider the 5 models below: lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &#39;lm&#39;) %&gt;% set_mode(&#39;regression&#39;) mod1 &lt;- fit(lm_spec, fatSiri ~ age+weight+neck+abdomen+thigh+forearm, data = bodyfat_train) mod2 &lt;- fit(lm_spec, fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps, data = bodyfat_train) mod3 &lt;- fit(lm_spec, fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps+chest+hip, data = bodyfat_train) mod4 &lt;- fit(lm_spec, fatSiri ~ ., # The . means all predictors data = bodyfat_train) bf_recipe &lt;- recipe(fatSiri ~ ., data = bodyfat_train) %&gt;% step_normalize(all_numeric_predictors()) bf_wf &lt;- workflow() %&gt;% add_recipe(bf_recipe) %&gt;% add_model(lm_spec) mod5 &lt;- fit(bf_wf, data = bodyfat_train) STAT 155 review: Look at the tidy() of mod1. Contextually interpret the coefficient for the weight predictor. Is anything surprising? Why might this be? Explain how mod5 is different than mod4. You may want to look at bf_recipe %&gt;% prep(bodyfat_train) %&gt;% juice() to see the preprocessed training data. Which model will have the lowest training RMSE, and why? Explain before calculating (that is part d). Compute the training RMSE for models 1 through 5 to check your answer for part c. Write a sentence interpreting one of values of RMSE in context. Below is a coding example to get you started. reg_metrics = metric_set(rmse) mod1 %&gt;% predict(new_data = bodyfat_train) %&gt;% reg_metrics(truth = bodyfat_train$fatSiri, estimate = .pred) Which model do you think is the “best”? You may calculate MAE and R squared as well to justify your answer. f, Which model do you think will perform worst on new test data? Why? Exercise 2: Evaluating the Test Data Now that you’ve thought about how well the models might perform on test data, deploy it in the real world by applying it to a new set of 172 adult males. You’ll need to update the new_data to use bodyfat_test instead of bodyfat_train. bodyfat_test &lt;- read_csv(&quot;https://www.dropbox.com/s/7gizws208u0oywq/bodyfat_test.csv?dl=1&quot;) Calculate the test RMSE, MAE, and R squared for all five of the models. Here is some code to get you started. # Use fit/trained models and evaluate on test data reg_metrics = metric_set(rmse, mae, rsq) mod1 %&gt;% predict(new_data = bodyfat_test) %&gt;% reg_metrics(truth = bodyfat_test$fatSiri, estimate = .pred) Look back to Exercise 1 and see which model you thought was “best” based on the training data. Is that the “best” model in terms of predicting on new data? Explain. In “real life” we only have one data set. To get a sense of predictive performance on new test data, we could split our data into two groups. Discuss pros and cons of ways you might split the data. How big should the training set be? How big should the testing set be? Exercise 3: Overfitting If you have time, consider the following relationship. Imagine a set of predictions that is overfit to this training data. You are not limited to lines. Draw a picture of that function of predictions on a piece of paper. set.seed(123) data_train &lt;- tibble::tibble( x = runif(15,0,7), y = x^2 + rnorm(15,sd = 7) ) data_train %&gt;% ggplot(aes(x = x, y = y)) + geom_point() + theme_classic() "],["cross-validation.html", "Topic 4 Cross-validation Learning Goals Exercises", " Topic 4 Cross-validation Learning Goals Explain why training/in-sample model evaluation metrics can provide a misleading view of true test/out-of-sample performance Accurately describe all steps of cross-validation to estimate the test/out-of-sample version of a model evaluation metric Explain what role CV has in a predictive modeling analysis and its connection to overfitting Explain the pros/cons of higher vs. lower k in k-fold CV in terms of sample size and computing time Implement cross-validation in R using the tidymodels package Slides from today are available here. Exercises You can download a template RMarkdown file to start from here. Context We’ll be working with a dataset containing physical measurements on 80 adult males. These measurements include body fat percentage estimates as well as body circumference measurements. fatBrozek: Percent body fat using Brozek’s equation: 457/Density - 414.2 fatSiri: Percent body fat using Siri’s equation: 495/Density - 450 density: Density determined from underwater weighing (gm/cm^3). age: Age (years) weight: Weight (lbs) height: Height (inches) neck: Neck circumference (cm) chest: Chest circumference (cm) abdomen: Abdomen circumference (cm) hip: Hip circumference (cm) thigh: Thigh circumference (cm) knee: Knee circumference (cm) ankle: Ankle circumference (cm) biceps: Biceps (extended) circumference (cm) forearm: Forearm circumference (cm) wrist: Wrist circumference (cm) It takes a lot of effort to estimate body fat percentage accurately through underwater weighing. The goal is to build the best predictive model for fatSiri using just circumference measurements, which are more easily attainable. (We won’t use fatBrozek or density as predictors because they’re other outcome variables.) library(readr) library(ggplot2) library(dplyr) library(broom) library(tidymodels) bodyfat_train &lt;- read_csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) # Remove the fatBrozek, density, and hipin variables bodyfat_train &lt;- bodyfat_train %&gt;% select(-fatBrozek, -density, -hipin) and consider the first four models we built on Thursday: lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &#39;lm&#39;) %&gt;% set_mode(&#39;regression&#39;) mod1 &lt;- fit(lm_spec, fatSiri ~ age+weight+neck+abdomen+thigh+forearm, data = bodyfat_train) mod2 &lt;- fit(lm_spec, fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps, data = bodyfat_train) mod3 &lt;- fit(lm_spec, fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps+chest+hip, data = bodyfat_train) mod4 &lt;- fit(lm_spec, fatSiri ~ ., # The . means all predictors data = bodyfat_train) Exercise 1: Cross-validation in Concept We are going to repeat what we did last week but use cross-validation to help us evaluate models in terms of the predictive performance. Explain to your table-mates the steps of cross validation (CV) in concept and then how you might use 10-fold CV with these 80 individual data points. Exercise 2: Cross-validation with tidymodels Complete the code below to perform 10-fold cross-validation for mod1 to estimate the test RMSE (\\(\\text{CV}_{(10)}\\)). Do we need to use set.seed()? Why or why not? (Is there a number of folds for which we would not need to set the seed?) # Do we need to use set.seed()? set.seed(2023) bodyfat_cv &lt;- vfold_cv(??, v = 10) model_wf &lt;- workflow() %&gt;% add_formula(??) %&gt;% add_model(lm_spec) mod1_cv &lt;- fit_resamples(model_wf, resamples = bodyfat_cv, metrics = metric_set(rmse, rsq, mae) ) Run the code below, and use this to calculate the 10-fold cross-validated RMSE “by hand” (you can use R code, but apply the formula mathematically). mod1_cv %&gt;% unnest(.metrics) Run the code below, and compare your answer to part b. mod1_cv %&gt;% collect_metrics() Exercise 3: Looking at the evaluation metrics Perform 10-fold CV using mod2, mod3, and mod4 by running the code below: model2_wf &lt;- workflow() %&gt;% add_formula(fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps) %&gt;% add_model(lm_spec) mod2_cv &lt;- fit_resamples(model2_wf, resamples = bodyfat_cv, metrics = metric_set(rmse, rsq, mae) ) model3_wf &lt;- workflow() %&gt;% add_formula(fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps+chest+hip) %&gt;% add_model(lm_spec) mod3_cv &lt;- fit_resamples(model3_wf, resamples = bodyfat_cv, metrics = metric_set(rmse, rsq, mae) ) model4_wf &lt;- workflow() %&gt;% add_formula(fatSiri ~ .) %&gt;% add_model(lm_spec) mod4_cv &lt;- fit_resamples(model4_wf, resamples = bodyfat_cv, metrics = metric_set(rmse, rsq, mae) ) mod1_cv %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;rmse&quot;) mod2_cv %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;rmse&quot;) mod3_cv %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;rmse&quot;) mod4_cv %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;rmse&quot;) Look at the completed table below of evaluation metrics for the 4 models. Model Training RMSE \\(\\text{CV}_{(10)}\\) mod1 3.811 4.193 mod2 3.767 4.305 mod3 3.752 4.368 mod4 3.572 4.438 Which model performed the best on the training data? Which model performed best on the test set? Explain why there’s a discrepancy between these 2 answers and why CV, in general, can help reduce the impact overfitting. Exercise 4: Practical issues: choosing \\(k\\) In terms of sample size, what are the pros/cons of low vs. high \\(k\\)? In terms of computational time, what are the pros/cons of low vs. high \\(k\\)? If possible, it is advisable to choose \\(k\\) to be a divisor of the sample size. Why do you think that is? Digging deeper If you have time, consider these exercises to further explore concepts related to today’s ideas. Consider leave-one-out-cross-validation (LOOCV) Would two different seeds make a difference in the results (using set.seed)? Why or why not? Using the information from your_output %&gt;% unnest(.metrics), construct a visualization to examine the variability of RMSE from case to case. What might explain any large values? What does this highlight about the quality of estimation of LOOCV? "],["variable-subset-selection.html", "Topic 5 Variable Subset Selection Learning Goals Exercises", " Topic 5 Variable Subset Selection Learning Goals Clearly describe the forward and backward stepwise selection algorithm and why they are examples of greedy algorithms Compare best subset and stepwise algorithms in terms of optimality of output and computational time Describe how selection algorithms can give a measure of variable importance Exercises You can download a template RMarkdown file to start from here. We’ll continue using the body fat dataset to explore subset selection methods. library(caret) library(ggplot2) library(dplyr) library(readr) bodyfat &lt;- read_csv(&quot;http://www.macalester.edu/~ajohns24/data/bodyfatsub.csv&quot;) # Take out the redundant Density and HeightFt variables bodyfat &lt;- bodyfat %&gt;% select(-Density, -HeightFt) Exercise 1: Backward stepwise selection: by hand In the backward stepwise procedure, we start with the full model, full_model, with all predictors: full_model &lt;- lm(BodyFat ~ Age + Weight + Height + Neck + Chest + Abdomen + Hip + Thigh + Knee + Ankle + Biceps + Forearm + Wrist, data = bodyfat) To practice the backward selection algorithm, step through a few steps of the algorithm using p-values as a selection criterion: Identify which predictor contributes the least to the model. One (problematic) approach is to identify the least significant predictor. Fit a new model which eliminates this predictor. Identify the least significant predictor in this model. Fit a new model which eliminates this predictor. Repeat 1 more time to get the hang of it. (We discussed in the video how the use of p-values for selection is problematic, but for now you’re just getting a handle on the algorithm. You’ll think about the problems with p-values in the next exercise.) Exercise 2: Interpreting the results Examine which predictors remain after the previous exercise. Are you surprised that, for example, Wrist is still in the model but Weight is not? Does this mean that Wrist is a better predictor of body fat percentage than Weight is? What statistical idea is relevant here? Exercise 3: Planning forward selection using CV Using p-values to perform stepwise selection has problems, as was discussed in the video. A better alternative to target predictive accuracy is to evaluate the models using cross-validation. Fully outline the steps required to use cross-validation to perform forward selection. Make sure to provide enough detail such that the stepwise selection and CV algorithms are made clear. Exercise 4: Stepwise selection in caret Run install.packages(\"leaps\") in the Console to install the leaps package before proceeding. Complete the caret code below to perform backward stepwise selection with cross-validation. The following points will help you complete the code: In R model formulas, y ~ . sets y as the outcome and all other predictors in the dataset as predictors. The specific method name for backward selection is \"leapBackward\". The tuneGrid argument is already filled in. It allows us to input tuning parameters into the fitting process. The tuning parameters for subset selection are the number of variables included in the models explored (nvmax). This can vary from 1 to 13 (the maximum number of predictors possible). Use 10-fold CV to estimate test performance of the models. Use \"MAE\" as the evaluation metric to choose how the best of the 1-variable, 2-variable, etc. models will be chosen. (Note: CV is only used to pick among the best 1, 2, 3, …, and 13 variable models. To find the best 1, 2, 3, …, and 13 variable models, training MSE is used. caret uses training MSE because within a subset size, all models have the same number of coefficients, which makes both ordinary R-squared and training MSE ok for comparing models.) set.seed(23) back_step_mod &lt;- train( y ~ x, data = bodyfat, method = ___, tuneGrid = data.frame(nvmax = 1:13), trControl = ___, metric = ___, na.action = na.omit ) Exercise 5: Exploring the results There are a number of ways to examine and use the output of the selection algorithm, which we’ll explore here. (It would be useful to make notes along the way - perhaps on your code note sheet.) Part a Let’s first examine the sequence of models explored. The stars in the table at the bottom indicate the variables included in the 1-variable, 2-variable, etc. models. summary(back_step_mod) Of the 13 models in the sequence, R only prints out the 11 smallest models (since, for reasons we’ll discuss below, it determines the 11 predictor model to be “best”). Which predictor is the last to remain in the model? Second-to-last to remain? How do you think we could use these results to identify which predictors were most/least important in predicting the outcome of body fat percentage? Part b Examine the 10-fold CV MAE for each of the 13 models in the backward stepwise sequence: # Plot metrics for each model in the sequence plot(back_step_mod) # Look at accuracy/error metrics for the different subset sizes back_step_mod$results Which size model has the lowest CV MAE? Which size model would you pick? Why? Part c In our model code, we used selectionFunction = \"best\" by default inside trainControl(). By doing so, we indicated that we wanted to find which model minimizes the CV MAE (i.e., has the “best” MAE). With respect to this criterion: # What tuning parameter gave the best performance? # i.e. What subset size gave the best model? back_step_mod$bestTune # Obtain the coefficients for the best model coef(back_step_mod$finalModel, id = back_step_mod$bestTune$nvmax) # Obtain the coefficients of any size model with at most as many variables as the overall best model (e.g., the 2-predictor model) coef(back_step_mod$finalModel, id = 2) Another sensible choice for the selection function is to not choose the model with the lowest estimated error but to account for the uncertainty in the estimation of that test error by picking the smallest model for which the CV MAE is within one standard error of the minimum CV MAE. What do you think the rationale for this is? (We’ll explore the code for this formally later, or you can try it out below in the Digging Deeper section.) Part d We should end by evaluating our final chosen model. Contextually interpret (with units) the CV MAE for the model. Make residual plots for the chosen model in one of 2 ways: (1) use lm() to fit the model with the chosen predictors or (2) use the following code to create a dataset called back_step_mod_out which contains the original data as well as predicted values and residuals (fitted and resid). back_step_mod_out &lt;- bodyfat %&gt;% mutate( fitted = predict(back_step_mod, newdata = bodyfat), resid = BodyFat - fitted ) Digging deeper As mentioned in Exercise 5c, we have another choice for the selectionFunction used to choose from many possible models. The use of selectionFunction = \"oneSE\" below picks the simplest model for which the CV MAE is within one standard error of the minimum CV MAE. Compare the output you obtain here with your best model from Exercise 5. back_step_mod_1se &lt;- train( BodyFat ~ ., data = bodyfat, method = &quot;leapBackward&quot;, tuneGrid = data.frame(nvmax = 1:13), trControl = trainControl(method = &quot;cv&quot;, number = 10, selectionFunction = &quot;oneSE&quot;), metric = &quot;MAE&quot;, na.action = na.omit ) Forward selection can be implemented in caret with analogous code, except that the method name is \"leapForward\". Try implementing forward selection and comparing your results. "],["lasso-shrinkageregularization.html", "Topic 6 LASSO: Shrinkage/Regularization Learning Goals LASSO models in tidymodels Exercises", " Topic 6 LASSO: Shrinkage/Regularization Learning Goals Explain how ordinary and penalized least squares are similar and different with regard to (1) the form of the objective function and (2) the goal of variable selection Explain why variable scaling is important for the performance of shrinkage methods Explain how the lambda tuning parameter affects model performance and how this is related to overfitting Describe how output from LASSO models can give a measure of variable importance Slides from today are available here. LASSO models in tidymodels To build LASSO models in tidymodels, first load the package and set the seed for the random number generator to ensure reproducible results: library(dplyr) library(readr) library(broom) library(ggplot2) library(tidymodels) tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions set.seed(___) # Pick your favorite number to fill in the parentheses Then adapt the following code: # Lasso Model Spec lm_lasso_spec &lt;- linear_reg() %&gt;% set_args(mixture = 1, penalty = 0) %&gt;% ## mixture = 1 indicates Lasso, we&#39;ll choose penalty later set_engine(engine = &#39;glmnet&#39;) %&gt;% # glmnet does regularization (LASSO, ridge, elastic net) set_mode(&#39;regression&#39;) # Recipe with standardization (!) data_rec &lt;- recipe( ___ ~ ___ , data = ___) %&gt;% step_nzv(all_predictors()) %&gt;% # removes variables with the same value step_novel(all_nominal_predictors()) %&gt;% # important if you have rare categorical variables step_normalize(all_numeric_predictors()) %&gt;% # important standardization step for LASSO step_dummy(all_nominal_predictors()) # creates indicator variables for categorical variables # Workflow (Recipe + Model) lasso_wf &lt;- workflow() %&gt;% add_recipe(data_rec) %&gt;% add_model(lm_lasso_spec) # Fit Model lasso_fit &lt;- lasso_wf %&gt;% fit(data = ___) # Fit to data Examining the LASSO model for each \\(\\lambda\\) The glmnet engine fits models for each \\(\\lambda\\) automatically, so we can visualize the estimates for each penalty value. plot(lasso_fit %&gt;% extract_fit_parsnip() %&gt;% pluck(&#39;fit&#39;), # way to get the original glmnet output xvar = &quot;lambda&quot;) # glmnet fits the model with a variety of lambda penalty values Identifying the “best” LASSO model To identify the best model, we need to tune the model using cross validation. Adapt the following code to tune a Lasso Model to choose Lambda: # Create CV folds data_cv10 &lt;- vfold_cv(___, v = 10) # Lasso Model Spec with tune lm_lasso_spec_tune &lt;- linear_reg() %&gt;% set_args(mixture = 1, penalty = tune()) %&gt;% ## mixture = 1 indicates Lasso set_engine(engine = &#39;glmnet&#39;) %&gt;% #note we are using a different engine set_mode(&#39;regression&#39;) # Workflow (Recipe + Model) lasso_wf_tune &lt;- workflow() %&gt;% add_recipe(data_rec) %&gt;% add_model(lm_lasso_spec_tune) # Tune Model (trying a variety of values of Lambda penalty) penalty_grid &lt;- grid_regular( penalty(range = c(-5, 3)), #log10 transformed 10^-5 to 10^3 levels = 30) tune_res &lt;- tune_grid( # new function for tuning parameters lasso_wf_tune, # workflow resamples = data_cv10, # cv folds metrics = metric_set(rmse, mae), grid = penalty_grid # penalty grid defined above ) # Visualize Model Evaluation Metrics from Tuning autoplot(tune_res) + theme_classic() # Summarize Model Evaluation Metrics (CV) collect_metrics(tune_res) %&gt;% filter(.metric == &#39;rmse&#39;) %&gt;% # or choose mae select(penalty, rmse = mean) best_penalty &lt;- select_best(tune_res, metric = &#39;rmse&#39;) # choose penalty value based on lowest mae or rmse # Fit Final Model final_wf &lt;- finalize_workflow(lasso_wf_tune, best_penalty) # incorporates penalty value to workflow final_fit &lt;- fit(final_wf, data = ___) tidy(final_fit) Exercises You can download a template RMarkdown file to start from here. We’ll use a new data set to explore LASSO modeling. This data comes from the US Department of Energy. You will predict the fuel efficiency of modern cars from characteristics of these cars, like transmission and engine displacement. Fuel efficiency is a numeric value that ranges smoothly from about 15 to 40 miles per gallon. library(dplyr) library(readr) library(broom) library(ggplot2) library(tidymodels) tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions set.seed(123) cars2018 &lt;- read_csv(&quot;https://raw.githubusercontent.com/juliasilge/supervised-ML-case-studies-course/master/data/cars2018.csv&quot;) head(cars2018) # Cleaning cars2018 &lt;- cars2018 %&gt;% select(-model_index) Exercise 1: A least squares model Let’s start by building an ordinary (not penalized) least squares model to review important concepts. We’ll fit a model to predict fuel efficiency measured in miles per gallon (mpg) with all possible predictors. lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &#39;lm&#39;) %&gt;% set_mode(&#39;regression&#39;) full_rec &lt;- recipe(mpg ~ ., data = cars2018) %&gt;% update_role(model, new_role = &#39;ID&#39;) %&gt;% # we want to keep the name of the car model but not as a predictor or outcome step_nzv(all_predictors()) %&gt;% # removes variables with the same value step_normalize(all_numeric_predictors()) %&gt;% # important standardization step for LASSO step_dummy(all_nominal_predictors()) # creates indicator variables for categorical variables full_lm_wf &lt;- workflow() %&gt;% add_recipe(full_rec) %&gt;% add_model(lm_spec) full_model &lt;- fit(full_lm_wf, data = cars2018) full_model %&gt;% tidy() Use tidymodels to perform 10-fold cross-validation to estimate test MAE for this model. How do you think the estimated test error would change with fewer predictors? This model fit with ordinary least squares corresponds to a special case of penalized least squares. What is the value of \\(\\lambda\\) in this special case? As \\(\\lambda\\) increases, what would you expect to happen to the number of predictors that remain in the model? Exercise 2: Fitting a LASSO model in tidymodels Adapt our general LASSO code to fit a set of LASSO models with the following parameters: Use 10-fold CV. Use mean absolute error (MAE) to select a final model. Select the simplest model for which the metric is within one standard error of the best metric. Use a sequence of 30 \\(\\lambda\\) values from 0.001 to 1. Before running the code, enter install.packages(\"glmnet\") in the Console. Save the CV-fit models from tune_grid() as tune_output. # Fit LASSO models for a grid of lambda values # Tune and fit a LASSO model to the data (with CV) set.seed(2023) # Create CV folds data_cv10 &lt;- vfold_cv(??, v = 10) # Lasso Model Spec with tune lm_lasso_spec_tune &lt;- linear_reg() %&gt;% set_args(mixture = 1, penalty = tune()) %&gt;% ## mixture = 1 indicates Lasso set_engine(engine = &#39;glmnet&#39;) %&gt;% #note we are using a different engine set_mode(&#39;regression&#39;) # Workflow (Recipe + Model) lasso_wf_tune &lt;- workflow() %&gt;% add_recipe(full_rec) %&gt;% # recipe defined above add_model(lm_lasso_spec_tune) # Tune Model (trying a variety of values of Lambda penalty) penalty_grid &lt;- grid_regular( penalty(range = c(??, ??)), #log10 transformed (e.g., put &#39;-3&#39; for 0.001) levels = ??) tune_output &lt;- tune_grid( # new function for tuning parameters lasso_wf_tune, # workflow resamples = data_cv10, # cv folds metrics = metric_set(rmse, mae), grid = penalty_grid # penalty grid defined above ) Let’s visualize the model evaluation metrics from tuning. We can use autoplot(). # Visualize Model Evaluation Metrics from Tuning autoplot(tune_output) + theme_classic() Inspect the shape of the plot. The errors go down at the very beginning then start going back up. Why do you think this happens? (This is an example of a very important idea that we’ll see shortly: the bias-variance tradeoff.) Next, we need to choose the lambda that leads to the best model. We can choose the lambda penalty value that leads to the lowest cross-validated MAE or we can take into account the variation of the cross-validated MAE and choose the largest lambda penalty value that is within 1 standard error of the lowest cross-validated MAE. How might the models that result from these two penalties differ? best_penalty &lt;- select_best(tune_output, metric = &#39;mae&#39;) # choose penalty value based on lowest cross-validated MAE best_penalty best_se_penalty &lt;- select_by_one_std_err(tune_output, metric = &#39;mae&#39;, desc(penalty)) # choose largest penalty value within 1 se of the lowest cross-validated MAE best_se_penalty Now check your understanding by fitting both “final” models and comparing the coefficients. How are these two models different? # Fit Final Model final_wf &lt;- finalize_workflow(lasso_wf_tune, best_penalty) # incorporates penalty value to workflow final_wf_se &lt;- finalize_workflow(lasso_wf_tune, best_se_penalty) # incorporates penalty value to workflow final_fit &lt;- fit(final_wf, data = cars2018) final_fit_se &lt;- fit(final_wf_se, data = cars2018) tidy(final_fit) tidy(final_fit_se) Exercise 3: Examining output: plot of coefficient paths Once we’ve used cross validation, a useful plot allows us to examine coefficient paths resulting from the final fitted LASSO models: coefficient estimates as a function of \\(\\lambda\\). Before running the code, run install.packages(“stringr”) and install.packages(“purrr”) in the Console. glmnet_output &lt;- final_fit_se %&gt;% extract_fit_parsnip() %&gt;% pluck(&#39;fit&#39;) # get the original glmnet output lambdas &lt;- glmnet_output$lambda coefs_lambdas &lt;- coefficients(glmnet_output, s = lambdas ) %&gt;% as.matrix() %&gt;% t() %&gt;% as.data.frame() %&gt;% mutate(lambda = lambdas ) %&gt;% select(lambda, everything(), -`(Intercept)`) %&gt;% pivot_longer(cols = -lambda, names_to = &quot;term&quot;, values_to = &quot;coef&quot;) %&gt;% mutate(var = purrr::map_chr(stringr::str_split(term,&quot;_&quot;),~.[1])) coefs_lambdas %&gt;% ggplot(aes(x = lambda, y = coef, group = term, color = var)) + geom_line() + geom_vline(xintercept = best_se_penalty %&gt;% pull(penalty), linetype = &#39;dashed&#39;) + theme_classic() + theme(legend.position = &quot;bottom&quot;, legend.text=element_text(size=8)) There’s a lot of information in this plot! Each colored line corresponds to a different predictor. The x-axis reflects the range of different \\(\\lambda\\) values. At each \\(\\lambda\\), the y-axis reflects the coefficient estimates for the predictors in the corresponding LASSO model. The vertical dashed line shows where the best penalty value (using the SE method) based on cross-validated MAE. Why do all of the lines head toward y = 0 on the far right of the plot? What variables seem to be more “important” or “persistent” (persistently present in the model) variable? Does this make sense in context? Exercise 4: Examining and evaluating the best LASSO model. Take a look at the predictors and coefficients for the “best” LASSO model. Are the predictors that remain in the model sensible? Do the coefficient signs make sense? # Obtain the predictors and coefficients of the &quot;best&quot; model # Filter out the coefficient are 0 final_fit_se %&gt;% tidy() %&gt;% filter(estimate != 0) Evaluate the best LASSO model: Contextually interpret (with units) the cross-validated MAE error for the best model by inspecting tune_output %&gt;% collect_metrics() %&gt;% filter(penalty == (best_se_penalty %&gt;% pull(penalty))). Make residual plots for the model by creating a dataset called lasso_mod_out which contains the original data as well as predicted values and residuals (.pred and resid). lasso_mod_out &lt;- final_fit_se %&gt;% predict(new_data = cars2018) %&gt;% bind_cols(cars2018) %&gt;% mutate(resid = mpg - .pred) Note: If you’re curious about making plots that show both test error estimates and their uncertainty, look at Digging Deeper. Digging deeper We used the plot of coefficient paths to evaluate the variable importance of our predictors. The code below does this systematically for each predictor so that we don’t have to eyeball. Step through and work out what each part is doing. It may help to look up function documentation with ?function_name in the Console. glmnet_output &lt;- final_fit_se %&gt;% extract_fit_engine() # Create a boolean matrix (predictors x lambdas) of variable exclusion bool_predictor_exclude &lt;- glmnet_output$beta==0 # Loop over each variable var_imp &lt;- sapply(seq_len(nrow(bool_predictor_exclude)), function(row) { this_coeff_path &lt;- bool_predictor_exclude[row,] if(sum(this_coeff_path) == ncol(bool_predictor_exclude)){ return(0)}else{ return(ncol(bool_predictor_exclude) - which.min(this_coeff_path) + 1)} }) # Create a dataset of this information and sort var_imp_data &lt;- tibble( var_name = rownames(bool_predictor_exclude), var_imp = var_imp ) var_imp_data %&gt;% arrange(desc(var_imp)) If you want more practice, the Hitters data in the ISLR package (be sure to to install and load) contains the salaries and performance measures for 322 Major League Baseball players. Use LASSO to determine the “best” predictive model of Salary. "],["knn-regression-and-the-bias-variance-tradeoff.html", "Topic 7 KNN Regression and the Bias-Variance Tradeoff Learning Goals KNN models in tidymodels Exercises", " Topic 7 KNN Regression and the Bias-Variance Tradeoff Learning Goals Clearly describe / implement by hand the KNN algorithm for making a regression prediction Explain how the number of neighbors relates to the bias-variance tradeoff Explain the difference between parametric and nonparametric methods Explain how the curse of dimensionality relates to the performance of KNN Slides from today are available here. KNN models in tidymodels To build KNN models in tidymodels, first load the package and set the seed for the random number generator to ensure reproducible results: library(dplyr) library(readr) library(broom) library(ggplot2) library(tidymodels) tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions set.seed(2023) # or choose your favorite number! Then adapt the following code: # CV Folds data_cv10 &lt;- vfold_cv(___, v = 10) # Model Specification knn_spec &lt;- nearest_neighbor() %&gt;% # new type of model! set_args(neighbors = tune()) %&gt;% # tuning parameter is neighbor; tuning spec set_engine(engine = &#39;kknn&#39;) %&gt;% # new engine set_mode(&#39;regression&#39;) # Recipe with standardization (!) data_rec &lt;- recipe( ___ ~ ___ , data = ___) %&gt;% step_nzv(all_predictors()) %&gt;% # removes variables with the same value step_novel(all_nominal_predictors()) %&gt;% # important if you have rare categorical variables step_normalize(all_numeric_predictors()) %&gt;% # important standardization step for KNN step_dummy(all_nominal_predictors()) # creates indicator variables for categorical variables (important for KNN!) # Workflow (Recipe + Model) knn_wf &lt;- workflow() %&gt;% add_model(knn_spec) %&gt;% add_recipe(data_rec) # Tune model trying a variety of values for neighbors (using 8-fold CV) penalty_grid &lt;- grid_regular( neighbors(range = c(1, 50)), # min and max of values for neighbors levels = 50) # number of neighbors values knn_fit_cv &lt;- tune_grid(knn_wf, # workflow resamples = data_cv10, #CV folds grid = penalty_grid, # grid specified above metrics = metric_set(rmse, mae)) Argument Meaning y ~ . Model formula for specifying response and predictors data Sample data preProcess \"scale\" indicates that predictor variables should be scaled to have the same variance (Why might this be important? Should we always do this?) method \"knn\" implements KNN regression (and classification) tuneGrid A mini-dataset (data.frame) of tuning parameters. \\(k\\) is the KNN neighborhood size. Supply a sequence as seq(begin, end, by = size of step). trControl Use cross-validation to estimate test performance for each model fit. The process used to pick a final model from among these is indicated by selectionFunction, with options including \"best\" and \"oneSE\". metric Evaluate and compare competing models with respect to their CV-MAE. na.action Set na.action = na.omit to prevent errors if the data has missing values. Note: When including categorical predictors, tidymodels automatically creates the corresponding indicator variables to allow Euclidean distance to still be used. You’ll think about the pros/cons of this in the exercises. An alternative to creating indicator variables is to use Gower distance. We’ll explore Gower distance more when we talk about clustering. Identifying the “best” KNN model The “best” model in the sequence of models fit is defined relative to the chosen selectionFunction and metric. knn_fit_cv %&gt;% autoplot() # Visualize Trained Model using CV knn_fit_cv %&gt;% show_best(metric = &#39;mae&#39;) # Show evaluation metrics for different values of neighbors, ordered # Choose value of Tuning Parameter (neighbors) tuned_knn_wf &lt;- knn_fit_cv %&gt;% select_by_one_std_err(metric = &#39;mae&#39;,desc(neighbors)) %&gt;% # Choose neighbors value that leads to the highest neighbors within 1 se of the lowest CV MAE finalize_workflow(knn_wf, .) # Fit final KNN model to data knn_fit_final &lt;- tuned_knn_wf %&gt;% fit(data = __) # Use the best model to make predictions # new_data should be a data.frame with required predictors predict(knn_fit_final, new_data = ___) Exercises You can download a template RMarkdown file to start from here. We’ll explore KNN regression using the College dataset in the ISLR package (install it with install.packages(\"tidymodels\") in the Console). You can use ?College in the Console to look at the data codebook. library(ISLR) library(dplyr) library(readr) library(broom) library(ggplot2) library(tidymodels) tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions data(College) # data cleaning college_clean &lt;- College %&gt;% mutate(school = rownames(College)) %&gt;% # creates variable with school name filter(Grad.Rate &lt;= 100) # Remove one school with grad rate of 118% rownames(college_clean) &lt;- NULL # Remove school names as row names Exercise 1: Bias-variance tradeoff warmup Think back to the LASSO algorithm which depends upon tuning parameter \\(\\lambda\\). For which values of \\(\\lambda\\) (small or large) will LASSO be the most biased, and why? For which values of \\(\\lambda\\) (small or large) will LASSO be the most variable, and why? The bias-variance tradeoff also comes into play when comparing across algorithms, not just within algorithms. Consider LASSO vs. least squares: Which will tend to be more biased? Which will tend to be more variable? When will LASSO outperform least squares in the bias-variance tradeoff? Exercise 2: Impact of distance metric Consider the 1-nearest neighbor algorithm to predict Grad.Rate on the basis of two predictors: Apps and Private. Let Yes for Private be represented with the value 1 and No with 0. We have a test case whose number of applications is 13,530 and is a private school. Suppose that we have the tiny 2-case training set below. What would the 1-nearest neighbor prediction be using Euclidean distance? college_clean %&gt;% filter(school %in% c(&quot;Princeton University&quot;, &quot;SUNY at Albany&quot;)) %&gt;% select(Apps, Private, Grad.Rate, school) Do you have any concerns about the resulting prediction? Based on this, comment on the impact of the distance metric chosen on KNN performance. How might you change the distance calculation (or correspondingly rescale the data) to generate a more sensible prediction in this situation? Exercise 3: Implementing KNN in tidymodels Adapt our general KNN code to “fit” a set of KNN models with the following specifications: Use the predictors Private, Top10perc (% of new students from top 10% of high school class), and S.F.Ratio (student/faculty ratio). Predict values of Grad.Rate Use 8-fold CV. (Why 8? Take a look at the sample size.) Use mean absolute error (MAE) to select a final model. Select the simplest model for which the metric is within one standard error of the best metric. Use a sequence of \\(K\\) values from 1 to 100 in increments of 5. Should you use preProcess = \"scale\"? After adapting the code (but before inspecting any output), answer the following conceptual questions: Explain your choice for using or not using preProcess = \"scale\". Why is “fit” in quotes? Does KNN actually fit a model as part of training? (This feature of KNN is known as “lazy learning”.) How is test MAE estimated? What are the steps of the KNN algorithm with cross-validation? Draw a picture of how you expect test MAE to vary with \\(K\\). In terms of the bias-variance tradeoff, why do you expect the plot to look this way? set.seed(2023) data_cv8 &lt;- vfold_cv(____, v = ____) # Model Specification knn_spec &lt;- nearest_neighbor() %&gt;% # new type of model! set_args(neighbors = tune()) %&gt;% # tuning parameter is neighbor; tuning spec set_engine(engine = &#39;kknn&#39;) %&gt;% # new engine set_mode(&#39;regression&#39;) # Recipe with standardization (!) data_rec &lt;- recipe( ___ ~ ___ , data = data_cv8) %&gt;% step_nzv(all_predictors()) %&gt;% # removes variables with the same value step_novel(all_nominal_predictors()) %&gt;% # important if you have rare categorical variables step_normalize(all_numeric_predictors()) %&gt;% # important standardization step for KNN step_dummy(all_nominal_predictors()) # creates indicator variables for categorical variables (important for KNN!) # Workflow (Recipe + Model) knn_wf &lt;- workflow() %&gt;% add_model(knn_spec) %&gt;% add_recipe(data_rec) # Tune model trying a variety of values for neighbors (using 8-fold CV) neighbor_grid &lt;- grid_regular( neighbors(range = c(______, _____)), # min and max of values for neighbors levels = _______) # number of neighbors values knn_fit_cv &lt;- tune_grid(knn_wf, # workflow resamples = data_cv8, #CV folds grid = neighbor_grid, # grid specified above metrics = metric_set(rmse, mae)) Exercise 4: Inspecting the results Use autoplot(knn_mod) to verify your expectations about the plot of test MAE vs. \\(K\\). Contextually interpret the test MAE. Does anything about the results surprise you? Exercise 5: Curse of dimensionality Just as with parametric models, we could keep going and add more and more predictors. However, the KNN algorithm is known to suffer from the “curse of dimensionality”. Why? Hint: First do a quick Google search of this new idea. "],["splines.html", "Topic 8 Splines Learning Goals Splines in tidymodels Exercises", " Topic 8 Splines Learning Goals Explain the advantages of splines over global transformations and other types of piecewise polynomials Explain how splines are constructed by drawing connections to variable transformations and least squares Explain how the number of knots relates to the bias-variance tradeoff Slides from today are available here. Splines in tidymodels To build models with splines in tidymodels, we proceed with the same structure we use for ordinary linear regression models but we’ll add some pre-processing steps. To work with splines, we’ll use the splines package. The step_ns() function based on ns() in that package creates the transformations needed to create a spline function for a quantitative predictor. The step_bs() function based on bs() in that package creates the transformations needed to create a basis spline (typically called B-splines) function for a quantitative predictor. # Linear Regression Model Spec lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &#39;lm&#39;) %&gt;% set_mode(&#39;regression&#39;) # Original Recipe lm_rec &lt;- recipe(___ ~ ___, data = ___) # Natural Spline Recipe ns2_rec &lt;- lm_rec %&gt;% step_ns(__, deg_free = __) # natural cubic spline (higher deg_free means more knots) # Basis Spline Recipe bs_rec &lt;- lm_rec %&gt;% step_bs(__, options = list(knots = c(__,__))) # b-spline (cubic by default) The deg_free argument in step_ns() stands for degrees of freedom: deg_free = # knots + 1 The degrees of freedom are the number of coefficients in the transformation functions that are free to vary (essentially the number of underlying parameters behind the transformations). The knots are chosen using percentiles of the observed values. The options argument in step_bs() allows you to specific specific knots: options = list(knots = c(2, 4)) sets a knot at 2 and a knot at 4 (the choice of knots should depend on the observed range of the quantitative predictor and where you see a change in the relationship) What is the difference between natural splines ns and B-splines bs? B-spline is a tool to incorporate splines into a linear regression setting. You have to choose the knots (which can be tricky sometimes). These functions can be unstable (high variance) near the boundaries, especially with higher polynomial degrees. Natural spline is a variant of the B-spline with additional constraints (the degree of the polynomial near the boundaries is lower). Cubic natural splines are the most common Typically knots are chosen based on quantiles of the predictor (e.g. 1 knot will be placed at the median, 2 knots will be placed at the 33rd and 66th percentiles, etc.) Exercises You can download a template RMarkdown file to start from here. Before proceeding, install the splines package by entering install.packages(\"splines\") in the Console. We’ll continue using the College dataset in the ISLR package to explore splines. You can use ?College in the Console to look at the data codebook. library(ISLR) library(dplyr) library(readr) library(broom) library(ggplot2) library(splines) library(tidymodels) tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions data(College) # A little data cleaning college_clean &lt;- College %&gt;% mutate(school = rownames(College)) %&gt;% filter(Grad.Rate &lt;= 100) # Remove one school with grad rate of 118% rownames(college_clean) &lt;- NULL # Remove school names as row names Exercise 1: Evaluating a fully linear model We will model Grad.Rate as a function of 4 predictors: Private, Terminal, Expend, and S.F.Ratio. Make a scatterplot of Grad.Rate as a function of Expend with 2 different smoothing lines to explore potential nonlinearity. Adding the following to the normal scatterplot code will create a smooth (curved) blue trend line and a red linear trend line. ggplot(___, aes(___)) + geom_point() + geom_smooth(color = &quot;blue&quot;, se = FALSE) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, se = FALSE) + theme_classic() Use tidymodels to fit an ordinary linear regression model (no splines yet) with the following specifications: Use 8-fold CV. Use CV mean absolute error (MAE) to select a final model. Use LASSO engine to do variable selection to select the simplest model for which the MAE is within one standard error of the best MAE Fit your “best” models and look at the coefficients of that final model set.seed(2023) # Create CV folds data_cv8 &lt;- vfold_cv(__, v = 8) # Lasso Model Spec with tune lm_lasso_spec_tune &lt;- linear_reg() %&gt;% set_args(mixture = 1, penalty = tune()) %&gt;% ## mixture = 1 indicates Lasso set_engine(engine = &#39;glmnet&#39;) %&gt;% set_mode(&#39;regression&#39;) # Recipe full_rec &lt;- recipe(__ ~ ___, data = college_clean) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) # Workflow (Recipe + Model) lasso_wf_tune &lt;- workflow() %&gt;% add_recipe(full_rec) %&gt;% add_model(lm_lasso_spec_tune) # Tune Model (trying a variety of values of Lambda penalty) penalty_grid &lt;- grid_regular( penalty(range = c(-3, 1)), #log10 transformed levels = 30) tune_output &lt;- tune_grid( lasso_wf_tune, # workflow resamples = data_cv8, # cv folds metrics = metric_set(___), grid = penalty_grid # penalty grid defined above ) # Select best model &amp; fit best_penalty &lt;- tune_output %&gt;% select_by_one_std_err(metric = &#39;mae&#39;, desc(penalty)) ls_mod &lt;- best_penalty %&gt;% finalize_workflow(lasso_wf_tune,.) %&gt;% fit(data = college_clean) # Note which variable is the &quot;least&quot; important ls_mod %&gt;% tidy() Make plots of the residuals vs. the 3 quantitative predictors to evaluate the appropriateness of linear terms. ls_mod_output &lt;- college_clean %&gt;% bind_cols(predict(ls_mod, new_data = college_clean)) %&gt;% mutate(resid = __ - __) ggplot(ls_mod_output, aes(__)) + ___ + ___ + geom_hline(yintercept = 0, color = &quot;red&quot;) + theme_classic() Exercise 2: Evaluating a spline model We’ll extend our linear regression model with spline functions of the quantitative predictors (leave Private as is). What tuning parameter is associated with splines? How do high/low values of this parameter relate to bias and variance? Update your recipe from Exercise 1 to fit a linear model (with the lm engine rather than lasso) with the 2 best quantitative predictors with natural splines that have 2 knots (= 3 degrees of freedom) and include Private. Fit this model with CV, fit_resamples, (same folds as before) to compare MAE and then fit the model to the whole training data. Call this fit model ns_mod. # Model Spec lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &#39;lm&#39;) %&gt;% set_mode(&#39;regression&#39;) # New Recipe (remove steps needed for LASSO, add splines) # Workflow (Recipe + Model) # CV to Evaluate cv_output &lt;- fit_resamples( ___, # workflow resamples = data_cv8, # cv folds metrics = metric_set(___) ) cv_output %&gt;% collect_metrics() # Fit with all data ns_mod &lt;- fit( ___, #workflow data = college_clean ) Make plots of the residuals vs. the 3 quantitative predictors to evaluate if splines improved the model. spline_mod_output &lt;- ___ Compare the CV MAE between models with and without the splines tune_output %&gt;% collect_metrics() %&gt;% filter(penalty == (best_penalty %&gt;% pull(penalty))) cv_output %&gt;% collect_metrics() Extra! Variable scaling What is your intuition about whether variable scaling matters for the performance of splines? Check your intuition by reusing code from Exercise 2, except by adding in step_normalize(all_numeric_predictors()) before step_ns(). Call this ns_mod2. How do the predictions from ns_mod and ns_mod2 compare? You could use a plot to compare or check out the all.equal() function. "],["local-regression-gams.html", "Topic 9 Local Regression &amp; GAMs Learning Goals 9.1 GAMs 9.2 GAMs - Options for Fitting GAMs in tidymodels Exercises", " Topic 9 Local Regression &amp; GAMs Learning Goals Clearly describe the local regression algorithm for making a prediction Explain how bandwidth (span) relate to the bias-variance tradeoff Describe some different formulations for a GAM (how the arbitrary functions are represented) Explain how to make a prediction from a GAM Interpret the output from a GAM Slides from today are available here. 9.1 GAMs Generalized Additive Models (GAMs) are extensions of linear models by allowing non-linear functions of any of the variables, while maintaining additivity in the relationships (no interactions; you can keep other variables fixed). In theory, we imagine that each predictor variable \\(x_{ij}\\) has its own relationship with the outcome, represented by the function \\(f_j\\): \\(y_i = \\beta_0 + f_1(x_{i1}) + ... + f_p(x_{ip}) + \\epsilon_i\\) That function could be linear, \\(f_j(x_{ij}) = \\beta_j x_{ij}\\), or non-linear (curved in some way). There are many ways we could model the non-linear functions. We can model non-linear relationships using local regression (LOESS) basis splines (natural or not) smoothing splines (fits splines with a penalty term) Smoothing splines are piecewise polynomials with the same properties we talked about before (continuity of function and derivatives) but are estimated with a penalty term. We minimize the penalized least squares, \\(RSS + \\lambda \\int g&#39;&#39;(t)^2 dt\\) \\(\\lambda\\) is a tuning parameter often referred to as the degree of smoothness and the spline function \\(g(t)\\) that minimizes this quantity is a smoothing spline. Note: the penalty term looks a bit different from LASSO; the second derivative, \\(g&#39;&#39;(t)\\), represents how much the slope changes along the function \\(g\\). We square the changes to remove negative changes and take the integral as a measure of overall “wiggliness” of the function (how much the slope changes along the function). Large \\(λ\\) penalizes you for having a too “wiggly” function so it may lead to a more simple linear function and small \\(λ\\) allows you to have a more “wiggly” function that may overfit the training data. 9.2 GAMs - Options for Fitting B-splines and OLS: use the lm engine with step_ns() (natural) or step_bs() (B-spline) LOESS: use the gam package GAMs in tidymodels To build GAMs in tidymodels, first load the package: library(dplyr) library(readr) library(broom) library(ggplot2) library(tidymodels) tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions Then adapt the following code: # Generalized Additive Regression (GAM) Model gam_spec &lt;- gen_additive_mod() %&gt;% set_engine(engine = &#39;mgcv&#39;) %&gt;% set_mode(&#39;regression&#39;) fit_gam_model &lt;- gam_spec %&gt;% # can&#39;t use a recipe with gam (yet) fit(y ~ s(x1) + x2 + s(x3), data = train_data) # s() stands for splines, indicating a non-linear relationship # To specify the number of knots, update the formula: s(x1, k = __) # To specify the degree of smoothing, update the formula: s(x1, sp = __) Ensuring a good fit for GAM The mgcv engine uses generalized cross-validation to select the degree of smoothness for a default number of knots, if these two tuning parameters aren’t specified by the user. Note: the number of knots must be large enough to capture the true “wiggliness” of the relationship, and the \\(λ\\) penalty does the rest of the work. In mgcv, the default number of knots is arbitrary so you need to run gam.check to make sure there are enough knots. # Summary: Parameter (linear) estimates and then Smooth Terms (H0: no relationship) fit_gam_model %&gt;% pluck(&#39;fit&#39;) %&gt;% summary() # Diagnostics: Check to see if the number of knots is large enough (if p-value is low, increase number of knots) par(mfrow=c(2,2)) fit_gam_model %&gt;% pluck(&#39;fit&#39;) %&gt;% mgcv::gam.check() Visualizing the GAM # Plot functions for each predictor # Dashed lines are +/- 2 SEs fit_gam_model %&gt;% pluck(&#39;fit&#39;) %&gt;% plot() Exercises You can download a template RMarkdown file to start from here. Before proceeding, install the mgcv package by entering install.packages(\"mgcv\") in the Console. We’ll continue using the College dataset in the ISLR package to explore splines. You can use ?College in the Console to look at the data codebook. library(ISLR) library(dplyr) library(readr) library(broom) library(ggplot2) library(tidymodels) tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions data(College) # A little data cleaning college_clean &lt;- College %&gt;% mutate(school = rownames(College)) %&gt;% filter(Grad.Rate &lt;= 100) # Remove one school with grad rate of 118% rownames(college_clean) &lt;- NULL # Remove school names as row names Exercise 1: Conceptual warmup How does high/low span relate to bias and variance of a local regression (LOESS) model? How does high/low lambda relate to bias and variance of a smoothing spline in a GAM model? Do you think that a GAM with all possible predictors will have better or worse performance than an ordinary (fully linear) least squares model with all possible predictors? Explain your thoughts. How should we choose predictors to be in a GAM? How could forward and backward stepwise selection and LASSO help with variable selection before a GAM? (Don’t answer right away; think about it as you go along in Exercise 2 and come back to this question.) 9.2.1 Exercise 2: Using LOESS Use LOESS (geom_smooth()) to estimate the relationship between Apps and Grad.Rate. Try different values of span between 0 and 1. Write one sentence summarizing what you learned about the relationship between Apps and Grad.Rate. college_clean %&gt;% ggplot(aes(x = Apps, y = Grad.Rate)) + geom_point(alpha = 0.2) + geom_smooth(span = 0.2, se = FALSE) + xlim(c(0,20000)) + theme_classic() Use LOESS (geom_smooth()) to estimate the relationship between Apps and Grad.Rate, separately for Private and Non-Private colleges. Try different values of span between 0 and 1. Write one sentence summarizing what you learned about the relationship between Apps and Grad.Rate. college_clean %&gt;% ggplot(aes(x = Apps, y = Grad.Rate, color = Private)) + geom_point(alpha = 0.2) + geom_smooth(span = 0.2, se = FALSE) + xlim(c(0,20000)) + theme_classic() Stop to Consider: Is there an additive relationship for Private and Apps as it relates to the outcome of Grad.Rate? 9.2.2 Exercise 3: Building a GAM in tidymodels Suppose that our initial variable selection investigations lead us to using the predictors indicated below in our GAM. Fit a GAM with the following variables in the model. set.seed(123) gam_spec &lt;- gen_additive_mod() %&gt;% set_engine(engine = &#39;mgcv&#39;) %&gt;% set_mode(&#39;regression&#39;) gam_mod &lt;- fit(gam_spec, Grad.Rate ~ Private + s(Apps) + s(Top10perc) + s(Top25perc) + s(P.Undergrad) + s(Outstate) + s(Room.Board) + s(Books) + s(Personal) + s(PhD) + s(perc.alumni), data = college_clean ) First, let’s look to see if the default number of knots (10) is large enough for each variable. The edf is the effective degrees of freedom; the larger the edf value, the more wiggly the estimated function is. If edf = 1, it is a straight line. Comment on the diagnostic plots and output. # Diagnostics: Check to see if the number of knots is large enough (if p-value is low, increase number of knots) par(mfrow=c(2,2)) gam_mod %&gt;% pluck(&#39;fit&#39;) %&gt;% mgcv::gam.check() Next, look at the summary of the output. For parametric (linear) estimates, the coefficients and standard errors are interpreted in the same way as OLS output. The approximate significance of smooth terms is a bit different. The edf is listed for each variable. The p-value is for the null hypothesis that there is no relationship between that predictor and the outcome (meaning the relationship is horizontal line). Do you think we should make any adjustments to the model? Do we need splines for all of these variables? # Summary: Parameter (linear) estimates and then Smooth Terms (H0: no relationship) gam_mod %&gt;% pluck(&#39;fit&#39;) %&gt;% summary() Lastly, before we make changes to the model, let’s visualize the estimated non-linear functions. What do you notice? What about these plots indicates that using GAM instead of ordinary linear regression was probably a good choice? # Visualize: Look at the estimated non-linear functions gam_mod %&gt;% pluck(&#39;fit&#39;) %&gt;% plot( all.terms = TRUE, pages = 1) 9.2.3 Exercise 4: Adjusting the “best” GAM First, let’s consider limiting the training sample given the outliers (the bottom vertical lines on the plot is called a “rug plot” and shows where you have observed values). Look at the outlier colleges. college_clean %&gt;% filter(Apps &gt; 30000 | P.Undergrad &gt; 20000) Now that you’ve seen which colleges we are setting aside, let’s refit the model without these two colleges. gam_mod2 &lt;- college_clean %&gt;% filter(________) %&gt;% fit(gam_spec, Grad.Rate ~ Private + s(Apps) + s(Top10perc) + s(Top25perc) + s(P.Undergrad) + s(Outstate) + s(Room.Board) + s(Books) + s(Personal) + s(PhD) + s(perc.alumni), data = . ) Let’s look at the estimated functions again. Pick 1 or 2 of these plots, and interpret your findings. Anything surprising or interesting? gam_mod2 %&gt;% pluck(&#39;fit&#39;) %&gt;% summary() gam_mod2 %&gt;% pluck(&#39;fit&#39;) %&gt;% plot( all.terms = TRUE, pages = 1) The PrivateYes plot might look odd. Not to worry - the GAM is treating this as a categorical (indicator) variable. What do you learn from this plot? Based on the output above, which variables may not need splines/non-linear relationships? How could we simplify the model? Now try fitting a more simple GAM model with tidymodels by only using a non-linear spline function for a variable if the edf is greater than 3 (3 is not a magic number; it is arbitrary). What do you notice from the output? gam_mod3 &lt;- college_clean %&gt;% filter(_____) %&gt;% # remove outliers fit(gam_spec, Grad.Rate ~ ______, # add in the formula data = . ) gam_mod3 %&gt;% pluck(&#39;fit&#39;) %&gt;% summary() gam_mod3 %&gt;% pluck(&#39;fit&#39;) %&gt;% plot( all.terms = TRUE, pages = 1) Exercise 5: GAM with recipes Use the edf (round to integer) from above to create a recipe by adding step_ns() for the variables you want model with a non-linear relationship and do CV. Compare this model to one without any splines. lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &#39;lm&#39;) %&gt;% set_mode(&#39;regression&#39;) data_cv8 &lt;- college_clean %&gt;% filter(___) %&gt;% # remove outliers vfold_cv(v = 8) college_rec &lt;- recipe(Grad.Rate ~ Private + Apps + Top10perc + Top25perc + P.Undergrad + Outstate + Room.Board + Books + Personal + PhD + perc.alumni, data = college_clean) spline_rec &lt;- college_rec %&gt;% step_ns(__, deg_free = __) college_wf &lt;- workflow() %&gt;% add_model(lm_spec) %&gt;% add_recipe(college_rec) spline_wf &lt;- workflow() %&gt;% add_model(lm_spec) %&gt;% add_recipe(spline_rec) fit_resamples( college_wf, resamples = data_cv8, # cv folds metrics = metric_set(mae,rmse,rsq) ) %&gt;% collect_metrics() fit_resamples( spline_wf, resamples = data_cv8, # cv folds metrics = metric_set(mae,rmse,rsq) ) %&gt;% collect_metrics() What do you observe? What do you conclude about these models? Exercise 6: putting a bow on regression Brainstorm the pros/cons of the different methods that we’ve explored. You may find it helpful to refer to the portfolio themes for each method. (Soon, as part of the Portfolio, you’ll be doing a similar synthesis of our regression unit, so this brainstorming session might help!) "],["homework-1.html", "Homework 1 Project Work Ethics in ML Portfolio Work", " Homework 1 **Submit by Sunday, Feb. 5th at 11:59pm to Moodle. Please turn in a single PDF document containing (1) your responses for the Project Work and Ethics in ML sections and (2) a LINK to the Google Doc with your responses for the Portfolio Work section. Project Work Goal: Find a dataset (or datasets) to use for your final project, and start to get to know the data. Details: Your dataset(s) should allow you to perform a (1) regression, (2) classification, and (3) unsupervised learning analysis. The following resources are good places to start looking for data: Google Dataset Search Kaggle UCI Machine Learning Repository Harvard Dataverse Even if you end up working with a partner on the project (which isn’t required - working alone is fine), please complete this initial work individually. It’s fine if you and a potential/future partner end up using the same dataset and collaborate on the finding of data, but complete the short bit of writing (below) individually. Check in with the instructor early if you need help. Deliverables: Write 1-2 paragraphs (no more than 350 words) summarizing: The information in the dataset(s) and the context behind the data. Use the prompts below to guide your thoughts. (Note: in some situations, there may be incomplete information on the data context. That’s fine. Just do your best to summarize what information is available, and acknowledge the lack of information where relevant.) What are the cases? Broadly describe the variables contained in the data. Who collected the data? When, why, and how? 3 research questions 1 that can be investigated in a regression setting 1 that can be investigated in a classification setting 1 that can be investigated in an unsupervised learning setting Also make sure that you can read the data into R. You don’t need to do any analysis in R yet, but making sure that you can read the data will make the next steps go more smoothly. Ethics in ML Read the article Amazon scraps secret AI recruiting tool that showed bias against women. Write a short (roughly 250 words), thoughtful response about the themes and cautions that the article brings forth. Portfolio Work Setup: In addition to your submission here, you’ll want to collect your Portfolio work in a single document. James shared a Google doc link with you on January 24th where you should keep your Portfolio responses. For each Homework submission, copy your Portfolio responses into the appropriate space. For example, Homework 1 has three prompts: Overfitting, Evaluating Regression Models, and Cross-validation. In your portfolio, copy and paste your responses under the appropriate header. Page maximum: 2 pages of text (pictures don’t count) Organization: Your choice! Use titles and section headings that make sense to you. (It probably makes sense to have a separate section for each method.) Deliverables: Put your responses for this part in a Google Doc, and update the link sharing so that anyone with the link at Macalester College can edit. Include the URL for the Google Doc in your submission. Note: Some prompts below may seem very open-ended. This is intentional. Crafting good responses requires looking back through our material to organize the concepts in a coherent, thematic way, which is extremely useful for your learning. Concepts to address: Overfitting: The video used the analogy of a cat picture model to explain overfitting. Come up with your own analogy to explain overfitting. Evaluating regression models: Describe how residuals are central to the evaluation of regression models. Explain how they arise in quantitative evaluation metrics and how they are used in evaluation plots. Include examples of plots that show desirable and undesirable model behavior (feel free to draw them by hand if you wish) and what steps can be taken to address that undesirable behavior. Cross-validation: In your own words, explain the rationale for cross-validation in relation to overfitting and model evaluation. Describe the algorithm in your own words in at most 2 sentences. "],["homework-2.html", "Homework 2 Project Work Ethics in ML Portfolio Work", " Homework 2 Due Monday, February 27th @ 11:59pm CST on Moodle Deliverables: Please use this template to knit an HTML document. Convert this HTML document to a PDF by opening the HTML document in your web browser. Print the document (Ctrl/Cmd-P) and change the destination to “Save as PDF”. Submit this one PDF to Moodle. Alternatively, you may knit your Rmd directly to PDF if you have LaTeX installed. Project Work Goal: Begin an analysis of your dataset to answer your regression research question. Collaboration: If you have already formed a team (of at most 3 members) for the project, this part can be done as a team. Only one team member should submit a Project Work section. Grading: Completing this Project Work section is a required part of the final project. Rather than receiving a grade for the analyses below, you will get qualitative feedback from the instructor. It is the synthesis of the analyses across homework assignments that will determine the final project grade. Data cleaning: If your dataset requires any cleaning (e.g., merging datasets, creation of new variables), first consult the R Resources page to see if your questions are answered there. If not, post on the #content-questions channel in our Slack workspace to ask for help. Please ask for help early and regularly to avoid stressful workloads. Required Analyses: Initial investigation: ignoring nonlinearity (for now) Use ordinary least squares (OLS) regression, forward and/or backward selection, and LASSO to build initial models for your quantitative outcome as a function of the predictors of interest. (As part of data cleaning, exclude any variables that you don’t want to consider as predictors.) These models should not include any transformations to deal with nonlinearity. You’ll explore this in the next investigation. Note: If you have highly collinear/redundant variables, you might see the message “Reordering variables and trying again” and associated warning()s about linear dependencies being found. Sometimes stepwise selection is able to handle the collinearity/redundancy by modifying the order of the variables tried. If collinearity/redundancy cannot be handled and causes an error, try reducing nvmax. Estimate test performance of the models from these different methods. Report and interpret (with units) these estimates along with a measure of uncertainty in the estimate (SD is most readily available from caret). Compare estimated test performance across methods. Which method(s) might you prefer? Use residual plots to evaluate whether some quantitative predictors might be better modeled with nonlinear relationships. Compare insights from variable importance analyses from the different methods (stepwise and LASSO, but not OLS). Are there variables for which the methods reach consensus? What insights are expected? Surprising? Note that if some (but not all) of the indicator terms for a categorical predictor are selected in the final models, the whole predictor should be treated as selected. Accounting for nonlinearity Update your stepwise selection model(s) and LASSO model to use natural splines for the quantitative predictors. You’ll need to update the model formula from y ~ . to something like y ~ cat_var1 + ns(quant_var1, df) + .... It’s recommended to use few knots (e.g., 2 knots = 3 degrees of freedom). Note that ns(x,3) replaces x with 3 transformations of x. Keep this in mind when setting nvmax in stepwise selection. Compare insights from variable importance analyses here and the corresponding results from Investigation 1. Now after having accounted for nonlinearity, have the most relevant predictors changed? Note that if some (but not all) of the spline terms are selected in the final models, the whole predictor should be treated as selected. Fit a GAM using LOESS terms using the set of variables deemed to be most relevant based on your investigations so far. How does test performance of the GAM compare to other models you explored? Do you gain any insights from the GAM output plots for each predictor? Don’t worry about KNN for now. Summarize investigations Decide on an overall best model based on your investigations so far. To do this, make clear your analysis goals. Predictive accuracy? Interpretability? A combination of both? Societal impact Are there any harms that may come from your analyses and/or how the data were collected? What cautions do you want to keep in mind when communicating your work? Ethics in ML Read the article Automated background checks are deciding who’s fit for a home. Write a short (roughly 250 words), thoughtful response about the ideas that the article brings forth. What themes recur from last week’s article (on an old Amazon recruiting tool)? What aspects are more particular to the context of equity in housing access? Reflection: Write a short, thoughtful reflection about how things went this week. Feel free to use whichever prompts below resonate most with you, but don’t feel limited to these prompts. How are class-related things going? Is there anything that you need from the instructor? What new strategies for watching videos, reading, reviewing, gaining insights from class work have you tried or would like to try? How is group work going? Did you try out any new collaboration strategies with your new group? How did they go? How is your work/life balance going? Did you try out any new activities or strategies for staying well? How did they go? Portfolio Work Deliverables: Continue writing your responses in the same Google Doc that you set up for Assignment 1. Include the URL for the Google Doc in your submission. Revisions: Make any revisions desired to previous concepts. Important formatting note: Please use a comment to mark the text that you want to be reread. (Highlight each span of text you want to be reread, and mark it with the comment “REVISION”.) Rubrics to past homeworks will be available on Moodle (under the Solutions section). Look at these rubrics to guide your revisions. You can always ask for guidance in office hours as well. New concepts to address: The following prompts are shared for all methods: Bias-variance tradeoff: What tuning parameters control the performance of the method? How do low/high values of the tuning parameters relate to bias and variance of the learned model? (3 sentences max.) Parametric / nonparametric: Where (roughly) does this method fall on the parametric-nonparametric spectrum, and why? (3 sentences max.) Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? (3 sentences max.) Subset selection: Algorithmic understanding: Look at Conceptual exercise 1, parts (a) and (b) in ISLR Section 6.8. What are the aspects of the subset selection algorithm(s) that are essential to answering these questions, and why? (Note: you’ll have to try to answer the ISLR questions to respond to this prompt, but the focus of your writing should be on the question in bold here.) Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? Computational time: What computational time considerations are relevant for this method (how long the algorithms take to run)? Interpretation of output: What parts of the algorithm output have useful interpretations, and what are those interpretations? Focus on output that allows us to measure variable importance. How do the algorithms/output allow us to learn about variable importance? Bias-variance tradeoff Parametric / nonparametric LASSO: Algorithmic understanding: Come up with your own analogy for explaining how the penalized least squares criterion works. Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? Computational time: What computational time considerations are relevant for this method (how long the algorithms take to run)? Interpretation of output: What parts of the algorithm output have useful interpretations, and what are those interpretations? Focus on output that allows us to measure variable importance. How do the algorithms/output allow us to learn about variable importance? Bias-variance tradeoff Parametric / nonparametric KNN: Algorithmic understanding: Draw and annotate pictures that show how the KNN (K = 2) regression algorithm would work for a test case in a 2 quantitative predictor setting. Also explain how the curse of dimensionality affects KNN performance. (5 sentences max.) Bias-variance tradeoff Parametric / nonparametric Scaling of variables Computational time: The KNN algorithm is often called a “lazy” learner. Discuss how this relates to the model training process and the computations that must be performed when predicting on a new test case. (3 sentences max.) Interpretation of output: The “lazy” learner feature of KNN in relation to model training affects the interpretability of output. How? (3 sentences max.) Splines: Algorithmic understanding: Explain the advantages of natural cubic splines over global transformations and piecewise polynomials. Also explain the connection between splines and the ordinary (least squares) regression framework. (5 sentences max.) Bias-variance tradeoff Parametric / nonparametric Scaling of variables Computational time: When using splines, how does computation time compare to fitting ordinary (least squares) regression models? (1 sentence) Interpretation of output: SKIP - will be covered in the GAMs section Local regression: Algorithmic understanding: Consider the R functions lm(), predict(), dist(), and dplyr::filter(). (Look up the documentation for unfamiliar functions in the Help pane of RStudio.) In what order would these functions need to be used in order to make a local regression prediction for a supplied test case? Explain. (5 sentences max.) Bias-variance tradeoff Parametric / nonparametric Scaling of variables Computational time: In general, local regression is very fast, but how would you expect computation time to vary with span? Explain. (3 sentences max.) Interpretation of output: SKIP - will be covered in the GAMs section GAMs: Algorithmic understanding: How do linear regression, splines, and local regression each relate to GAMs? Why would we want to model with GAMs? (5 sentences max.) Bias-variance tradeoff Parametric / nonparametric Scaling of variables Computational time: How a GAM is specified affects the time required to fit the model - why? (3 sentences max.) Interpretation of output: How does the interpretation of ordinary regression coefficients compare to the interpretation of GAM output? (3 sentences max.) "],["r-resources.html", "R Resources Outside resources Example code", " R Resources Outside resources Lisa Lendway’s COMP/STAT 112 website (with code examples and videos) RStudio cheat sheets ggplot2 reference R Programming Wikibook Debugging in R Article Video Colors in R Data import tutorial Free online textbooks R for Data Science Exploratory Data Analysis with R Example code Creating new variables case_when() from the dplyr package is a very versatile function for creating new variables based on existing variables. This can be useful for creating categorical or quantitative variables and for creating indices from multiple variables. # Turn quant_var into a Low/Med/High version data &lt;- data %&gt;% mutate(cat_var = case_when( quant_var &lt; 10 ~ &quot;Low&quot;, quant_var &gt;= 10 &amp; quant_var &lt;= 20 ~ &quot;Med&quot;, quant_var &gt; 20 ~ &quot;High&quot; ) ) # Turn cat_var (A, B, C categories) into another categorical variable # (collapse A and B into one category) data &lt;- data %&gt;% mutate(new_cat_var = case_when( cat_var %in% c(&quot;A&quot;, &quot;B&quot;) ~ &quot;A or B&quot; cat_var==&quot;C&quot; ~ &quot;C&quot; ) ) # Turn a categorical variable (x1) encoded as a numerical 0/1/2 variable into a different quantitative variable # Doing this for multiple variables allows you to create an index data &lt;- data %&gt;% mutate(x1_score = case_when( x1==0 ~ 10, x1==1 ~ 20, x1==2 ~ 50 ) ) # Add together multiple variables with mutate data &lt;- data %&gt;% mutate(index = x1_score + x2_score + x3_score) "],["final-project.html", "Final Project Requirements Grading Rubric", " Final Project Requirements You will be analyzing a dataset using a regression and a classification analysis. An unsupervised learning analysis is no longer required for the project. Collaboration: You may work in teams of up to 3 members. Individual work is fine. The weekly homework assignments will note whether work for that week should be submitted individually or if just one team member should submit work. There will be a required synthesis of the weekly homework investigations at the end of the course. If working on a team, this should be done in groups, rather than individually. Final deliverables: Only one team member has to submit these materials to Moodle. The due date is Thursday, May 4th at 11:59pm CST. Submit a final knitted HTML file (must knit without errors) and corresponding Rmd file containing code for your analysis Include a 10-15 minute video presentation of your project that addresses the items in the Grading Rubric below. (Recording the presentation over Zoom is a good option for creating the video. You can record to your computer or to the cloud.) Upload the video itself to Moodle. If it’s too large, share a link to a recording on the web or in a shared drive. All team members should have an equal speaking role in the presentation. If a video presentation would be difficult for you and your team to make, you may instead submit an annotated set of slides. The speaker notes beneath each slide should contain what you would have said in the video presentation. Grading Rubric Data context (10 points) Clearly describe what the cases in the final clean dataset represent. Broadly describe the variables used in your analyses. Who collected the data? When, why, and how? Answer as much of this as the available information allows. Research questions (10 points) Research question(s) for the regression task make clear the outcome variable and its units. Research question(s) for the classification task make clear the outcome variable and its possible categories. HW3 investigations - Methods (10 points) Describe the models used in your HW3 project work investigations. Describe what you did to evaluate models. Indicate how you estimated quantitative evaluation metrics. Indicate what plots you used to evaluate models. Describe the goals / purpose of the methods used in the overall context of your research investigations. HW3 investigations - Results - Variable Importance (10 points) Summarize results from HW3 Investigations 1 (and 2, if applicable) on variable importance measures. Note: Investigation 2 won’t be applicable to your project if you only have categorical predictors. Summarization should show evidence of acknowledging the data context in thinking about the sensibility of these results. HW3 investigations - Summary (10 points) If it was appropriate to fit a GAM for your investigations (having some quantitative predictors), show plots of estimated functions for each predictor, and provide some general interpretations. Compare the different models tried in HW3 in light of evaluation metrics, plots, variable importance, and data context. Display evaluation metrics for different models in a clean, organized way. This display should include both the estimated metric as well as its standard deviation. (Hint: you should be using caret_mod$results.) Broadly summarize conclusions from looking at these evaluation metrics and their measures of uncertainty. Summarize conclusions from residual plots from initial models (don’t have to display them though). Decide an overall most preferable model. Show and interpret some representative examples of residual plots for your final model. Does the model show acceptable results in terms of any systematic biases? Interpret evaluation metric(s) for the final model in context with units. Does the model show an acceptable amount of error? Classification analysis - Methods (10 points) Indicate 2 different methods used to answer your classification research question. Describe what you did to evaluate the 2 models explored. Indicate how you estimated quantitative evaluation metrics. Describe the goals / purpose of the methods used in the overall context of your research investigations. Classification analysis - Results - Variable Importance (10 points) Summarize results about variable importance measures in your classification analysis. Summarization should show evidence of acknowledging the data context in thinking about the sensibility of these results. Classification analysis - Summary (10 points) Compare the 2 different classification models tried in light of evaluation metrics, variable importance, and data context. Display evaluation metrics for different models in a clean, organized way. This display should include both the estimated metric as well as its standard deviation. (This won’t be available from OOB error estimation. If using OOB, don’t worry about reporting the SD.) Broadly summarize conclusions from looking at these evaluation metrics and their measures of uncertainty. Decide an overall most preferable model. Interpret evaluation metric(s) for the final model in context. Does the model show an acceptable amount of error? If using OOB error estimation, display the test (OOB) confusion matrix, and use it to interpret the strengths and weaknesses of the final model. Code (20 points) Knitted, error-free HTML and corresponding Rmd file submitted Code corresponding to all analyses above is present and correct "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
