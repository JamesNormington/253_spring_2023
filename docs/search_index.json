[["index.html", "STAT 253: Statistical Machine Learning Welcome!", " STAT 253: Statistical Machine Learning Welcome! Image source This is the class manual for Statistical Machine Learning (STAT 253) at Macalester College for Spring 2023, taught by Professor James Normington. The content largely draws upon our class textbook, An Introduction to Statistical Learning with Applications in R. In addition, much of this site’s content was created by Professor Alicia Johnson, Professor Brianna Heggeseth, and Professor Leslie Myint. This bookdown website was constructed by Professor Brianna Heggeseth. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["schedule-syllabus.html", "Schedule &amp; Syllabus", " Schedule &amp; Syllabus The schedule below is a tentative outline of our plans for the module. Here is the syllabus, current as of Jan. 23, 2023. Before each class period, please watch the indicated videos and check on your understanding by actively reviewing the associated Learning Objectives. The readings listed below are optional but serve as a nice complement to the videos and class activities. Readings refer to chapters/sections in the Introduction to Statistical Learning (ISLR) textbook (available online here). Week 1: 1/19 - 1/20 Day(s) Topic Readings 1/19 Introductions ISLR: Chap 1, Chap 2 - Section 2.1 (Skip 2.1.2, 2.1.3 for now.) Week 2: 1/23 - 1/27 Day(s) Topic Videos/Readings Slides 1/24 Evaluating Regression Models Evaluating Regression Models R: Introduction to TidyModels ISLR: 2.2 PDF 1/26 Overfitting Overfitting R: Pre-processing and recipes PDF Homework 1 due Friday, 2/3 at 11:59pm CST "],["learning-objectives.html", "Learning Objectives", " Learning Objectives Learning objectives for our course topics are listed below. Use these to guide your synthesis of video and reading material. Introduction to Statistical Machine Learning Formulate research questions that align with regression, classification, or unsupervised learning tasks Evaluating Regression Models Create and interpret residuals vs. fitted, residuals vs. predictor plots to identify improvements in modeling and address ethical concerns Interpret MSE, RMSE, MAE, and R-squared in a contextually meaningful way Overfitting and cross-validation Explain why training/in-sample model evaluation metrics can provide a misleading view of true test/out-of-sample performance Accurately describe all steps of cross-validation to estimate the test/out-of-sample version of a model evaluation metric Explain what role CV has in a predictive modeling analysis and its connection to overfitting Explain the pros/cons of higher vs. lower k in k-fold CV in terms of sample size and computing time Subset selection Clearly describe the forward and backward stepwise selection algorithm and why they are examples of greedy algorithms Compare best subset and stepwise algorithms in terms of optimality of output and computational time LASSO (shrinkage/regularization) Explain how ordinary and penalized least squares are similar and different with regard to (1) the form of the objective function and (2) the goal of variable selection Explain how the lambda tuning parameter affects model performance and how this is related to overfitting KNN Regression and the Bias-Variance Tradeoff Clearly describe / implement by hand the KNN algorithm for making a regression prediction Explain how the number of neighbors relates to the bias-variance tradeoff Explain the difference between parametric and nonparametric methods Explain how the curse of dimensionality relates to the performance of KNN (not in the video–will be discussed in class) Modeling Nonlinearity: Polynomial Regression and Splines Explain the advantages of splines over global transformations and other types of piecewise polynomials Explain how splines are constructed by drawing connections to variable transformations and least squares Explain how the number of knots relates to the bias-variance tradeoff Local Regression and Generalized Additive Models Clearly describe the local regression algorithm for making a prediction Explain how bandwidth (span) relate to the bias-variance tradeoff Describe some different formulations for a GAM (how the arbitrary functions are represented) Explain how to make a prediction from a GAM Interpret the output from a GAM Logistic regression Use a logistic regression model to make hard (class) and soft (probability) predictions Interpret non-intercept coefficients from logistic regression models in the data context Evaluating classification models Calculate (by hand from confusion matrices) and contextually interpret overall accuracy, sensitivity, and specificity Construct and interpret plots of predicted probabilities across classes Explain how a ROC curve is constructed and the rationale behind AUC as an evaluation metric Appropriately use and interpret the no-information rate to evaluate accuracy metrics Decision trees Clearly describe the recursive binary splitting algorithm for tree building for both regression and classification Compute the weighted average Gini index to measure the quality of a classification tree split Compute the sum of squared residuals to measure the quality of a regression tree split Explain how recursive binary splitting is a greedy algorithm Explain how different tree parameters relate to the bias-variance tradeoff Bagging and random forests Explain the rationale for bagging Explain the rationale for selecting a random subset of predictors at each split (random forests) Explain how the size of the random subset of predictors at each split relates to the bias-variance tradeoff Explain the rationale for and implement out-of-bag error estimation for both regression and classification Explain the rationale behind the random forest variable importance measure and why it is biased towards quantitative predictors (in class) K-means clustering Clearly describe / implement by hand the k-means algorithm Describe the rationale for how clustering algorithms work in terms of within-cluster variation Describe the tradeoff of more vs. less clusters in terms of interpretability Implement strategies for interpreting / contextualizing the clusters Hierarchical clustering Clearly describe / implement by hand the hierarchical clustering algorithm Compare and contrast k-means and hierarchical clustering in their outputs and algorithms Interpret cuts of the dendrogram for single and complete linkage Describe the rationale for how clustering algorithms work in terms of within-cluster variation Describe the tradeoff of more vs. less clusters in terms of interpretability Implement strategies for interpreting / contextualizing the clusters "],["r-and-rstudio-setup.html", "R and RStudio Setup Troubleshooting", " R and RStudio Setup Before class on Tuesday, Jan 24, you should follow these instructions to set up the software that we’ll be using throughout the semester. Even if you’ve already downloaded both R and RStudio, you’ll want to re-download to make sure that you have the most current versions. Highly recommended: Change the default file download location for your internet browser. Generally by default, internet browsers automatically save all files to the Downloads folder on your computer. This does not encourage good file organization practices. It is highly recommended that you change this option so that your browser asks you where to save each file before downloading it. This page has information on how to do this for the most common browsers. Required: Download R and RStudio FIRST: Download R here. You will see three links “Download R for …” Choose the link that corresponds to your computer. As of December 30, 2022, the latest version of R is 4.2.2. SECOND: Download RStudio here. Click the button under step 2 to install the version of RStudio recommended for your computer. As of December 30, 2022, the latest version of RStudio is 2022.12.0+353. Suggested: Watch this video made by Professor Lisa Lendway that describes essential configuration options for RStudio. Required: Install required packages. An R package is an extra bit of functionality that will help us in our data analysis efforts in a variety of ways. Open RStudio and click inside the Console pane (by default, the bottom left pane). Copy and paste the following command into the Console. You should see the text below appear to the right of the &gt;, which is called the R prompt. After you paste, hit Enter. install.packages(c(&quot;ggplot2&quot;, &quot;dplyr&quot;, &quot;readr&quot;, &quot;rmarkdown&quot;, &quot;broom&quot;, &quot;caret&quot;)) You will see a lot of text from status messages appearing in the Console as the packages are being installed. Wait until you see the &gt; again. Enter the command library(ggplot2) and hit enter. If you see the message Error in library(ggplot2) : there is no package called ggplot2, then there was a problem installing this package. Jump down to the Troubleshooting section below. (Any other messages that appear are fine, and a lack of any messages is also fine.) Repeat the above step for the commands: library(dplyr) library(readr) library(rmarkdown) library(broom) library(caret) Quit RStudio. You’re done setting up! Optional: For a refresher on RStudio features, watch this video. It also shows you how to customize the layout and color scheme of RStudio. Troubleshooting Problem: You are on a Mac and getting the following error: Error: package or namespace load failed for ‘ggplot2’ in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]): there is no package called ‘rlang’ Here’s how to fix it: First install the suite of Command Line Tools for Mac using the instructions here. Next enter install.packages(\"rlang\") in the Console. Finally check that entering library(ggplot2) gives no errors. You should be good now for dplyr, readr, and rmarkdown. "],["introductions.html", "Topic 1 Introductions Envisioning a Community of Learners Explorations", " Topic 1 Introductions Slides from today are available here. Envisioning a Community of Learners Directions: In your groups, please first introduce yourselves in whatever way you feel appropriate (e.g., preferred name, pronouns, how you’re feeling at the moment, things you’re looking forward to). When everyone is ready, discuss the 3 prompts below and record thoughts in this Google Doc. The instructor will summarize responses from both sections to create a resource that everyone can use. Prompts: Wonderful leaders in the Macalester community have complied the following set of agreements to consider when working in small groups. Which of the following do you think are most important to keep in mind for our time together in this course and why? Confidentiality: Take the lesson, not the story Use “I” statements; Speak your truth W.A.I.T. (Why Am I Talking/Why Aren’t I Talking) Listen for understanding Unpack assumptions Extend and receive grace Understand impact vs. intention Breathe and lean into discomfort Accept non-closure What strategies have you found work well for you to succeed both in and out of class in these times? What are some things that have contributed to positive experiences in your courses that you would like to see again this module? What has contributed to negative experiences that you would like to prevent? Explorations Each of the data contexts below prompts a broad research goal. For each, sharpen the focus of that goal by coming up with more targeted research questions that: Can be studied with a regression exploration Can be studied with a classification exploration Can be studied with an unsupervised learning exploration Are there harms that you anticipate arising from the collection of data or its analysis? Context 1: The New York Times is trying to better understand the popularity of its different articles with the hope of using this understanding to improve hiring of writers and improve their website layout. Context 2: The Minnesota Department of Health is trying to better understand the different health trajectories of people who have contracted a certain illness to improve funding for relevant health services. Context 3: The campaign management team for a political candidate is trying to better understand voter turnout in different regions of the country to run a better campaign in the next election. "],["evaluating-regression-models.html", "Topic 2 Evaluating Regression Models Learning Goals Exercises", " Topic 2 Evaluating Regression Models Learning Goals Create and interpret residuals vs. fitted, residuals vs. predictor plots to identify improvements in modeling and address ethical concerns Interpret MSE, RMSE, MAE, and R-squared in a contextually meaningful way We’ll also start to develop some new ideas relating to our next topics. Slides from today are available here. Exercises You can download a template RMarkdown file to start from here. Context We’ll be working with a dataset containing physical measurements on 80 adult males. These measurements include body fat percentage estimates as well as body circumference measurements. fatBrozek: Percent body fat using Brozek’s equation: 457/Density - 414.2 fatSiri: Percent body fat using Siri’s equation: 495/Density - 450 density: Density determined from underwater weighing (gm/cm^3). age: Age (years) weight: Weight (lbs) height: Height (inches) neck: Neck circumference (cm) chest: Chest circumference (cm) abdomen: Abdomen circumference (cm) hip: Hip circumference (cm) thigh: Thigh circumference (cm) knee: Knee circumference (cm) ankle: Ankle circumference (cm) biceps: Biceps (extended) circumference (cm) forearm: Forearm circumference (cm) wrist: Wrist circumference (cm) It takes a lot of effort to estimate body fat percentage accurately through underwater weighing. The goal is to build the best predictive model for fatSiri using just circumference measurements, which are more easily attainable. (We won’t use fatBrozek or density as predictors because they’re other outcome variables.) library(readr) library(ggplot2) library(dplyr) library(broom) library(tidymodels) tidymodels_prefer() bodyfat &lt;- read_csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) # Remove the fatBrozek and density variables bodyfat &lt;- bodyfat %&gt;% select(-fatBrozek, -density) Class investigations We’ll work through this section together to review concepts and code. You’ll then work on the remainder of the exercises in your groups. # Exploratory plots # Univariate distribution of outcome ggplot(bodyfat, aes(???)) + geom_???() # Scatterplot of fatSiri vs. weight ggplot(bodyfat, aes(???)) + geom_???() Let’s fit a linear regression model to predict body fat percentage from weight. mod1 &lt;- lm(??? ~ ???, data = bodyfat) We can use the augment() function from the broom package to augment our original data with useful information from the model fitting. In particular, residuals are stored in the .fitted column, and fitted (predicted) values for the cases supplied in newdata are stored in the .fitted column. mod1_output &lt;- broom::augment(mod1, newdata = bodyfat) head(mod1_output) We can use the augment() output to compute error metrics, and glance() to obtain R^2: mod1 %&gt;% augment() %&gt;% summarize( mse = mean((fatSiri - .fitted)^2), rmse = sqrt(mse), mae = mean(abs(fatSiri - .fitted)) ) # R-squared - interpretation? (unit-less) mod1 %&gt;% glance() %&gt;% select(r.squared) …and to create residual plots: # Univariate plot of residuals mod1 %&gt;% augment() %&gt;% ggplot(aes(x = ???)) + geom_histogram() + theme_minimal() # Fitted vs. Residual plot mod1 %&gt;% augment() %&gt;% ggplot(aes(x = ???, y = ???)) + geom_point() + geom_smooth() + geom_hline(yintercept = 0) + theme_minimal() # Predictor vs. Residual plot mod1 %&gt;% augment() %&gt;% left_join(bodyfat) %&gt;% #Merge the remaining variables into data set ggplot(aes(x = height, y = ???)) + #note patterns in residual/error with height geom_point() + geom_smooth() + geom_hline(yintercept = 0) + theme_minimal() Exercise 1 First decide on what you think would be a good model by picking variables based on context. Fit this model, calling it mod_initial. (Remember that you can include several predictors with a + in the model formula - like y ~ x1+x2.) # Code to fit initial model Use residual plot explorations to check if you need to update your model. # Residual plot explorations Fit your updated model, and call it model_updated. # Code to fit updated model Exercise 2 Compute and contextually interpret relevant evaluation metrics for your model. # Code to compute evaluation metrics Exercise 3 Now that you’ve selected your best model, deploy it in the real world by applying it to a new set of 172 adult males. You’ll need to update the newdata argument of the broom::augment() code to use bodyfat_test instead of bodyfat. bodyfat_test &lt;- read_csv(&quot;https://www.dropbox.com/s/7gizws208u0oywq/bodyfat_test.csv?dl=1&quot;) Compare your evaluation metrics from Exercise 2 the metrics here. What do you notice? (Note: this observation is just based on your one particular fitted model. You’ll make some more comprehensive observations in the next exercise.) In general, do you think that models with more or less variables would perform better in this comparison? Explain. Exercise 4 The code below systematically looks at the same comparison that you made in Exercise 3 but for every possible linear regression model formed from inclusion/exclusion of the predictors (without transformations or interactions). Run the code to make a plot of the results of this systematic investigation. (Feel free to inspect the code if you’re curious, but otherwise, don’t worry about understanding it fully.) What do you notice? What do you wonder? get_maes &lt;- function(mod, train_data, test_data) { mod_output_train &lt;- broom::augment(mod, newdata = train_data) mod_output_test &lt;- broom::augment(mod, newdata = test_data) train_mae &lt;- mean(abs(mod_output_train$.resid)) test_mae &lt;- mean(abs(mod_output_test$.resid)) c(train_mae, test_mae) } possible_predictors &lt;- setdiff(colnames(bodyfat), c(&quot;fatSiri&quot;, &quot;hipin&quot;)) results &lt;- bind_rows(lapply(1:13, function(i) { combos &lt;- combn(possible_predictors, i) bind_rows(lapply(seq_len(ncol(combos)), function(j) { formula &lt;- paste(&quot;fatSiri ~&quot;, paste(combos[,j], collapse = &quot;+&quot;)) mod &lt;- lm(as.formula(formula), data = bodyfat) maes &lt;- get_maes(mod = mod, train_data = bodyfat, test_data = bodyfat_test) tibble( form = formula, train_mae = maes[1], test_mae = maes[2], num_predictors = i ) })) })) # Relabel the num_predictors variable results &lt;- results %&gt;% mutate(num_predictors = paste(&quot;# predictors:&quot;, num_predictors)) %&gt;% mutate(num_predictors = factor(num_predictors, levels = paste(&quot;# predictors:&quot;, 1:13))) # Plot results ggplot(results, aes(x = train_mae, y = test_mae, color = num_predictors)) + geom_point() + coord_cartesian(xlim = c(0,7.5), ylim = c(0,7.5)) + geom_abline(slope = 1, intercept = 0) + facet_wrap(~num_predictors) + guides(color = FALSE) + labs(x = &quot;MAE on original data&quot;, y = &quot;MAE on new data on 172 men&quot;) + theme_classic() "],["overfitting.html", "Topic 3 Overfitting Learning Goals The tidymodels package Exercises", " Topic 3 Overfitting Learning Goals Explain why training/in-sample model evaluation metrics can provide a misleading view of true test/out-of-sample performance Implement testing and training sets in R using the tidymodels package Slides from today are available here. The tidymodels package (If you have not already installed the tidymodels package, install it with install.packages(\"tidymodels\").) Over this course, we will looking at a broad but linked set of specialized tools applied in statistical machine learning. Specialized tools generally require specialized code. Each tool has been developed separately and coded in a unique way. In order to facilitate and streamline the user experience, there have been attempts at creating a uniform interface, such as the caret R package. The developers of the caret package are no longer maintaining those packages. They are working on a newer package, called tidymodels. In this class, we will use the tidymodels package, which uses the tidyverse syntax you learned in Stat 155. The tidymodels package is a relatively new package and continues to be developed as we speak. This means that I’m learning with you and in a month or two, there may be improved functionality. As Prof. Heggeseth introduced in the R code videos, we have a general workflow structure that includes a model specification and a recipe (formula + preprocessing steps). # Load the package library(tidymodels) tidymodels_prefer() # Set the seed for the random number generator set.seed(123) # Specify Model model_spec &lt;- linear_reg() %&gt;% # type of model set_engine(engine = ____) #%&gt;% # algorithm to fit the model set_args(__) %&gt;% # hyperparameters/tuning parameters are needed for some models set_mode(__) # regression or classification # Specify Recipe (if you have preprocessing steps) rec &lt;- recipe(formula, data) %&gt;% step_{FILLIN}() %&gt;% # e.g., step_filter() to subset the rows step_{FILLIN}() # e.g., step_lincomb() to remove all predictors which are perfect linear combinations of another # Create Workflow (Model + Recipe) model_wf &lt;- workflow() %&gt;% add_recipe(rec) %&gt;% #or add_formula() add_model(model_spec) We can fit that workflow to training data. # Fit Model to training data (without a recipe) fit_model &lt;- fit(model_spec, formula, data_train) # Fit Model &amp; Recipe to training data fit_model &lt;- fit(model_wf, data_train) And then we can evaluate that fit model on testing data (new data that has not been used to fit the model). # Evaluate on testing data model_output &lt;- fit_model %&gt;% predict(new_data = data_test) %&gt;% # this function will apply recipe to new_data and do prediction bind_cols(data_test) reg_metrics &lt;- metric_set(rmse, rsq, mae) model_output %&gt;% reg_metrics(truth = __, estimate = .pred) The power of tidymodels is that it allows us to streamline the vast world of machine learning techniques into one common syntax. On top of \"lm\", there are many other different machine learning methods that we can use. In the exercises below, you’ll need to adapt the code above to fit a linear regression model (engine = \"lm\"). Exercises You can download a template RMarkdown file to start from here. Context We’ll be working with a dataset containing physical measurements on 80 adult males. These measurements include body fat percentage estimates as well as body circumference measurements. fatBrozek: Percent body fat using Brozek’s equation: 457/Density - 414.2 fatSiri: Percent body fat using Siri’s equation: 495/Density - 450 density: Density determined from underwater weighing (gm/cm^3). age: Age (years) weight: Weight (lbs) height: Height (inches) neck: Neck circumference (cm) chest: Chest circumference (cm) abdomen: Abdomen circumference (cm) hip: Hip circumference (cm) thigh: Thigh circumference (cm) knee: Knee circumference (cm) ankle: Ankle circumference (cm) biceps: Biceps (extended) circumference (cm) forearm: Forearm circumference (cm) wrist: Wrist circumference (cm) It takes a lot of effort to estimate body fat percentage accurately through underwater weighing. The goal is to build the best predictive model for fatSiri using just circumference measurements, which are more easily attainable. (We won’t use fatBrozek or density as predictors because they’re other outcome variables.) library(readr) library(ggplot2) library(dplyr) library(broom) library(tidymodels) bodyfat_train &lt;- read_csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) # Remove the fatBrozek, density, and hipin variables bodyfat_train &lt;- bodyfat_train %&gt;% select(-fatBrozek, -density, -hipin) Exercise 1: 5 models Consider the 5 models below: lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &#39;lm&#39;) %&gt;% set_mode(&#39;regression&#39;) mod1 &lt;- fit(lm_spec, fatSiri ~ age+weight+neck+abdomen+thigh+forearm, data = bodyfat_train) mod2 &lt;- fit(lm_spec, fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps, data = bodyfat_train) mod3 &lt;- fit(lm_spec, fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps+chest+hip, data = bodyfat_train) mod4 &lt;- fit(lm_spec, fatSiri ~ ., # The . means all predictors data = bodyfat_train) bf_recipe &lt;- recipe(fatSiri ~ ., data = bodyfat_train) %&gt;% step_normalize(all_numeric_predictors()) bf_wf &lt;- workflow() %&gt;% add_recipe(bf_recipe) %&gt;% add_model(lm_spec) mod5 &lt;- fit(bf_wf, data = bodyfat_train) STAT 155 review: Look at the tidy() of mod1. Contextually interpret the coefficient for the weight predictor. Is anything surprising? Why might this be? Explain how mod5 is different than mod4. You may want to look at bf_recipe %&gt;% prep(bodyfat_train) %&gt;% juice() to see the preprocessed training data. Which model will have the lowest training RMSE, and why? Explain before calculating (that is part d). Compute the training RMSE for models 1 through 5 to check your answer for part c. Write a sentence interpreting one of values of RMSE in context. Which model do you think is the “best”? You may calculate MAE and R squared as well to justify your answer. f, Which model do you think will perform worst on new test data? Why? Exercise 2: Visualizing Predictions Sequentially run the code below, ending before pipe, comma, or +. For each row of code below, discuss what it does. Add comments to the end of the line after the pipe (with # in front) to explain what each line does. mod5 %&gt;% #comment here tidy() %&gt;% # slice(-1) %&gt;% # mutate(lower = estimate - 1.96*std.error, upper = estimate + 1.96*std.error) %&gt;% # ggplot() + # geom_vline(xintercept=0, linetype=4) + # geom_point(aes(x=estimate, y=term)) + # geom_segment(aes(y=term, yend=term, x=lower, xend=upper), arrow = arrow(angle=90, ends=&#39;both&#39;, length = unit(0.1, &#39;cm&#39;))) + # labs(x = &#39;Coefficient estimate (95% CI)&#39;, y = &#39;Feature&#39;) + # theme_classic() # Sequentially run each line of code (below), end before pipe or comma. For each row of code below, discuss what it does. Add comments to the end of the line after the pipe (with # in front) to explain what each line does. bodyfat_train %&gt;% # mutate(id = row_number()) %&gt;% # pivot_longer(-c(fatSiri, id), names_to = &#39;key&#39;, values_to = &#39;value&#39;) %&gt;% # right_join( # (mod4 %&gt;% tidy() %&gt;% slice(-1) %&gt;% select(term, estimate)), # by = c(&#39;key&#39;=&#39;term&#39;) ) %&gt;% #right_join finishes here mutate(effect = value * estimate) %&gt;% # ggplot(aes(x = effect, y = key)) + # geom_boxplot() + # geom_vline(xintercept = 0, color = &#39;grey&#39;) + # labs(x = &#39;Effect/Contribution to Predicted BodyFat Percent&#39;, y = &#39;Feature&#39;) + # theme_classic() # Exercise 3: Evaluating the Test Data Now that you’ve thought about how well the models might perform on test data, deploy it in the real world by applying it to a new set of 172 adult males. You’ll need to update the new_data to use bodyfat_test instead of bodyfat_train. bodyfat_test &lt;- read_csv(&quot;https://www.dropbox.com/s/7gizws208u0oywq/bodyfat_test.csv?dl=1&quot;) # Use fit/trained models and evaluate on test data Calculate the test RMSE, MAE, and R squared for all five of the models. Look back to Exercise 1 and see which model you thought was “best” based on the training data. Is that the “best” model in terms of predicting on new data? Explain. In “real life” we only have one data set. To get a sense of predictive performance on new test data, we could split our data into two groups. Discuss pros and cons of ways you might split the data. How big should the training set be? How big should the testing set be? Exercise 4: Overfitting If you have time, consider the following relationship. Imagine a set of predictions that is overfit to this training data. You are not limited to lines. Draw a picture of that function of predictions on a piece of paper. set.seed(123) data_train &lt;- tibble::tibble( x = runif(15,0,7), y = x^2 + rnorm(15,sd = 7) ) data_train %&gt;% ggplot(aes(x = x, y = y)) + geom_point() + theme_classic() "],["homework-1.html", "Homework 1 Project Work Ethics in ML Portfolio Work", " Homework 1 **Submit by Friday, Feb 3 at 11:59pm to Moodle. Please turn in a single PDF document containing (1) your responses for the Project Work and Ethics in ML sections and (2) a LINK to the Google Doc with your responses for the Portfolio Work section. Project Work Goal: Find a dataset (or datasets) to use for your final project, and start to get to know the data. Details: Your dataset(s) should allow you to perform a (1) regression, (2) classification, and (3) unsupervised learning analysis. The following resources are good places to start looking for data: Google Dataset Search Kaggle UCI Machine Learning Repository Harvard Dataverse Even if you end up working with a partner on the project (which isn’t required - working alone is fine), please complete this initial work individually. It’s fine if you and a potential/future partner end up using the same dataset and collaborate on the finding of data, but complete the short bit of writing (below) individually. Check in with the instructor early if you need help. Deliverables: Write 1-2 paragraphs (no more than 350 words) summarizing: The information in the dataset(s) and the context behind the data. Use the prompts below to guide your thoughts. (Note: in some situations, there may be incomplete information on the data context. That’s fine. Just do your best to summarize what information is available, and acknowledge the lack of information where relevant.) What are the cases? Broadly describe the variables contained in the data. Who collected the data? When, why, and how? 3 research questions 1 that can be investigated in a regression setting 1 that can be investigated in a classification setting 1 that can be investigated in an unsupervised learning setting Also make sure that you can read the data into R. You don’t need to do any analysis in R yet, but making sure that you can read the data will make the next steps go more smoothly. Ethics in ML Read the article Amazon scraps secret AI recruiting tool that showed bias against women. Write a short (roughly 250 words), thoughtful response about the themes and cautions that the article brings forth. Portfolio Work Setup: In addition to your submission here, you’ll want to collect your Portfolio work in a single document. To do so, complete the following steps: 1. Make a copy of this document. 2. For each Homework submission, copy your Portfolio responses into the appropriate space. For example, Homework 1 has three prompts: Overfitting, Evaluating Regression Models, and Cross-validation. In your portfolio, copy and paste your responses under the appropriate header. Page maximum: 2 pages of text (pictures don’t count) Organization: Your choice! Use titles and section headings that make sense to you. (It probably makes sense to have a separate section for each method.) Deliverables: Put your responses for this part in a Google Doc, and update the link sharing so that anyone with the link at Macalester College can edit. Include the URL for the Google Doc in your submission. Note: Some prompts below may seem very open-ended. This is intentional. Crafting good responses requires looking back through our material to organize the concepts in a coherent, thematic way, which is extremely useful for your learning. Concepts to address: Overfitting: The video used the analogy of a cat picture model to explain overfitting. Come up with your own analogy to explain overfitting. Evaluating regression models: Describe how residuals are central to the evaluation of regression models. Explain how they arise in quantitative evaluation metrics and how they are used in evaluation plots. Include examples of plots that show desirable and undesirable model behavior (feel free to draw them by hand if you wish) and what steps can be taken to address that undesirable behavior. Cross-validation: In your own words, explain the rationale for cross-validation in relation to overfitting and model evaluation. Describe the algorithm in your own words in at most 2 sentences. "],["r-resources.html", "R Resources Outside resources Example code", " R Resources Outside resources Lisa Lendway’s COMP/STAT 112 website (with code examples and videos) RStudio cheat sheets ggplot2 reference R Programming Wikibook Debugging in R Article Video Colors in R Data import tutorial Free online textbooks R for Data Science Exploratory Data Analysis with R Example code Creating new variables case_when() from the dplyr package is a very versatile function for creating new variables based on existing variables. This can be useful for creating categorical or quantitative variables and for creating indices from multiple variables. # Turn quant_var into a Low/Med/High version data &lt;- data %&gt;% mutate(cat_var = case_when( quant_var &lt; 10 ~ &quot;Low&quot;, quant_var &gt;= 10 &amp; quant_var &lt;= 20 ~ &quot;Med&quot;, quant_var &gt; 20 ~ &quot;High&quot; ) ) # Turn cat_var (A, B, C categories) into another categorical variable # (collapse A and B into one category) data &lt;- data %&gt;% mutate(new_cat_var = case_when( cat_var %in% c(&quot;A&quot;, &quot;B&quot;) ~ &quot;A or B&quot; cat_var==&quot;C&quot; ~ &quot;C&quot; ) ) # Turn a categorical variable (x1) encoded as a numerical 0/1/2 variable into a different quantitative variable # Doing this for multiple variables allows you to create an index data &lt;- data %&gt;% mutate(x1_score = case_when( x1==0 ~ 10, x1==1 ~ 20, x1==2 ~ 50 ) ) # Add together multiple variables with mutate data &lt;- data %&gt;% mutate(index = x1_score + x2_score + x3_score) "],["final-project.html", "Final Project Requirements Grading Rubric", " Final Project Requirements You will be analyzing a dataset using a regression and a classification analysis. An unsupervised learning analysis is no longer required for the project. Collaboration: You may work in teams of up to 3 members. Individual work is fine. The weekly homework assignments will note whether work for that week should be submitted individually or if just one team member should submit work. There will be a required synthesis of the weekly homework investigations at the end of the course. If working on a team, this should be done in groups, rather than individually. Final deliverables: Only one team member has to submit these materials to Moodle. The due date is Thursday, May 4th at 11:59pm CST. Submit a final knitted HTML file (must knit without errors) and corresponding Rmd file containing code for your analysis Include a 10-15 minute video presentation of your project that addresses the items in the Grading Rubric below. (Recording the presentation over Zoom is a good option for creating the video. You can record to your computer or to the cloud.) Upload the video itself to Moodle. If it’s too large, share a link to a recording on the web or in a shared drive. All team members should have an equal speaking role in the presentation. If a video presentation would be difficult for you and your team to make, you may instead submit an annotated set of slides. The speaker notes beneath each slide should contain what you would have said in the video presentation. Grading Rubric Data context (10 points) Clearly describe what the cases in the final clean dataset represent. Broadly describe the variables used in your analyses. Who collected the data? When, why, and how? Answer as much of this as the available information allows. Research questions (10 points) Research question(s) for the regression task make clear the outcome variable and its units. Research question(s) for the classification task make clear the outcome variable and its possible categories. HW3 investigations - Methods (10 points) Describe the models used in your HW3 project work investigations. Describe what you did to evaluate models. Indicate how you estimated quantitative evaluation metrics. Indicate what plots you used to evaluate models. Describe the goals / purpose of the methods used in the overall context of your research investigations. HW3 investigations - Results - Variable Importance (10 points) Summarize results from HW3 Investigations 1 (and 2, if applicable) on variable importance measures. Note: Investigation 2 won’t be applicable to your project if you only have categorical predictors. Summarization should show evidence of acknowledging the data context in thinking about the sensibility of these results. HW3 investigations - Summary (10 points) If it was appropriate to fit a GAM for your investigations (having some quantitative predictors), show plots of estimated functions for each predictor, and provide some general interpretations. Compare the different models tried in HW3 in light of evaluation metrics, plots, variable importance, and data context. Display evaluation metrics for different models in a clean, organized way. This display should include both the estimated metric as well as its standard deviation. (Hint: you should be using caret_mod$results.) Broadly summarize conclusions from looking at these evaluation metrics and their measures of uncertainty. Summarize conclusions from residual plots from initial models (don’t have to display them though). Decide an overall most preferable model. Show and interpret some representative examples of residual plots for your final model. Does the model show acceptable results in terms of any systematic biases? Interpret evaluation metric(s) for the final model in context with units. Does the model show an acceptable amount of error? Classification analysis - Methods (10 points) Indicate 2 different methods used to answer your classification research question. Describe what you did to evaluate the 2 models explored. Indicate how you estimated quantitative evaluation metrics. Describe the goals / purpose of the methods used in the overall context of your research investigations. Classification analysis - Results - Variable Importance (10 points) Summarize results about variable importance measures in your classification analysis. Summarization should show evidence of acknowledging the data context in thinking about the sensibility of these results. Classification analysis - Summary (10 points) Compare the 2 different classification models tried in light of evaluation metrics, variable importance, and data context. Display evaluation metrics for different models in a clean, organized way. This display should include both the estimated metric as well as its standard deviation. (This won’t be available from OOB error estimation. If using OOB, don’t worry about reporting the SD.) Broadly summarize conclusions from looking at these evaluation metrics and their measures of uncertainty. Decide an overall most preferable model. Interpret evaluation metric(s) for the final model in context. Does the model show an acceptable amount of error? If using OOB error estimation, display the test (OOB) confusion matrix, and use it to interpret the strengths and weaknesses of the final model. Code (20 points) Knitted, error-free HTML and corresponding Rmd file submitted Code corresponding to all analyses above is present and correct "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
