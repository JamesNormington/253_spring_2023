[["homework-4-solution-key.html", "Homework 4 SOlution Key Project Work Ethics in ML Portfolio Work", " Homework 4 SOlution Key **Due Friday, April 14th at 11:59pm CST on Moodle. Deliverables: Submit a single PDF containing your responses for Course Engagement. Project Work There is no new project work for this assignment. However – you should begin thinking about some classification models you can use for your Final Project. Take a look at the Final Project page – you’ll need to have explored at least two classification models, and decided on a final “best” model Ethics in ML Read the article How to Support your Data Interpretations. Write a short (roughly 250 words), thoughtful response about the ideas that the article brings forth. Which pillar(s) do you think is/are hardest to do well for groups that rely on data analytics, and why? As always, grade on good faith effort. Portfolio Work Revisions: Make any revisions desired to previous concepts. Make it clear to the preceptors what you want re-read (highlighted, commented, etc.) New concepts to address: Decision trees: Algorithmic understanding: Consider a dataset with two predictors: x1 is categorical with levels A, B, or C. x2 is quantitative with integer values from 1 to 100. How many different splits must be considered when recursive binary splitting attempts to make a split? Explain. (2 sentences max.) Three for each level of x1, and 100 for each possible value of x2. Explain the “recursive”, “binary”, and “splitting” parts of the recursive binary splitting algorithm. Make sure to discuss the Gini index as a measure of node (im)purity and how it is defined differently for classification and regression trees. “Splitting” refers to the result of the predictor and predictor value which yields the lowest weighted Gini index (or, highest node purity) when splitting a node in two. For classification, the weighted Gini index measures node purity as the proportion of classes at each node; less variance in classes leads to lower node impurity. For regression, the weighted Gini index measures node purity as the variance of the response values within each node. The algorithm is “recursive” because the splitting decision is made in repeated succession, it is “binary” because each split yields two branches. Bias-variance tradeoff: What tuning parameters control the performance of the method? How do low/high values of the tuning parameters relate to bias and variance of the learned model? (3 sentences max.) There are several answers here. Students should mention at least one of the following cost complexity: if this value is too large, too-small of trees will be grown, which will yield low variance but high bias. If it’s too small, too-large trees will be grown, which will yield high variance but low bias. min_n (number of observations needed in a node to attempt a split): if this value is too large, too-small trees will be grown, which will yield low variance but high bias. If it’s too small, too-large trees will be grown, which will yield high variance but low bias. depth: if this value is too large, too-large trees will be grown, which will yield high variance but low bias. If it’s too small, too-small trees will be grown, which will yield low variance but high bias. Parametric / nonparametric: Where (roughly) does this method fall on the parametric-nonparametric spectrum, and why? (3 sentences max.) This is fully nonparametric. There is no defined form dependent on parameters; it’s a sequence of binary logical checks depending on the predictors present. Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? (3 sentences max.) Not really. Assuming an adequate number of predictors values are tested in the binary splitting algorithm, the quality of the splits don’t depend on the scale. The only way scaling does matter is in defining which and how many split values to test. Computational time: Recursive binary splitting does not find the overall optimal sequence of splits for a tree. What type of behavior is this? What method have we seen before that also exhibits this type of behavior? Briefly explain the parallels between these methods and what implications this have for computational time. (5 sentences max.) Greedy. We saw this type of behavior when studying stepwise (forward/backward) selection. Both methods make the optimal choice in the short term – stepwise regression selects/drops the variable which leads to the next best model (conditional on the current), while decision trees make the optimal split given the previous branches/nodes. Both of these methods perform greedy searches due to computational reasons. In stepwise regression, rather than testing all \\(2^p\\) possible models, only \\(1 + \\frac{p(p+1)}{2}\\) models need to be fit. For decision trees, it’s (usually) computationally infeasible to consider all possible decision trees, so the recursive binary splitting algorithm is greedy to reduce the space we search through. Interpretation of output: Explain the rationale behind the variable importance measures that decision trees provide. (4 sentences max.) Variable importance in trees is measured by the total decrease in node impurities from splitting on that variable. This makes sense, since in the extreme case, if \\(x_j\\) yielded two completely pure nodes, then it means \\(x_j\\) is very important in predicting \\(y\\), and the total decrease in node impurity in the case will be very high. If instead \\(x_j\\) is not important, the node impurity won’t be decreased. Bagging &amp; Random Forests: Algorithmic understanding: Explain the rationale for extending single decision trees to bagging models and then to random forest models. What specific improvements to predictive performance are being sought? (5 sentences max.) Single decision trees are highly unstable – that is, the predicted values have very high variance. Bagging leverages the fact that average predictions tend to be less variable than single predictions, so we combine several trees to make predictions that will have less variance. However, because bagging relies on bootstrapped samples of our training sample (which themselves are highly correlated), it will often yield highly correlated trees (that is, trees which are very similar). The variance of predicted values from averages across correlated models is still high. To de-correlate these trees, we introduce a step to randomly select a subset of predictor to consider; this gives us a random forest. Again, the goal here is to reduce the variance of the predicted values. Bias-variance tradeoff: What tuning parameters control the performance of the method? How do low/high values of the tuning parameters relate to bias and variance of the learned model? (3 sentences max.) mtry, the number of predictors to randomly choose for consideration at each split of the tree. Too large of values for mtry will cause the trees to be highly correlated, which will reduce the variance of within-model trees. However, this will ultimately lead to overfitting. In the extreme case, when mtry = p, this is equivalent to bagging, which has high variance (but low bias). Too small of values for mtry will cause the trees to not use enough data in each split, which will reduce the variance but induce bias. In the extreme case, when mtry = 1, you are only using one randomly selected predictor at each split, causing underfitting. Parametric / nonparametric: Where (roughly) does this method fall on the parametric-nonparametric spectrum, and why? (3 sentences max.) Fully non-parametric. Since these methods are aggregated decision trees, which are nonparametric, this is nonparametric. Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? (3 sentences max.) No. Similar to my answer above, the quality of the splits won’t depend on the scale assuming you are searching through an adequate space of predictor values. Computational time: Explain why cross-validation is computationally intensive for many-tree algorithms. What method do we have to reduce this computational burden, and why is it faster? (5 sentences max.) Recursive binary splitting is a computationally intensive process, since it involves testing splits for (often, hundreds or thousands) \\(p\\) predictors. K-fold cross-validation makes this even more expensive, because you have to grow \\(K\\) trees per bootstrap resample! An alternative which is computationally less expensive is out-of-bag error estimation, which only fits one tree per bootstrap re-sample. Interpretation of output: Explain the rationale behind the variable importance measures that random forest models provide. (4 sentences max.) Impurity: like decision trees, variable importance in random forests can be thought of as total decreases in node impurities. (see answer above) Permutation: Permuting, or shuffling, the values of a truly important predictor destroys its relationship with the response \\(Y\\). Thus, those predictors which yield significantly poorer testing metrics after permutation must be important to the predictive power of the algorithm. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
