[["index.html", "STAT 253: Statistical Machine Learning Welcome!", " STAT 253: Statistical Machine Learning Welcome! Image source This is the class manual for Statistical Machine Learning (STAT 253) at Macalester College for Spring 2021. The content here was made by Leslie Myint and draws upon our class textbook, An Introduction to Statistical Learning with Applications in R, and on materials prepared by Alicia Johnson. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["schedule.html", "Schedule", " Schedule The schedule below is a tentative outline of our plans for the module. Before each class period, please watch the indicated videos and check on your understanding by actively reviewing the associated Learning Objectives. The readings listed below are optional but serve as a nice complement to the videos and class activities. Readings refer to chapters/sections in the Introduction to Statistical Learning (ISLR) textbook (available online here). Week 1: 3/18 - 3/19 Day(s) Topic Videos/Readings Slides 3/18 Introductions ISLR: Chap 1, Chap 2 - Section 2.1 (Skip 2.1.2, 2.1.3 for now.) 3/19 Evaluating Regression Models Evaluating Regression Models ISLR: 2.2 PDF Homework 1 due Friday, 3/26 at midnight CST Week 2: 3/22 - 3/25 Day(s) Topic Videos/Readings Slides 3/22 Overfitting and Cross-validation Overfitting Cross-validation ISLR: 5.1 PDF PDF 3/23 Subset Selection Variable Subset Selection ISLR: 6.1 PDF 3/24-3/25 LASSO (Shrinkage/Regularization) LASSO (Shrinkage/Regularization) ISLR: 6.2 PDF Homework 1 due Friday, 3/26 at midnight CST Week 3: 3/29 - 4/1 Day(s) Topic Videos/Readings Slides 3/29 KNN Regression and the Bias-Variance Tradeoff KNN Regression and the Bias-Variance Tradeoff ISLR: 2.2.2 for the bias-variance tradeoff; 3.5 for KNN regression PDF 3/30 Modeling Nonlinearity: Polynomial Regression and Splines Modeling Nonlinearity: Polynomial Regression and Splines ISLR: 7.1-7.4 PDF 3/31 Local Regression and Generalized Additive Models Local Regression and Generalized Additive Models ISLR: 7.6-7.7 PDF 4/1 Review: Regression Homework 2 due Friday, 4/2 at midnight CST "],["learning-objectives.html", "Learning Objectives", " Learning Objectives Learning objectives for our course topics are listed below. Use these to guide your synthesis of video and reading material. Introduction to Statistical Machine Learning Formulate research questions that align with regression, classification, or unsupervised learning tasks Evaluating Regression Models Create and interpret residuals vs. fitted, residuals vs. predictor plots to identify improvements in modeling and address ethical concerns Interpret MSE, RMSE, MAE, and R-squared in a contextually meaningful way Overfitting and cross-validation Explain why training/in-sample model evaluation metrics can provide a misleading view of true test/out-of-sample performance Accurately describe all steps of cross-validation to estimate the test/out-of-sample version of a model evaluation metric Explain what role CV has in a predictive modeling analysis and its connection to overfitting Explain the pros/cons of higher vs. lower k in k-fold CV in terms of sample size and computing time Subset selection Clearly describe the forward and backward stepwise selection algorithm and why they are examples of greedy algorithms Compare best subset and stepwise algorithms in terms of optimality of output and computational time LASSO (shrinkage/regularization) Explain how ordinary and penalized least squares are similar and different with regard to (1) the form of the objective function and (2) the goal of variable selection Explain how the lambda tuning parameter affects model performance and how this is related to overfitting KNN Regression and the Bias-Variance Tradeoff Clearly describe / implement by hand the KNN algorithm for making a regression prediction Explain how the number of neighbors relates to the bias-variance tradeoff Explain the difference between parametric and nonparametric methods Explain how the curse of dimensionality relates to the performance of KNN (not in the video–will be discussed in class) Modeling Nonlinearity: Polynomial Regression and Splines Explain the advantages of splines over global transformations and other types of piecewise polynomials Explain how splines are constructed by drawing connections to variable transformations and least squares Explain how the number of knots relates to the bias-variance tradeoff Local Regression and Generalized Additive Models Clearly describe the local regression algorithm for making a prediction Explain how bandwidth (span) relate to the bias-variance tradeoff Describe some different formulations for a GAM (how the arbitrary functions are represented) Explain how to make a prediction from a GAM Interpret the output from a GAM "],["r-and-rstudio-setup.html", "R and RStudio Setup Troubleshooting", " R and RStudio Setup Before class on Friday, March 19, you should follow these instructions to set up the software that we’ll be using throughout the semester. Even if you’ve already downloaded both R and RStudio, you’ll want to re-download to make sure that you have the most current versions. Highly recommended: Change the default file download location for your internet browser. Generally by default, internet browsers automatically save all files to the Downloads folder on your computer. This does not encourage good file organization practices. It is highly recommended that you change this option so that your browser asks you where to save each file before downloading it. This page has information on how to do this for the most common browsers. Required: Download R and RStudio FIRST: Download R here. You will see three links “Download R for …” Choose the link that corresponds to your computer. As of March 16, 2021, the latest version of R is 4.0.4. SECOND: Download RStudio here. Click the button under step 2 to install the version of RStudio recommended for your computer. As of March 16, 2021, the latest version of RStudio is 1.4.1106. Suggested: Watch this video made by Professor Lisa Lendway that describes essential configuration options for RStudio. Required: Install required packages. An R package is an extra bit of functionality that will help us in our data analysis efforts in a variety of ways. Open RStudio and click inside the Console pane (by default, the bottom left pane). Copy and paste the following command into the Console. You should see the text below appear to the right of the &gt;, which is called the R prompt. After you paste, hit Enter. install.packages(c(&quot;ggplot2&quot;, &quot;dplyr&quot;, &quot;readr&quot;, &quot;rmarkdown&quot;, &quot;broom&quot;, &quot;caret&quot;)) You will see a lot of text from status messages appearing in the Console as the packages are being installed. Wait until you see the &gt; again. Enter the command library(ggplot2) and hit enter. If you see the message Error in library(ggplot2) : there is no package called ggplot2, then there was a problem installing this package. Jump down to the Troubleshooting section below. (Any other messages that appear are fine, and a lack of any messages is also fine.) Repeat the above step for the commands: library(dplyr) library(readr) library(rmarkdown) library(broom) library(caret) Quit RStudio. You’re done setting up! Optional: For a refresher on RStudio features, watch this video. It also shows you how to customize the layout and color scheme of RStudio. Troubleshooting Problem: You are on a Mac and getting the following error: Error: package or namespace load failed for ‘ggplot2’ in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]): there is no package called ‘rlang’ Here’s how to fix it: First install the suite of Command Line Tools for Mac using the instructions here. Next enter install.packages(\"rlang\") in the Console. Finally check that entering library(ggplot2) gives no errors. You should be good now for dplyr, readr, and rmarkdown. "],["introductions.html", "Topic 1 Introductions Envisioning an Ideal Module 4 Explorations", " Topic 1 Introductions Slides from today are available here. Envisioning an Ideal Module 4 Directions: In your breakout rooms, please first introduce yourselves in whatever way you feel appropriate (e.g., preferred name, pronouns, how you’re feeling at the moment, things you’re looking forward to). When everyone is ready, discuss the 3 prompts below and record thoughts in this Google Doc. The instructor will summarize responses from both sections to create a resource that everyone can use. Prompts: Wonderful leaders in the Macalester community have complied the following set of agreements to consider when working in small groups. Which of the following do you think are most important to keep in mind for our time together in this course and why? Confidentiality: Take the lesson, not the story Use “I” statements; Speak your truth W.A.I.T. (Why Am I Talking/Why Aren’t I Talking) Listen for understanding Unpack assumptions Extend and receive grace Understand impact vs. intention Breathe and lean into discomfort Accept non-closure What strategies have you found work well for you to succeed both in and out of class in these times? What are some things that have contributed to positive experiences in your courses that you would like to see again this module? What has contributed to negative experiences that you would like to prevent? Explorations Each of the data contexts below prompts a broad research goal. For each, sharpen the focus of that goal by coming up with more targeted research questions that: Can be studied with a regression exploration Can be studied with a classification exploration Can be studied with an unsupervised learning exploration Are there harms that you anticipate arising from the collection of data or its analysis? Context 1: The New York Times is trying to better understand the popularity of its different articles with the hope of using this understanding to improve hiring of writers and improve their website layout. Context 2: The Minnesota Department of Health is trying to better understand the different health trajectories of people who have contracted a certain illness to improve funding for relevant health services. Context 3: The campaign management team for a political candidate is trying to better understand voter turnout in different regions of the country to run a better campaign in the next election. "],["evaluating-regression-models.html", "Topic 2 Evaluating Regression Models Learning Goals Exercises", " Topic 2 Evaluating Regression Models Learning Goals Create and interpret residuals vs. fitted, residuals vs. predictor plots to identify improvements in modeling and address ethical concerns Interpret MSE, RMSE, MAE, and R-squared in a contextually meaningful way We’ll also start to develop some new ideas relating to our next topics. Slides from today are available here. Exercises You can download a template RMarkdown file to start from here. Context We’ll be working with a dataset containing physical measurements on 80 adult males. These measurements include body fat percentage estimates as well as body circumference measurements. fatBrozek: Percent body fat using Brozek’s equation: 457/Density - 414.2 fatSiri: Percent body fat using Siri’s equation: 495/Density - 450 density: Density determined from underwater weighing (gm/cm^3). age: Age (years) weight: Weight (lbs) height: Height (inches) neck: Neck circumference (cm) chest: Chest circumference (cm) abdomen: Abdomen circumference (cm) hip: Hip circumference (cm) thigh: Thigh circumference (cm) knee: Knee circumference (cm) ankle: Ankle circumference (cm) biceps: Biceps (extended) circumference (cm) forearm: Forearm circumference (cm) wrist: Wrist circumference (cm) It takes a lot of effort to estimate body fat percentage accurately through underwater weighing. The goal is to build the best predictive model for fatSiri using just circumference measurements, which are more easily attainable. (We won’t use fatBrozek or density as predictors because they’re other outcome variables.) library(readr) library(ggplot2) library(dplyr) library(broom) bodyfat &lt;- read_csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) # Remove the fatBrozek and density variables bodyfat &lt;- bodyfat %&gt;% select(-fatBrozek, -density) Class investigations We’ll work through this section together to review concepts and code. You’ll then work on the remainder of the exercises in your groups. # Exploratory plots # Univariate distribution of outcome ggplot(bodyfat, aes(???)) + geom_???() # Scatterplot of fatSiri vs. weight ggplot(bodyfat, aes(???)) + geom_???() Let’s fit a linear regression model to predict body fat percentage from weight. mod1 &lt;- lm(??? ~ ???, data = bodyfat) We can use the augment() function from the broom package to augment our original data with useful information from the model fitting. In particular, residuals are stored in the .fitted column, and fitted (predicted) values for the cases supplied in newdata are stored in the .fitted column. mod1_output &lt;- broom::augment(mod1, newdata = bodyfat) head(mod1_output) We can use the augment() output to compute error metrics… # MSE - what is the interpretation with units? mean(mod1_output$.resid^2) # RMSE - what is the interpretation with units? mean(mod1_output$.resid^2) %&gt;% sqrt() # MAE - what is the interpretation with units? mean(abs(mod1_output$.resid)) # R-squared - interpretation? (unit-less) 1 - (var(mod1_output$.resid) / var(mod1_output$fatSiri)) …and to create residual plots: # Quick plot ggplot(mod1_output, aes(x = .fitted, y = .resid)) + geom_point() + geom_smooth() + geom_hline(yintercept = 0, color = &quot;red&quot;) # Residuals vs. predictors ggplot(mod1_output, aes(x = height, y = .resid)) + geom_point() + geom_smooth() + geom_hline(yintercept = 0, color = &quot;red&quot;) Exercise 1 First decide on what you think would be a good model by picking variables based on context. Fit this model, calling it mod_initial. (Remember that you can include several predictors with a + in the model formula - like y ~ x1+x2.) # Code to fit initial model Use residual plot explorations to check if you need to update your model. # Residual plot explorations Fit your updated model, and call it model_updated. # Code to fit updated model Exercise 2 Compute and contextually interpret relevant evaluation metrics for your model. # Code to compute evaluation metrics Exercise 3 Now that you’ve selected your best model, deploy it in the real world by applying it to a new set of 172 adult males. You’ll need to update the newdata argument of the broom::augment() code to use bodyfat_test instead of bodyfat. bodyfat_test &lt;- read_csv(&quot;https://www.dropbox.com/s/7gizws208u0oywq/bodyfat_test.csv?dl=1&quot;) Compare your evaluation metrics from Exercise 2 the metrics here. What do you notice? (Note: this observation is just based on your one particular fitted model. You’ll make some more comprehensive observations in the next exercise.) In general, do you think that models with more or less variables would perform better in this comparison? Explain. Exercise 4 The code below systematically looks at the same comparison that you made in Exercise 3 but for every possible linear regression model formed from inclusion/exclusion of the predictors (without transformations or interactions). Run the code to make a plot of the results of this systematic investigation. (Feel free to inspect the code if you’re curious, but otherwise, don’t worry about understanding it fully.) What do you notice? What do you wonder? get_maes &lt;- function(mod, train_data, test_data) { mod_output_train &lt;- broom::augment(mod, newdata = train_data) mod_output_test &lt;- broom::augment(mod, newdata = test_data) train_mae &lt;- mean(abs(mod_output_train$.resid)) test_mae &lt;- mean(abs(mod_output_test$.resid)) c(train_mae, test_mae) } possible_predictors &lt;- setdiff(colnames(bodyfat), c(&quot;fatSiri&quot;, &quot;hipin&quot;)) results &lt;- bind_rows(lapply(1:13, function(i) { combos &lt;- combn(possible_predictors, i) bind_rows(lapply(seq_len(ncol(combos)), function(j) { formula &lt;- paste(&quot;fatSiri ~&quot;, paste(combos[,j], collapse = &quot;+&quot;)) mod &lt;- lm(as.formula(formula), data = bodyfat) maes &lt;- get_maes(mod = mod, train_data = bodyfat, test_data = bodyfat_test) tibble( form = formula, train_mae = maes[1], test_mae = maes[2], num_predictors = i ) })) })) # Relabel the num_predictors variable results &lt;- results %&gt;% mutate(num_predictors = paste(&quot;# predictors:&quot;, num_predictors)) %&gt;% mutate(num_predictors = factor(num_predictors, levels = paste(&quot;# predictors:&quot;, 1:13))) # Plot results ggplot(results, aes(x = train_mae, y = test_mae, color = num_predictors)) + geom_point() + coord_cartesian(xlim = c(0,7.5), ylim = c(0,7.5)) + geom_abline(slope = 1, intercept = 0) + facet_wrap(~num_predictors) + guides(color = FALSE) + labs(x = &quot;MAE on original data&quot;, y = &quot;MAE on new data on 172 men&quot;) + theme_classic() "],["overfitting-cross-validation.html", "Topic 3 Overfitting &amp; Cross-validation Learning Goals The caret package Exercises", " Topic 3 Overfitting &amp; Cross-validation Learning Goals Explain why training/in-sample model evaluation metrics can provide a misleading view of true test/out-of-sample performance Accurately describe all steps of cross-validation to estimate the test/out-of-sample version of a model evaluation metric Explain what role CV has in a predictive modeling analysis and its connection to overfitting Explain the pros/cons of higher vs. lower k in k-fold CV in terms of sample size and computing time Implement cross-validation in R using the caret package Slides from today are available here. The caret package (If you have not already installed the caret package, install it with install.packages(\"caret\").) Over this course, we will looking at a broad but linked set of specialized tools applied in statistical machine learning. Specialized tools generally require specialized code. The caret (Classification And REgression Training) package helps us streamline much of this specialized code. We’ll learn some tweaks along the way, but the majority of our caret code will use the train() function, which allows us to build and evaluate various models. It looks like a lot, but remember that we’ll be using this over and over: # Load the package library(caret) # Set the seed for the random number generator set.seed(___) # Run the algorithm my_model &lt;- train( y ~ x, data = ___, method = ___, tuneGrid = data.frame(___), trControl = trainControl(method = ___, number = ___, selectionFunction = ___), metric = ___, na.action = na.omit ) Argument Meaning y ~ x Specifies the outcome and predictor variables (just like lm()). data Sample data method Modeling method to use (e.g., \"lm\", \"knn\") tuneGrid Modeling method’s tuning parameters (parameters which define how it works) trControl Method by which to train and test the model (typically cross-validation). When we build multiple models, selectionFunction describes the process by which to select a final model. metric When we build multiple models, this is the metric by which we’ll evaluate and compare them (e.g., RMSE, MAE). na.action What to do about missing data values. We typically set na.action = na.omit to prevent errors if the data has missing values. The power of caret is that it allows us to streamline the vast world of machine learning techniques into one common syntax. On top of \"lm\", check out the different machine learning methods that we can use in caret. We’ll see several of these throughout the course: names(getModelInfo()) You can find more detailed descriptions of these methods here and in a searchable table here. In the exercises below, you’ll need to adapt the following code to perform the CV procedure on a linear regression model (\"lm\"): # Regression model with CV error my_model &lt;- train( y ~ x, data = ___, method = &quot;lm&quot;, trControl = trainControl(method = &quot;cv&quot;, number = your_k), na.action = na.omit ) Notice how this specifies the general caret code: method = \"lm\" indicates that we want to fit a linear regression model trControl = trainControl(method = \"cv\", number = your_k) indicates that we want to train and test the model using cross validation (\"cv\"). You need to specify the number of folds, your_k. tuneGrid: absent Linear regression doesn’t depend upon any tuning parameters (we’re just minimizing the sum of squared residuals). selectionFunction and metric: absent We’re only building one model, not comparing multiple models, so we don’t need a metric by which to compare models or a method by which to select a model. After building the model, you can check out the results: # Summarize the model summary(my_model) # Get model evaluation metrics for each fold my_model$resample # Get CV evaluation metrics my_model$results Exercises You can download a template RMarkdown file to start from here. Context We’ll be working with a dataset containing physical measurements on 80 adult males. These measurements include body fat percentage estimates as well as body circumference measurements. fatBrozek: Percent body fat using Brozek’s equation: 457/Density - 414.2 fatSiri: Percent body fat using Siri’s equation: 495/Density - 450 density: Density determined from underwater weighing (gm/cm^3). age: Age (years) weight: Weight (lbs) height: Height (inches) neck: Neck circumference (cm) chest: Chest circumference (cm) abdomen: Abdomen circumference (cm) hip: Hip circumference (cm) thigh: Thigh circumference (cm) knee: Knee circumference (cm) ankle: Ankle circumference (cm) biceps: Biceps (extended) circumference (cm) forearm: Forearm circumference (cm) wrist: Wrist circumference (cm) It takes a lot of effort to estimate body fat percentage accurately through underwater weighing. The goal is to build the best predictive model for fatSiri using just circumference measurements, which are more easily attainable. (We won’t use fatBrozek or density as predictors because they’re other outcome variables.) library(readr) library(ggplot2) library(dplyr) library(broom) library(caret) bodyfat_train &lt;- read_csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) # Remove the fatBrozek and density variables bodyfat_train &lt;- bodyfat_train %&gt;% select(-fatBrozek, -density) Exercise 1: 4 models Consider the 4 models below: mod1 &lt;- lm(fatSiri ~ age+weight+neck+abdomen+thigh+forearm, data = bodyfat_train) mod2 &lt;- lm(fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps, data = bodyfat_train) mod3 &lt;- lm(fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps+chest+hip, data = bodyfat_train) mod4 &lt;- lm(fatSiri ~ ., data = bodyfat_train) # The . means all predictors Which model will have the lowest training RMSE, and why? Compute the RMSE for models 1 and 4 to (partially) check your answer for part a. Which model do you think will perform worst on new test data? Why? Exercise 2: Cross-validation with caret Complete the code below to perform 10-fold cross-validation for mod1 to estimate the test RMSE (\\(\\text{CV}_{(10)}\\)). Do we need to use set.seed()? Why or why not? (Is there a number of folds for which we would not need to set the seed?) # Do we need to use set.seed()? mod1_cv &lt;- train( ) STAT 155 review: Look at the summary() of mod1_cv. Contextually interpret the coefficient for the weight predictor. Is anything surprising? Why might this be? Look at mod1_cv$resample, and use this to calculate the 10-fold cross-validated RMSE by hand (the idea is the same as when using MSE). (Note: We haven’t done this together, but how can you adapt code that we’ve used before?) Check your answer to part c by directly printing out the CV metrics: mod1_cv$results. Interpret this metric. Exercise 3: Looking at the evaluation metrics Look at the completed table below of evaluation metrics for the 4 models. Which model performed the best on the training data? Which model performed best on the test set? Explain why there’s a discrepancy between these 2 answers and why CV, in general, can help prevent overfitting. Model Training RMSE \\(\\text{CV}_{(10)}\\) mod1 3.810712 4.389568 mod2 3.766645 4.438637 mod3 3.752362 4.517281 mod4 3.572299 4.543343 Exercise 4: Practical issues: choosing \\(k\\) In terms of sample size, what are the pros/cons of low vs. high \\(k\\)? In terms of computational time, what are the pros/cons of low vs. high \\(k\\)? If possible, it is advisable to choose \\(k\\) to be a divisor of the sample size. Why do you think that is? Digging deeper If you have time, consider these exercises to further explore concepts related to today’s ideas. caret’s trainControl() function also has a \"repeatedcv\" method. Just from the name, how do you think this method differs from \"cv\"? What are the pros/cons of \"repeatedcv\" as compared to \"cv\"? Adapt the train() code to perform leave-one-out-cross-validation (LOOCV). Hint: nrow(dataset) obtains the number of cases in the dataset. Do we need set.seed()? Why or why not? Using the information from your_output$resample (which is a dataset), construct a visualization to examine the variability of RMSE from case to case. What might explain any very large values? What does this highlight about the quality of estimation of the LOOCV process? "],["variable-subset-selection.html", "Topic 4 Variable Subset Selection Learning Goals Exercises", " Topic 4 Variable Subset Selection Learning Goals Clearly describe the forward and backward stepwise selection algorithm and why they are examples of greedy algorithms Compare best subset and stepwise algorithms in terms of optimality of output and computational time Describe how selection algorithms can give a measure of variable importance Exercises You can download a template RMarkdown file to start from here. We’ll continue using the body fat dataset to explore subset selection methods. library(caret) library(ggplot2) library(dplyr) library(readr) bodyfat &lt;- read_csv(&quot;http://www.macalester.edu/~ajohns24/data/bodyfatsub.csv&quot;) # Take out the redundant Density and HeightFt variables bodyfat &lt;- bodyfat %&gt;% select(-Density, -HeightFt) Exercise 1: Backward stepwise selection: by hand In the backward stepwise procedure, we start with the full model, full_model, with all predictors: full_model &lt;- lm(BodyFat ~ Age + Weight + Height + Neck + Chest + Abdomen + Hip + Thigh + Knee + Ankle + Biceps + Forearm + Wrist, data = bodyfat) To practice the backward selection algorithm, step through a few steps of the algorithm using p-values as a selection criterion: Identify which predictor contributes the least to the model. One (problematic) approach is to identify the least significant predictor. Fit a new model which eliminates this predictor. Identify the least significant predictor in this model. Fit a new model which eliminates this predictor. Repeat 1 more time to get the hang of it. (We discussed in the video how the use of p-values for selection is problematic, but for now you’re just getting a handle on the algorithm. You’ll think about the problems with p-values in the next exercise.) Exercise 2: Interpreting the results Examine which predictors remain after the previous exercise. Are you surprised that, for example, Wrist is still in the model but Weight is not? Does this mean that Wrist is a better predictor of body fat percentage than Weight is? What statistical idea is relevant here? Exercise 3: Planning forward selection using CV Using p-values to perform stepwise selection has problems, as was discussed in the video. A better alternative to target predictive accuracy is to evaluate the models using cross-validation. Fully outline the steps required to use cross-validation to perform forward selection. Make sure to provide enough detail such that the stepwise selection and CV algorithms are made clear. Exercise 4: Stepwise selection in caret Run install.packages(\"leaps\") in the Console to install the leaps package before proceeding. Complete the caret code below to perform backward stepwise selection with cross-validation. The following points will help you complete the code: In R model formulas, y ~ . sets y as the outcome and all other predictors in the dataset as predictors. The specific method name for backward selection is \"leapBackward\". The tuneGrid argument is already filled in. It allows us to input tuning parameters into the fitting process. The tuning parameters for subset selection are the number of variables included in the models explored (nvmax). This can vary from 1 to 13 (the maximum number of predictors possible). Use 10-fold CV to estimate test performance of the models. Use \"MAE\" as the evaluation metric to choose how the best of the 1-variable, 2-variable, etc. models will be chosen. (Note: CV is only used to pick among the best 1, 2, 3, …, and 13 variable models. To find the best 1, 2, 3, …, and 13 variable models, training MSE is used. caret uses training MSE because within a subset size, all models have the same number of coefficients, which makes both ordinary R-squared and training MSE ok for comparing models.) set.seed(23) back_step_mod &lt;- train( y ~ x, data = bodyfat, method = ___, tuneGrid = data.frame(nvmax = 1:13), trControl = ___, metric = ___, na.action = na.omit ) Exercise 5: Exploring the results There are a number of ways to examine and use the output of the selection algorithm, which we’ll explore here. (It would be useful to make notes along the way - perhaps on your code note sheet.) Part a Let’s first examine the sequence of models explored. The stars in the table at the bottom indicate the variables included in the 1-variable, 2-variable, etc. models. summary(back_step_mod) Of the 13 models in the sequence, R only prints out the 11 smallest models (since, for reasons we’ll discuss below, it determines the 11 predictor model to be “best”). Which predictor is the last to remain in the model? Second-to-last to remain? How do you think we could use these results to identify which predictors were most/least important in predicting the outcome of body fat percentage? Part b Examine the 10-fold CV MAE for each of the 13 models in the backward stepwise sequence: # Plot metrics for each model in the sequence plot(back_step_mod) # Look at accuracy/error metrics for the different subset sizes back_step_mod$results Which size model has the lowest CV MAE? Which size model would you pick? Why? Part c In our model code, we used selectionFunction = \"best\" by default inside trainControl(). By doing so, we indicated that we wanted to find which model minimizes the CV MAE (i.e., has the “best” MAE). With respect to this criterion: # What tuning parameter gave the best performance? # i.e. What subset size gave the best model? back_step_mod$bestTune # Obtain the coefficients for the best model coef(back_step_mod$finalModel, id = back_step_mod$bestTune$nvmax) # Obtain the coefficients of any size model with at most as many variables as the overall best model (e.g., the 2-predictor model) coef(back_step_mod$finalModel, id = 2) Another sensible choice for the selection function is to not choose the model with the lowest estimated error but to account for the uncertainty in the estimation of that test error by picking the smallest model for which the CV MAE is within one standard error of the minimum CV MAE. What do you think the rationale for this is? (We’ll explore the code for this formally later, or you can try it out below in the Digging Deeper section.) Part d We should end by evaluating our final chosen model. Contextually interpret (with units) the CV MAE for the model. Make residual plots for the chosen model in one of 2 ways: (1) use lm() to fit the model with the chosen predictors or (2) use the following code to create a dataset called back_step_mod_out which contains the original data as well as predicted values and residuals (fitted and resid). back_step_mod_out &lt;- bodyfat %&gt;% mutate( fitted = predict(back_step_mod, newdata = bodyfat), resid = BodyFat - fitted ) Digging deeper As mentioned in Exercise 5c, we have another choice for the selectionFunction used to choose from many possible models. The use of selectionFunction = \"oneSE\" below picks the simplest model for which the CV MAE is within one standard error of the minimum CV MAE. Compare the output you obtain here with your best model from Exercise 5. back_step_mod_1se &lt;- train( BodyFat ~ ., data = bodyfat, method = &quot;leapBackward&quot;, tuneGrid = data.frame(nvmax = 1:13), trControl = trainControl(method = &quot;cv&quot;, number = 10, selectionFunction = &quot;oneSE&quot;), metric = &quot;MAE&quot;, na.action = na.omit ) Forward selection can be implemented in caret with analogous code, except that the method name is \"leapForward\". Try implementing forward selection and comparing your results. "],["lasso-shrinkageregularization.html", "Topic 5 LASSO: Shrinkage/Regularization Learning Goals LASSO models in caret Exercises", " Topic 5 LASSO: Shrinkage/Regularization Learning Goals Explain how ordinary and penalized least squares are similar and different with regard to (1) the form of the objective function and (2) the goal of variable selection Explain why variable scaling is important for the performance of shrinkage methods Explain how the lambda tuning parameter affects model performance and how this is related to overfitting Describe how output from LASSO models can give a measure of variable importance Slides from today are available here. LASSO models in caret To build LASSO models in caret, first load the package and set the seed for the random number generator to ensure reproducible results: library(caret) set.seed(___) # Pick your favorite number to fill in the parentheses Then adapt the following code: # Perform LASSO lasso_mod &lt;- train( y ~ ., data = ___, method = &quot;glmnet&quot;, tuneGrid = data.frame(alpha = 1, lambda = seq(___, ___, length.out = ___)), trControl = trainControl(method = &quot;cv&quot;, number = ___, selectionFunction = ___), metric = &quot;MAE&quot;, na.action = na.omit ) Argument Meaning y ~ . Shorthand for the model of y by all possible predictors x in the data (every other variable in the dataset except y) data Sample data which only includes y and potential predictors x. (Make sure to remove unneeded variables with select(-variable) first.) method \"glmnet\" implements the LASSO and other regularization methods tuneGrid A mini-dataset (data.frame) of tuning parameters. alpha = 1 specifies that we’re interested in the LASSO (not a different regularization method, like ridge regression). lambda = seq(___, ___, length.out = ___) specifies that LASSO should be run for each tuning parameter value in the specified sequence. trControl Use cross-validation to estimate test performance for each LASSO model fit (corresponding to each tuning parameter in tuneGrid). The process used to pick a final model from among these is indicated by the selectionFunction argument, options including \"best\" and \"oneSE\". The best option will pick the model with the best metric. The oneSE option will pick the simplest model for which the metric is within one standard error of the best metric. metric Evaluate and compare competing LASSO models with respect to their CV-MAE. na.action Set na.action = na.omit to prevent errors if the data has missing values. Note: caret automatically implements variable standardization (scaling variables to have mean 0 and standard deviation 1) when using the \"glmnet\" method. Examining the LASSO model for each \\(\\lambda\\) # Plot coefficient paths as a function of lambda plot(lasso_mod$finalModel, xvar = &quot;lambda&quot;, label = TRUE, col = rainbow(20)) # Codebook for which variables the numbers correspond to rownames(lasso_mod$finalModel$beta) Identifying the “best” LASSO model The “best” model in the sequence of models fit is defined relative to the chosen selectionFunction and metric. # Plot CV-estimated test performance versus the tuning parameter (lambda) plot(lasso_mod) # CV metrics for each LASSO model lasso_mod$results # Identify which tuning parameter (lambda) is &quot;best&quot; lasso_mod$bestTune # Obtain the predictors and coefficients of the &quot;best&quot; model # The .&#39;s indicate that the coefficient is 0 coef(lasso_mod$finalModel, lasso_mod$bestTune$lambda) # Obtain the predictors and coefficients of LASSO model w/ different lambda # e.g., lambda = 1 (roughly) coef(lasso_mod$finalModel, 1) # Get information from all CV iterations for the &quot;best&quot; model lasso_mod$resample Exercises You can download a template RMarkdown file to start from here. We’ll continue using the body fat dataset to explore LASSO modeling. library(caret) library(ggplot2) library(dplyr) library(readr) bodyfat &lt;- read_csv(&quot;http://www.macalester.edu/~ajohns24/data/bodyfatsub.csv&quot;) # Take out the redundant Density and HeightFt variables bodyfat &lt;- bodyfat %&gt;% select(-Density, -HeightFt) Exercise 1: A least squares model Let’s start by building an ordinary (not penalized) least squares model to review important concepts. We’ll fit a model with all possible predictors. ls_mod &lt;- lm(BodyFat ~ ., data = bodyfat) Use caret to perform 10-fold cross-validation to estimate test MAE for this model. How do you think the estimated test error would change with fewer predictors? This model fit with ordinary least squares corresponds to a special case of penalized least squares. What is the value of \\(\\lambda\\) in this special case? As \\(\\lambda\\) increases, what would you expect to happen to the number of predictors that remain in the model? Exercise 2: Fitting a LASSO model in caret Adapt our general LASSO code to fit a set of LASSO models with the following parameters: Use 10-fold CV. Use mean absolute error (MAE) to select a final model. Select the simplest model for which the metric is within one standard error of the best metric. Use a sequence of 100 \\(\\lambda\\) values from 0 to 10. Before running the code, enter install.packages(\"glmnet\") in the Console. We’ll explore output from lasso_mod in the next exercises. # Fit LASSO models for a grid of lambda values set.seed(74) lasso_mod &lt;- train( ) Exercise 3: Examining output: plot of coefficient paths A useful first plot allows us to examine coefficient paths resulting from the fitted LASSO models: coefficient estimates as a function of \\(\\lambda\\). # Plot coefficient paths as a function of lambda plot(lasso_mod$finalModel, xvar = &quot;lambda&quot;, label = TRUE, col = rainbow(20)) # Codebook for which variables the numbers correspond to rownames(lasso_mod$finalModel$beta) # e.g., What are variables 2 and 4? rownames(lasso_mod$finalModel$beta)[c(2,4)] There’s a lot of information in this plot! Each colored line corresponds to a different predictor. The small number to the left of each line indicates a predictor by its position in rownames(lasso_mod$finalModel$beta). The x-axis reflects the range of different \\(\\lambda\\) values (on the log-scale) considered in lasso_mod (in tuneGrid). At each \\(\\lambda\\), the y-axis reflects the coefficient estimates for the predictors in the corresponding LASSO model. At each \\(\\lambda\\), the numbers at the top of the plot indicate how many predictors remain in the corresponding model. Very roughly eyeball the coefficient estimates at the smallest value of \\(\\lambda\\). Do they look like they correspond to the coefficient estimates from ordinary least squares in exercise 2? Why do all of the lines head toward y = 0 on the far right of the plot? We can zoom in on the plot by setting the y-axis limits to go from -0.5 to 1 with ylim as below. Compare the lines for variables 6 and 12. What are variables 6 and 12? Which seems to be a more “important” or “persistent” (persistently present in the model) variable? Does this make sense in context? # Zoom in plot(lasso_mod$finalModel, xvar = &quot;lambda&quot;, label = TRUE, col = rainbow(20), ylim = c(-0.5,1)) Which predictor seems least “persistent”? In general, how might we use these coefficient paths to measure the predictive importance of our predictors? Note: If you’re curious about code to automate this visual inspection of variable importance, look at Digging Deeper Exercise 2. Exercise 4: Tuning \\(\\lambda\\) In order to pick which \\(\\lambda\\) (hence LASSO model) is “best”, we can plot CV error estimates for the different models: # Plot a summary of the performance of the different models plot(lasso_mod) Inspect the shape of the plot. The errors go down at the very beginning then start going back up. Based on this, what are the consequences of picking a \\(\\lambda\\) that is too small or too large? (This is an example of a very important idea that we’ll see shortly: the bias-variance tradeoff.) Based on visual inspection, roughly what value of \\(\\lambda\\) results in the best model? (Remind yourself of the interpretation of “best” as defined by our selectionFunction of \"oneSE\".) Identify the “best” \\(\\lambda\\). # Identify which tuning parameter (lambda) is &quot;best&quot; lasso_mod$bestTune Note: If you’re curious about making plots that show both test error estimates and their uncertainty, look at Digging Deeper Exercise 1. Exercise 5: Examining and evaluating the best LASSO model Take a look at the predictors and coefficients for the “best” LASSO model. Are the predictors that remain in the model sensible? Do the coefficient signs make sense? # Obtain the predictors and coefficients of the &quot;best&quot; model # The .&#39;s indicate that the coefficient is 0 coef(lasso_mod$finalModel, lasso_mod$bestTune$lambda) # Obtain the predictors and coefficients of LASSO model w/ different lambda # In case it&#39;s of interest to look at a slightly different model # e.g., lambda = 1 (roughly) coef(lasso_mod$finalModel, 1) Evaluate the best LASSO model: Contextually interpret (with units) the CV error for the model by inspecting lasso_mod$resample or lasso_mod$results. Make residual plots for the model by creating a dataset called lasso_mod_out which contains the original data as well as predicted values and residuals (fitted and resid). lasso_mod_out &lt;- bodyfat %&gt;% mutate( fitted = predict(lasso_mod, newdata = bodyfat), resid = BodyFat - fitted ) Digging deeper These exercises are recommended for further exploring code useful in an applied analysis. plot(lasso_mod) only shows estimates of test error but doesn’t show the uncertainty in those estimates. Use lasso_mod$results to plot both MAE and its standard deviation (MAESD). Using this ggplot2 cheat sheet might help. In Exercise 3, we used the plot of coefficient paths to evaluate the variable importance of our predictors. The code below does this systematically for each predictor so that we don’t have to eyeball. Step through and work out what each part is doing. It may help to look up function documentation with ?function_name in the Console. # Create a boolean matrix (predictors x lambdas) of variable exclusion bool_predictor_exclude &lt;- lasso_mod$finalModel$beta==0 # Loop over each variable var_imp &lt;- sapply(seq_len(nrow(bool_predictor_exclude)), function(row) { # Extract coefficient path (sorted from highest to lowest lambda) this_coeff_path &lt;- bool_predictor_exclude[row,] # Compute and return the # of lambdas until this variable is out forever ncol(bool_predictor_exclude)-which.min(this_coeff_path)+1 }) # Create a dataset of this information and sort var_imp_data &lt;- tibble( var_name = rownames(bool_predictor_exclude), var_imp = var_imp ) var_imp_data %&gt;% arrange(desc(var_imp)) "],["knn-regression-and-the-bias-variance-tradeoff.html", "Topic 6 KNN Regression and the Bias-Variance Tradeoff Learning Goals KNN models in caret Exercises", " Topic 6 KNN Regression and the Bias-Variance Tradeoff Learning Goals Clearly describe / implement by hand the KNN algorithm for making a regression prediction Explain how the number of neighbors relates to the bias-variance tradeoff Explain the difference between parametric and nonparametric methods Explain how the curse of dimensionality relates to the performance of KNN Slides from today are available here. KNN models in caret To build KNN models in caret, first load the package and set the seed for the random number generator to ensure reproducible results: library(caret) set.seed(___) # Pick your favorite number to fill in the parentheses Then adapt the following code: knn_mod &lt;- train( y ~ ., data = ___, preProcess = &quot;scale&quot;, method = &quot;knn&quot;, tuneGrid = data.frame(k = seq(___, ___, by = ___)), trControl = trainControl(method = &quot;cv&quot;, number = ___, selectionFunction = ___), metric = &quot;MAE&quot;, na.action = na.omit ) Argument Meaning y ~ . Model formula for specifying response and predictors data Sample data preProcess \"scale\" indicates that predictor variables should be scaled to have the same variance (Why might this be important? Should we always do this?) method \"knn\" implements KNN regression (and classification) tuneGrid A mini-dataset (data.frame) of tuning parameters. \\(k\\) is the KNN neighborhood size. Supply a sequence as seq(begin, end, by = size of step). trControl Use cross-validation to estimate test performance for each model fit. The process used to pick a final model from among these is indicated by selectionFunction, with options including \"best\" and \"oneSE\". metric Evaluate and compare competing models with respect to their CV-MAE. na.action Set na.action = na.omit to prevent errors if the data has missing values. Note: When including categorical predictors, caret automatically creates the corresponding indicator variables to allow Euclidean distance to still be used. You’ll think about the pros/cons of this in the exercises. An alternative to creating indicator variables is to use Gower distance. We’ll explore Gower distance more when we talk about clustering. Identifying the “best” KNN model The “best” model in the sequence of models fit is defined relative to the chosen selectionFunction and metric. # Plot CV-estimated test performance versus the tuning parameter plot(knn_mod) # CV metrics for each model knn_mod$results # Identify which tuning parameter is &quot;best&quot; knn_mod$bestTune # Get information from all CV iterations for the &quot;best&quot; model knn_mod$resample # Use the best model to make predictions # newdata should be a data.frame with required predictors predict(knn_mod, newdata = ___) Exercises You can download a template RMarkdown file to start from here. We’ll explore KNN regression using the College dataset in the ISLR package (install it with install.packages(\"caret\") in the Console). You can use ?College in the Console to look at the data codebook. library(caret) library(ggplot2) library(dplyr) library(readr) library(ISLR) data(College) # A little data cleaning college_clean &lt;- College %&gt;% mutate(school = rownames(College)) %&gt;% filter(Grad.Rate &lt;= 100) # Remove one school with grad rate of 118% rownames(college_clean) &lt;- NULL # Remove school names as row names Hello, how are things? We’re about a week and a half into our last module of the year - how are you feeling? What’s on your mind? Exercise 1: Bias-variance tradeoff warmup Think back to the LASSO algorithm which depends upon tuning parameter \\(\\lambda\\). For which values of \\(\\lambda\\) (small or large) will LASSO be the most biased, and why? For which values of \\(\\lambda\\) (small or large) will LASSO be the most variable, and why? The bias-variance tradeoff also comes into play when comparing across algorithms, not just within algorithms. Consider LASSO vs. least squares: Which will tend to be more biased? Which will tend to be more variable? When will LASSO outperform least squares in the bias-variance tradeoff? Exercise 2: Impact of distance metric Consider the 1-nearest neighbor algorithm to predict Grad.Rate on the basis of two predictors: Apps and Private. Let Yes for Private be represented with the value 1 and No with 0. We have a test case whose number of applications is 13,530 and is a private school. Suppose that we have the tiny 2-case training set below. What would the 1-nearest neighbor prediction be using Euclidean distance? college_clean %&gt;% filter(school %in% c(&quot;Princeton University&quot;, &quot;SUNY at Albany&quot;)) %&gt;% select(Apps, Private, Grad.Rate, school) Do you have any concerns about the resulting prediction? Based on this, comment on the impact of the distance metric chosen on KNN performance. How might you change the distance calculation (or correspondingly rescale the data) to generate a more sensible prediction in this situation? Exercise 3: Implementing KNN in caret Adapt our general KNN code to “fit” a set of KNN models with the following specifications: Use the predictors Private, Top10perc (% of new students from top 10% of high school class), and S.F.Ratio (student/faculty ratio). Use 8-fold CV. (Why 8? Take a look at the sample size.) Use mean absolute error (MAE) to select a final model. Select the simplest model for which the metric is within one standard error of the best metric. Use a sequence of \\(K\\) values from 1 to 100 in increments of 5. Should you use preProcess = \"scale\"? After adapting the code (but before inspecting any output), answer the following conceptual questions: Explain your choice for using or not using preProcess = \"scale\". Why is “fit” in quotes? Does KNN actually fit a model as part of training? (This feature of KNN is known as “lazy learning”.) How is test MAE estimated? What are the steps of the KNN algorithm with cross-validation? Draw a picture of how you expect test MAE to vary with \\(K\\). In terms of the bias-variance tradeoff, why do you expect the plot to look this way? set.seed(2021) knn_mod &lt;- train( ) Exercise 4: Inspecting the results Use plot(knn_mod) to verify your expectations about the plot of test MAE vs. \\(K\\). Contextually interpret the test MAE. How else could you evaluate the KNN model? Does your KNN model help you understand which predictors of graduation rate are most important? Why or why not? Exercise 5: Curse of dimensionality Just as with parametric models, we could keep going and add more and more predictors. However, the KNN algorithm is known to suffer from the “curse of dimensionality”. Why? Hint: First do a quick Google search of this new idea. "],["splines.html", "Topic 7 Splines Learning Goals Splines in caret Exercises", " Topic 7 Splines Learning Goals Explain the advantages of splines over global transformations and other types of piecewise polynomials Explain how splines are constructed by drawing connections to variable transformations and least squares Explain how the number of knots relates to the bias-variance tradeoff Slides from today are available here. Splines in caret To build models with splines in caret, we proceed with the same structure for train() as we use for ordinary linear regression models. (Why can we just use least squares?) Refer to code from Topic 3: Overfitting &amp; CV for a refresher. To work with splines, we’ll use the splines package. The ns() function in that package creates the transformations needed to create a spline function for a quantitative predictor. This involves a small update to the formula: # Before: all linear terms ls_mod &lt;- train( y ~ quant_x1 + quant_x2, ... ) # With splines ls_mod &lt;- train( y ~ ns(quant_x1, df = 3) + ns(quant_x2, df = 3), ... ) The df argument in ns() stands for degrees of freedom: df = # knots + 1 The degrees of freedom are the number of coefficients in the transformation functions that are free to vary (essentially the number of underlying parameters behind the transformations). Exercises You can download a template RMarkdown file to start from here. Before proceeding, install the splines package by entering install.packages(\"splines\") in the Console. We’ll continue using the College dataset in the ISLR package to explore splines. You can use ?College in the Console to look at the data codebook. library(caret) library(ggplot2) library(dplyr) library(readr) library(ISLR) library(splines) data(College) # A little data cleaning college_clean &lt;- College %&gt;% mutate(school = rownames(College)) %&gt;% filter(Grad.Rate &lt;= 100) # Remove one school with grad rate of 118% rownames(college_clean) &lt;- NULL # Remove school names as row names Exercise 1: Evaluating a fully linear model We will model Grad.Rate as a function of 4 predictors: Private, Terminal, Expend, and S.F.Ratio. Make scatterplots with 2 different smoothing lines to explore potential nonlinearity. Adding the following to the normal scatterplot code will create a smooth (curved) blue trend line and a red linear trend line. geom_smooth(color = &quot;blue&quot;, se = FALSE) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, se = FALSE) Use caret to fit an ordinary linear regression model (no splines yet) with the following specifications: Use 8-fold CV. Use mean absolute error (MAE) to select a final model. Select the simplest model for which the metric is within one standard error of the best metric. set.seed(___) ls_mod &lt;- train( ) Make plots of the residuals vs. the 3 quantitative predictors to evaluate the appropriateness of linear terms. ls_mod_data &lt;- college_clean %&gt;% mutate( pred = predict(ls_mod, newdata = college_clean), resid = ___ ) ggplot(ls_mod_data, ???) + ___ + ___ + geom_hline(yintercept = 0, color = &quot;red&quot;) Exercise 2: Evaluating a spline model We’ll extend our linear regression model with spline functions of the quantitative predictors (leave Private as is). What tuning parameter is associated with splines? How do high/low values of this parameter relate to bias and variance? Update your code from Exercise 1 to model the 3 quantitative predictors with natural splines that have 2 knots (= 3 degrees of freedom). set.seed(___) spline_mod &lt;- train( ) Make plots of the residuals vs. the 3 quantitative predictors to evaluate if splines improved the model. spline_mod_data &lt;- ___ # Residual plots Extra! Variable scaling What is your intuition about whether variable scaling matters for the performance of splines? Check you intuition by reusing code from Exercise 2, except with preProcess = \"scale\" inside train(). Call this spline_mod2. How do the predictions from spline_mod and spline_mod2 compare? You could use a plot to compare or check out the all.equal() function. "],["homework-1.html", "Homework 1 Project Work Portfolio Work Course Engagement", " Homework 1 Due Friday, March 26 at midnight CST on Moodle Please turn in a single PDF document containing (1) your responses for the Project Work and Course Engagement sections and (2) a LINK to the Google Doc with your responses for the Portfolio Work section. Project Work Goal: Find a dataset (or datasets) to use for your final project, and start to get to know the data. Details: Your dataset(s) should allow you to perform a (1) regression, (2) classification, and (3) unsupervised learning analysis. The following resources are good places to start looking for data: Google Dataset Search Kaggle UCI Machine Learning Repository Harvard Dataverse Even if you end up working with a partner on the project (which isn’t required - working alone is fine), please complete this initial work individually. It’s fine if you and a potential/future partner end up using the same dataset and collaborate on the finding of data, but complete the short bit of writing (below) individually. Check in with the instructor early if you need help. Deliverables: Write 1-2 paragraphs (no more than 350 words) summarizing: The information in the dataset(s) and the context behind the data. Use the prompts below to guide your thoughts. (Note: in some situations, there may be incomplete information on the data context. That’s fine. Just do your best to summarize what information is available, and acknowledge the lack of information where relevant.) What are the cases? Broadly describe the variables contained in the data. Who collected the data? When, why, and how? 3 research questions 1 that can be investigated in a regression setting 1 that can be investigated in a classification setting 1 that can be investigated in an unsupervised learning setting Also make sure that you can read the data into R. You don’t need to do any analysis in R yet, but making sure that you can read the data will make the next steps go more smoothly. Portfolio Work Page maximum: 2 pages of text (pictures don’t count) Organization: Your choice! Use titles and section headings that make sense to you. (It probably makes sense to have a separate section for each method.) Deliverables: Put your responses for this part in a Google Doc, and update the link sharing so that anyone with the link at Macalester College can edit. Include the URL for the Google Doc in your submission. Note: Some prompts below may seem very open-ended. This is intentional. Crafting good responses requires looking back through our material to organize the concepts in a coherent, thematic way, which is extremely useful for your learning. Concepts to address: Overfitting: The video used the analogy of a cat picture model to explain overfitting. Come up with your own analogy to explain overfitting. Evaluating regression models: Describe how residuals are central to the evaluation of regression models. Explain how they arise in quantitative evaluation metrics and how they are used in evaluation plots. Include examples of plots that show desirable and undesirable model behavior (feel free to draw them by hand if you wish) and what steps can be taken to address that undesirable behavior. Cross-validation: In your own words, explain the rationale for cross-validation in relation to overfitting and model evaluation. Describe the algorithm in your own words in at most 2 sentences. Subset selection: Algorithmic understanding: Look at Conceptual exercise 1, parts (a) and (b) in ISLR Section 6.8. What are the aspects of the subset selection algorithm(s) that are essential to answering these questions, and why? (Note: you’ll have to try to answer the ISLR questions to respond to this prompt, but the focus of your writing should be on the question in bold here.) Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? Computational time: What computational time considerations are relevant for this method (how long the algorithms take to run)? Interpretation of output: What parts of the algorithm output have useful interpretations, and what are those interpretations? Focus on output that allows us to measure variable importance. How do the algorithms/output allow us to learn about variable importance? LASSO: Algorithmic understanding: Come up with your own analogy for explaining how the penalized least squares criterion works. Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? Computational time: What computational time considerations are relevant for this method (how long the algorithms take to run)? Interpretation of output: What parts of the algorithm output have useful interpretations, and what are those interpretations? Focus on output that allows us to measure variable importance. How do the algorithms/output allow us to learn about variable importance? Course Engagement The Ethics component below is the only required piece. The others are optional depending on the modes of engagement you wish to pursue consistently throughout the module. Ethics: (REQUIRED) Read the article Amazon scraps secret AI recruiting tool that showed bias against women. Write a short (roughly 250 words), thoughtful response about the themes and cautions that the article brings forth. Reflection: Write a short, thoughtful reflection about how things went this week. Feel free to use whichever prompts below resonate most with you, but don’t feel limited to these prompts. How is your understanding of the material? What ideas/topics have stuck out for you? How is group work going? Any strategies for improving collaboration that you want to try out next week? How is your work/life balance going? Any new activities or strategies that you want to try out for next week? Note-taking: Share link(s) to file(s) where you keep your notes on videos/readings and on code from class. Q &amp; A: In one short paragraph, summarize your engagement in at least 2 of the 3 following areas: (1) preceptor / instructor office hours, (2) on Slack, (3) in small groups during synchronous class sessions. "],["homework-2.html", "Homework 2 Project Work Portfolio Work Course Engagement", " Homework 2 Due Friday, April 2 at midnight CST on Moodle Deliverables: Please use this template to knit an HTML document. Convert this HTML document to a PDF by opening the HTML document in your web browser. Print the document (Ctrl/Cmd-P) and change the destination to “Save as PDF”. Submit this one PDF to Moodle. Alternatively, you may knit your Rmd directly to PDF if you have LaTeX installed. Project Work Goal: Begin an analysis of your dataset to answer your regression research question. Collaboration: If you have already formed a team (of at most 3 members) for the project, this part can be done as a team. Only one team member should submit a Project Work section. Grading: Completing this Project Work section is a required part of the final project. Rather than receiving a grade for the analyses below, you will get qualitative feedback from the instructor. It is the synthesis of the analyses across homework assignments that will determine the final project grade. Data cleaning: If your dataset requires any cleaning (e.g., merging datasets, creation of new variables), first consult the R Resources page to see if your questions are answered there. If not, post on the #content-questions channel in our Slack workspace to ask for help. Please ask for help early and regularly to avoid stressful workloads. Required Analyses: Initial investigation: ignoring nonlinearity (for now) Use ordinary least squares (OLS) regression, forward and/or backward selection, and LASSO to build initial models for your quantitative outcome as a function of the predictors of interest. (As part of data cleaning, exclude any variables that you don’t want to consider as predictors.) These models should not include any transformations to deal with nonlinearity. You’ll explore this in the next investigation. Note: If you have highly collinear/redundant variables, you might see the message “Reordering variables and trying again” and associated warning()s about linear dependencies being found. Sometimes stepwise selection is able to handle the collinearity/redundancy by modifying the order of the variables tried. If collinearity/redundancy cannot be handled and causes an error, try reducing nvmax. Estimate test performance of the models from these different methods. Report and interpret (with units) these estimates along with a measure of uncertainty in the estimate (SD is most readily available from caret). Compare estimated test performance across methods. Which method(s) might you prefer? Use residual plots to evaluate whether some quantitative predictors might be better modeled with nonlinear relationships. Compare insights from variable importance analyses from the different methods (stepwise and LASSO, but not OLS). Are there variables for which the methods reach consensus? What insights are expected? Surprising? Note that if some (but not all) of the indicator terms for a categorical predictor are selected in the final models, the whole predictor should be treated as selected. Accounting for nonlinearity Update your stepwise selection model(s) and LASSO model to use natural splines for the quantitative predictors. You’ll need to update the model formula from y ~ . to something like y ~ cat_var1 + ns(quant_var1, df) + .... It’s recommended to use few knots (e.g., 2 knots = 3 degrees of freedom). Note that ns(x,3) replaces x with 3 transformations of x. Keep this in mind when setting nvmax in stepwise selection. Compare insights from variable importance analyses here and the corresponding results from Investigation 1. Now after having accounted for nonlinearity, have the most relevant predictors changed? Note that if some (but not all) of the spline terms are selected in the final models, the whole predictor should be treated as selected. Fit a GAM using LOESS terms using the set of variables deemed to be most relevant based on your investigations so far. How does test performance of the GAM compare to other models you explored? Do you gain any insights from the GAM output plots for each predictor? Don’t worry about KNN for now. Summarize investigations Decide on an overall best model based on your investigations so far. To do this, make clear your analysis goals. Predictive accuracy? Interpretability? A combination of both? Societal impact Are there any harms that may come from your analyses and/or how the data were collected? What cautions do you want to keep in mind when communicating your work? Portfolio Work Length requirements: Detailed for each section below. Organization: To help the instructor and preceptors grade, please organize your document as shown in this example. Clear section headers and new pages for each method help a lot. Thank you! Deliverables: Continue writing your responses in the same Google Doc that you set up for Homework 1. Include the URL for the Google Doc in your submission. Note: Some prompts below may seem very open-ended. This is intentional. Crafting good responses requires looking back through our material to organize the concepts in a coherent, thematic way, which is extremely useful for your learning. Revisions: Make any revisions desired to previous concepts. Important formatting note: Please use a comment to mark the text that you want to be reread. (Highlight each span of text you want to be reread, and mark it with the comment “REVISION”.) Rubrics to past homeworks will be available on Moodle (under the Solutions section). Look at these rubrics to guide your revisions. You can always ask for guidance in office hours as well. New concepts to address: The following prompts are shared for all methods: Bias-variance tradeoff: What tuning parameters control the performance of the method? How do low/high values of the tuning parameters relate to bias and variance of the learned model? (3 sentences max.) Parametric / nonparametric: Where (roughly) does this method fall on the parametric-nonparametric spectrum, and why? (3 sentences max.) Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? (3 sentences max.) Subset selection: Bias-variance tradeoff Parametric / nonparametric LASSO: Bias-variance tradeoff Parametric / nonparametric KNN: Algorithmic understanding: Draw and annotate pictures that show how the KNN (K = 2) regression algorithm would work for a test case in a 2 quantitative predictor setting. Also explain how the curse of dimensionality affects KNN performance. (5 sentences max.) Bias-variance tradeoff Parametric / nonparametric Scaling of variables Computational time: The KNN algorithm is often called a “lazy” learner. Discuss how this relates to the model training process and the computations that must be performed when predicting on a new test case. (3 sentences max.) Interpretation of output: The “lazy” learner feature of KNN in relation to model training affects the interpretability of output. How? (3 sentences max.) Splines: Algorithmic understanding: Explain the advantages of natural cubic splines over global transformations and piecewise polynomials. Also explain the connection between splines and the ordinary (least squares) regression framework. (5 sentences max.) Bias-variance tradeoff Parametric / nonparametric Scaling of variables Computational time: When using splines, how does computation time compare to fitting ordinary (least squares) regression models? (1 sentence) Interpretation of output: SKIP - will be covered in the GAMs section Local regression: Algorithmic understanding: Consider the R functions lm(), predict(), dist(), and dplyr::filter(). (Look up the documentation for unfamiliar functions in the Help pane of RStudio.) In what order would these functions need to be used in order to make a local regression prediction for a supplied test case? Explain. (5 sentences max.) Bias-variance tradeoff Parametric / nonparametric Scaling of variables Computational time: In general, local regression is very fast, but how would you expect computation time to vary with span? Explain. (3 sentences max.) Interpretation of output: SKIP - will be covered in the GAMs section GAMs: Algorithmic understanding: How do linear regression, splines, and local regression each relate to GAMs? Why would we want to model with GAMs? (5 sentences max.) Bias-variance tradeoff Parametric / nonparametric Scaling of variables Computational time: How a GAM is specified affects the time required to fit the model - why? (3 sentences max.) Interpretation of output: How does the interpretation of ordinary regression coefficients compare to the interpretation of GAM output? (3 sentences max.) Course Engagement The Ethics component below is the only required piece. The others are optional depending on the modes of engagement you wish to pursue consistently throughout the module. Ethics: (REQUIRED) Read the article Automated background checks are deciding who’s fit for a home. Write a short (roughly 250 words), thoughtful response about the ideas that the article brings forth. What themes recur from last week’s article (on an old Amazon recruiting tool)? What aspects are more particular to the context of equity in housing access? Reflection: Write a short, thoughtful reflection about how things went this week. Feel free to use whichever prompts below resonate most with you, but don’t feel limited to these prompts. How are class-related things going? Is there anything that you need from the instructor? What new strategies for watching videos, reading, reviewing, gaining insights from class work have you tried or would like to try? How is group work going? Did you try out any new collaboration strategies with your new group? How did they go? How is your work/life balance going? Did you try out any new activities or strategies for staying well? How did they go? Note-taking: Share link(s) to file(s) where you keep your notes on videos/readings and on code from class. Q &amp; A: In one short paragraph, summarize your engagement in at least 2 of the 3 following areas: (1) preceptor / instructor office hours, (2) on Slack, (3) in small groups during synchronous class sessions. "],["final-project.html", "Final Project Requirements", " Final Project Requirements You will be analyzing a dataset using a regression, classification, and unsupervised learning analysis. You must complete the Project Work parts of the 4 weekly homework assignments. These are incremental investigations that build from our applied class exercises. You will synthesize these investigations at the end of the module. Collaboration: You may work in teams of up to 3 members. Individual work is fine. The weekly homework assignments will note whether work for that week should be submitted individually or if just one team member should submit work. There will be a required synthesis of the weekly homework investigations at the end of the course (e.g., a final report and/or presentation). If working on a team, this should be done in groups, rather than individually. Still being determined. Updates will appear on this page later in the course. "],["r-resources.html", "R Resources Outside resources Example code", " R Resources Outside resources Lisa Lendway’s COMP/STAT 112 website (with code examples and videos) RStudio cheat sheets ggplot2 reference R Programming Wikibook Debugging in R Article Video Colors in R Data import tutorial Free online textbooks R for Data Science Exploratory Data Analysis with R Example code Creating new variables case_when() from the dplyr package is a very versatile function for creating new variables based on existing variables. This can be useful for creating categorical or quantitative variables and for creating indices from multiple variables. # Turn quant_var into a Low/Med/High version data &lt;- data %&gt;% mutate(cat_var = case_when( quant_var &lt; 10 ~ &quot;Low&quot;, quant_var &gt;= 10 &amp; quant_var &lt;= 20 ~ &quot;Med&quot;, quant_var &gt; 20 ~ &quot;High&quot; ) ) # Turn cat_var (A, B, C categories) into another categorical variable # (collapse A and B into one category) data &lt;- data %&gt;% mutate(new_cat_var = case_when( cat_var %in% c(&quot;A&quot;, &quot;B&quot;) ~ &quot;A or B&quot; cat_var==&quot;C&quot; ~ &quot;C&quot; ) ) # Turn a categorical variable (x1) encoded as a numerical 0/1/2 variable into a different quantitative variable # Doing this for multiple variables allows you to create an index data &lt;- data %&gt;% mutate(x1_score = case_when( x1==0 ~ 10, x1==1 ~ 20, x1==2 ~ 50 ) ) # Add together multiple variables with mutate data &lt;- data %&gt;% mutate(index = x1_score + x2_score + x3_score) "]]
