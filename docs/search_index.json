[["index.html", "STAT 253: Statistical Machine Learning Welcome!", " STAT 253: Statistical Machine Learning Welcome! Image source This is the class manual for Statistical Machine Learning (STAT 253) at Macalester College for Spring 2021. The content here was made by Leslie Myint and draws upon our class textbook, An Introduction to Statistical Learning with Applications in R, and on materials prepared by Alicia Johnson. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["schedule.html", "Schedule", " Schedule The schedule below is a tentative outline of our plans for the module. Before each class period, please watch the indicated videos and check on your understanding by actively reviewing the associated Learning Objectives. The readings listed below are optional but serve as a nice complement to the videos and class activities. Readings refer to chapters/sections in the Introduction to Statistical Learning (ISLR) textbook (available online here). Week 1: 3/18 - 3/19 Day(s) Topic Videos/Readings Slides 3/18 Introductions ISLR: Chap 1, Chap 2 - Section 2.1 (Skip 2.1.2, 2.1.3 for now.) 3/19 Evaluating Regression Models Evaluating Regression Models ISLR: 2.2 PDF Homework 1 due Friday, 3/26 at midnight CST Week 2: 3/22 - 3/25 Day(s) Topic Videos/Readings Slides 3/22 Overfitting and Cross-validation Overfitting Cross-validation ISLR: 5.1 PDF PDF 3/23 Subset Selection Variable Subset Selection ISLR: 6.1 PDF 3/24-3/25 LASSO (Shrinkage/Regularization) LASSO (Shrinkage/Regularization) ISLR: 6.2 PDF Homework 1 due Friday, 3/26 at midnight CST Week 3: 3/29 - 4/1 Day(s) Topic Videos/Readings Slides 3/29 KNN Regression and the Bias-Variance Tradeoff KNN Regression and the Bias-Variance Tradeoff ISLR: 2.2.2 for the bias-variance tradeoff; 3.5 for KNN regression PDF 3/30 Modeling Nonlinearity: Polynomial Regression and Splines Modeling Nonlinearity: Polynomial Regression and Splines ISLR: 7.1-7.4 PDF 3/31 Local Regression and Generalized Additive Models Local Regression and Generalized Additive Models ISLR: 7.6-7.7 PDF 4/1 Review: Regression Homework 2 due Friday, 4/2 at midnight CST Week 4: 4/5 - 4/8 Day(s) Topic Videos/Readings Slides 4/5 Quiz 1 (no class - complete quiz on your own) 4/6 Logistic Regression Logistic Regression ISLR: 4.1 - 4.3 PDF 4/7 Evaluating Classification Models Evaluating Classification Models PDF 4/8 Decision Trees Decision Trees ISLR: 8.1 PDF Homework 3 due Friday, 4/9 at midnight CST Week 5: 4/12 - 4/15 Day(s) Topic Videos/Readings Slides 4/12 Wellness Day (no class) 4/13 Decision Trees (continued) 4/14 Bagging and Random Forests Bagging and Random Forests ISLR: 8.2 PDF Homework 4 due Friday, 4/16 at midnight CST Week 6: 4/19 - 4/22 Day(s) Topic Videos/Readings Slides 4/19 Quiz 2 (no class - complete quiz on your own) 4/20 K-Means Clustering K-Means Clustering ISLR: 10.3.1 PDF 4/21 Hierarchical Clustering Hierarchical Clustering ISLR: 10.3.2 PDF 4/22 Asynchronous work day Week 7: 4/26 - 4/29 Day(s) Topic Videos/Readings Slides 4/26 Wellness Day (no class) 4/27 - 4/28 Principal Components Analysis Principal Components Analysis ISLR: 10.2 PDF 4/29 Asynchronous work day Week 8: 5/3 - 5/5 Day(s) Topic Videos/Readings Slides 5/3 Quiz 3 (no class - complete quiz on your own) 5/4 - 5/5 Asynchronous work days "],["learning-objectives.html", "Learning Objectives", " Learning Objectives Learning objectives for our course topics are listed below. Use these to guide your synthesis of video and reading material. Introduction to Statistical Machine Learning Formulate research questions that align with regression, classification, or unsupervised learning tasks Evaluating Regression Models Create and interpret residuals vs. fitted, residuals vs. predictor plots to identify improvements in modeling and address ethical concerns Interpret MSE, RMSE, MAE, and R-squared in a contextually meaningful way Overfitting and cross-validation Explain why training/in-sample model evaluation metrics can provide a misleading view of true test/out-of-sample performance Accurately describe all steps of cross-validation to estimate the test/out-of-sample version of a model evaluation metric Explain what role CV has in a predictive modeling analysis and its connection to overfitting Explain the pros/cons of higher vs. lower k in k-fold CV in terms of sample size and computing time Subset selection Clearly describe the forward and backward stepwise selection algorithm and why they are examples of greedy algorithms Compare best subset and stepwise algorithms in terms of optimality of output and computational time LASSO (shrinkage/regularization) Explain how ordinary and penalized least squares are similar and different with regard to (1) the form of the objective function and (2) the goal of variable selection Explain how the lambda tuning parameter affects model performance and how this is related to overfitting KNN Regression and the Bias-Variance Tradeoff Clearly describe / implement by hand the KNN algorithm for making a regression prediction Explain how the number of neighbors relates to the bias-variance tradeoff Explain the difference between parametric and nonparametric methods Explain how the curse of dimensionality relates to the performance of KNN (not in the video–will be discussed in class) Modeling Nonlinearity: Polynomial Regression and Splines Explain the advantages of splines over global transformations and other types of piecewise polynomials Explain how splines are constructed by drawing connections to variable transformations and least squares Explain how the number of knots relates to the bias-variance tradeoff Local Regression and Generalized Additive Models Clearly describe the local regression algorithm for making a prediction Explain how bandwidth (span) relate to the bias-variance tradeoff Describe some different formulations for a GAM (how the arbitrary functions are represented) Explain how to make a prediction from a GAM Interpret the output from a GAM Logistic regression Use a logistic regression model to make hard (class) and soft (probability) predictions Interpret non-intercept coefficients from logistic regression models in the data context Evaluating classification models Calculate (by hand from confusion matrices) and contextually interpret overall accuracy, sensitivity, and specificity Construct and interpret plots of predicted probabilities across classes Explain how a ROC curve is constructed and the rationale behind AUC as an evaluation metric Appropriately use and interpret the no-information rate to evaluate accuracy metrics Decision trees Clearly describe the recursive binary splitting algorithm for tree building for both regression and classification Compute the weighted average Gini index to measure the quality of a classification tree split Compute the sum of squared residuals to measure the quality of a regression tree split Explain how recursive binary splitting is a greedy algorithm Explain how different tree parameters relate to the bias-variance tradeoff Bagging and random forests Explain the rationale for bagging Explain the rationale for selecting a random subset of predictors at each split (random forests) Explain how the size of the random subset of predictors at each split relates to the bias-variance tradeoff Explain the rationale for and implement out-of-bag error estimation for both regression and classification Explain the rationale behind the random forest variable importance measure and why it is biased towards quantitative predictors (in class) "],["r-and-rstudio-setup.html", "R and RStudio Setup Troubleshooting", " R and RStudio Setup Before class on Friday, March 19, you should follow these instructions to set up the software that we’ll be using throughout the semester. Even if you’ve already downloaded both R and RStudio, you’ll want to re-download to make sure that you have the most current versions. Highly recommended: Change the default file download location for your internet browser. Generally by default, internet browsers automatically save all files to the Downloads folder on your computer. This does not encourage good file organization practices. It is highly recommended that you change this option so that your browser asks you where to save each file before downloading it. This page has information on how to do this for the most common browsers. Required: Download R and RStudio FIRST: Download R here. You will see three links “Download R for …” Choose the link that corresponds to your computer. As of March 16, 2021, the latest version of R is 4.0.4. SECOND: Download RStudio here. Click the button under step 2 to install the version of RStudio recommended for your computer. As of March 16, 2021, the latest version of RStudio is 1.4.1106. Suggested: Watch this video made by Professor Lisa Lendway that describes essential configuration options for RStudio. Required: Install required packages. An R package is an extra bit of functionality that will help us in our data analysis efforts in a variety of ways. Open RStudio and click inside the Console pane (by default, the bottom left pane). Copy and paste the following command into the Console. You should see the text below appear to the right of the &gt;, which is called the R prompt. After you paste, hit Enter. install.packages(c(&quot;ggplot2&quot;, &quot;dplyr&quot;, &quot;readr&quot;, &quot;rmarkdown&quot;, &quot;broom&quot;, &quot;caret&quot;)) You will see a lot of text from status messages appearing in the Console as the packages are being installed. Wait until you see the &gt; again. Enter the command library(ggplot2) and hit enter. If you see the message Error in library(ggplot2) : there is no package called ggplot2, then there was a problem installing this package. Jump down to the Troubleshooting section below. (Any other messages that appear are fine, and a lack of any messages is also fine.) Repeat the above step for the commands: library(dplyr) library(readr) library(rmarkdown) library(broom) library(caret) Quit RStudio. You’re done setting up! Optional: For a refresher on RStudio features, watch this video. It also shows you how to customize the layout and color scheme of RStudio. Troubleshooting Problem: You are on a Mac and getting the following error: Error: package or namespace load failed for ‘ggplot2’ in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]): there is no package called ‘rlang’ Here’s how to fix it: First install the suite of Command Line Tools for Mac using the instructions here. Next enter install.packages(\"rlang\") in the Console. Finally check that entering library(ggplot2) gives no errors. You should be good now for dplyr, readr, and rmarkdown. "],["introductions.html", "Topic 1 Introductions Envisioning an Ideal Module 4 Explorations", " Topic 1 Introductions Slides from today are available here. Envisioning an Ideal Module 4 Directions: In your breakout rooms, please first introduce yourselves in whatever way you feel appropriate (e.g., preferred name, pronouns, how you’re feeling at the moment, things you’re looking forward to). When everyone is ready, discuss the 3 prompts below and record thoughts in this Google Doc. The instructor will summarize responses from both sections to create a resource that everyone can use. Prompts: Wonderful leaders in the Macalester community have complied the following set of agreements to consider when working in small groups. Which of the following do you think are most important to keep in mind for our time together in this course and why? Confidentiality: Take the lesson, not the story Use “I” statements; Speak your truth W.A.I.T. (Why Am I Talking/Why Aren’t I Talking) Listen for understanding Unpack assumptions Extend and receive grace Understand impact vs. intention Breathe and lean into discomfort Accept non-closure What strategies have you found work well for you to succeed both in and out of class in these times? What are some things that have contributed to positive experiences in your courses that you would like to see again this module? What has contributed to negative experiences that you would like to prevent? Explorations Each of the data contexts below prompts a broad research goal. For each, sharpen the focus of that goal by coming up with more targeted research questions that: Can be studied with a regression exploration Can be studied with a classification exploration Can be studied with an unsupervised learning exploration Are there harms that you anticipate arising from the collection of data or its analysis? Context 1: The New York Times is trying to better understand the popularity of its different articles with the hope of using this understanding to improve hiring of writers and improve their website layout. Context 2: The Minnesota Department of Health is trying to better understand the different health trajectories of people who have contracted a certain illness to improve funding for relevant health services. Context 3: The campaign management team for a political candidate is trying to better understand voter turnout in different regions of the country to run a better campaign in the next election. "],["evaluating-regression-models.html", "Topic 2 Evaluating Regression Models Learning Goals Exercises", " Topic 2 Evaluating Regression Models Learning Goals Create and interpret residuals vs. fitted, residuals vs. predictor plots to identify improvements in modeling and address ethical concerns Interpret MSE, RMSE, MAE, and R-squared in a contextually meaningful way We’ll also start to develop some new ideas relating to our next topics. Slides from today are available here. Exercises You can download a template RMarkdown file to start from here. Context We’ll be working with a dataset containing physical measurements on 80 adult males. These measurements include body fat percentage estimates as well as body circumference measurements. fatBrozek: Percent body fat using Brozek’s equation: 457/Density - 414.2 fatSiri: Percent body fat using Siri’s equation: 495/Density - 450 density: Density determined from underwater weighing (gm/cm^3). age: Age (years) weight: Weight (lbs) height: Height (inches) neck: Neck circumference (cm) chest: Chest circumference (cm) abdomen: Abdomen circumference (cm) hip: Hip circumference (cm) thigh: Thigh circumference (cm) knee: Knee circumference (cm) ankle: Ankle circumference (cm) biceps: Biceps (extended) circumference (cm) forearm: Forearm circumference (cm) wrist: Wrist circumference (cm) It takes a lot of effort to estimate body fat percentage accurately through underwater weighing. The goal is to build the best predictive model for fatSiri using just circumference measurements, which are more easily attainable. (We won’t use fatBrozek or density as predictors because they’re other outcome variables.) library(readr) library(ggplot2) library(dplyr) library(broom) bodyfat &lt;- read_csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) # Remove the fatBrozek and density variables bodyfat &lt;- bodyfat %&gt;% select(-fatBrozek, -density) Class investigations We’ll work through this section together to review concepts and code. You’ll then work on the remainder of the exercises in your groups. # Exploratory plots # Univariate distribution of outcome ggplot(bodyfat, aes(???)) + geom_???() # Scatterplot of fatSiri vs. weight ggplot(bodyfat, aes(???)) + geom_???() Let’s fit a linear regression model to predict body fat percentage from weight. mod1 &lt;- lm(??? ~ ???, data = bodyfat) We can use the augment() function from the broom package to augment our original data with useful information from the model fitting. In particular, residuals are stored in the .fitted column, and fitted (predicted) values for the cases supplied in newdata are stored in the .fitted column. mod1_output &lt;- broom::augment(mod1, newdata = bodyfat) head(mod1_output) We can use the augment() output to compute error metrics… # MSE - what is the interpretation with units? mean(mod1_output$.resid^2) # RMSE - what is the interpretation with units? mean(mod1_output$.resid^2) %&gt;% sqrt() # MAE - what is the interpretation with units? mean(abs(mod1_output$.resid)) # R-squared - interpretation? (unit-less) 1 - (var(mod1_output$.resid) / var(mod1_output$fatSiri)) …and to create residual plots: # Quick plot ggplot(mod1_output, aes(x = .fitted, y = .resid)) + geom_point() + geom_smooth() + geom_hline(yintercept = 0, color = &quot;red&quot;) # Residuals vs. predictors ggplot(mod1_output, aes(x = height, y = .resid)) + geom_point() + geom_smooth() + geom_hline(yintercept = 0, color = &quot;red&quot;) Exercise 1 First decide on what you think would be a good model by picking variables based on context. Fit this model, calling it mod_initial. (Remember that you can include several predictors with a + in the model formula - like y ~ x1+x2.) # Code to fit initial model Use residual plot explorations to check if you need to update your model. # Residual plot explorations Fit your updated model, and call it model_updated. # Code to fit updated model Exercise 2 Compute and contextually interpret relevant evaluation metrics for your model. # Code to compute evaluation metrics Exercise 3 Now that you’ve selected your best model, deploy it in the real world by applying it to a new set of 172 adult males. You’ll need to update the newdata argument of the broom::augment() code to use bodyfat_test instead of bodyfat. bodyfat_test &lt;- read_csv(&quot;https://www.dropbox.com/s/7gizws208u0oywq/bodyfat_test.csv?dl=1&quot;) Compare your evaluation metrics from Exercise 2 the metrics here. What do you notice? (Note: this observation is just based on your one particular fitted model. You’ll make some more comprehensive observations in the next exercise.) In general, do you think that models with more or less variables would perform better in this comparison? Explain. Exercise 4 The code below systematically looks at the same comparison that you made in Exercise 3 but for every possible linear regression model formed from inclusion/exclusion of the predictors (without transformations or interactions). Run the code to make a plot of the results of this systematic investigation. (Feel free to inspect the code if you’re curious, but otherwise, don’t worry about understanding it fully.) What do you notice? What do you wonder? get_maes &lt;- function(mod, train_data, test_data) { mod_output_train &lt;- broom::augment(mod, newdata = train_data) mod_output_test &lt;- broom::augment(mod, newdata = test_data) train_mae &lt;- mean(abs(mod_output_train$.resid)) test_mae &lt;- mean(abs(mod_output_test$.resid)) c(train_mae, test_mae) } possible_predictors &lt;- setdiff(colnames(bodyfat), c(&quot;fatSiri&quot;, &quot;hipin&quot;)) results &lt;- bind_rows(lapply(1:13, function(i) { combos &lt;- combn(possible_predictors, i) bind_rows(lapply(seq_len(ncol(combos)), function(j) { formula &lt;- paste(&quot;fatSiri ~&quot;, paste(combos[,j], collapse = &quot;+&quot;)) mod &lt;- lm(as.formula(formula), data = bodyfat) maes &lt;- get_maes(mod = mod, train_data = bodyfat, test_data = bodyfat_test) tibble( form = formula, train_mae = maes[1], test_mae = maes[2], num_predictors = i ) })) })) # Relabel the num_predictors variable results &lt;- results %&gt;% mutate(num_predictors = paste(&quot;# predictors:&quot;, num_predictors)) %&gt;% mutate(num_predictors = factor(num_predictors, levels = paste(&quot;# predictors:&quot;, 1:13))) # Plot results ggplot(results, aes(x = train_mae, y = test_mae, color = num_predictors)) + geom_point() + coord_cartesian(xlim = c(0,7.5), ylim = c(0,7.5)) + geom_abline(slope = 1, intercept = 0) + facet_wrap(~num_predictors) + guides(color = FALSE) + labs(x = &quot;MAE on original data&quot;, y = &quot;MAE on new data on 172 men&quot;) + theme_classic() "],["overfitting-cross-validation.html", "Topic 3 Overfitting &amp; Cross-validation Learning Goals The caret package Exercises", " Topic 3 Overfitting &amp; Cross-validation Learning Goals Explain why training/in-sample model evaluation metrics can provide a misleading view of true test/out-of-sample performance Accurately describe all steps of cross-validation to estimate the test/out-of-sample version of a model evaluation metric Explain what role CV has in a predictive modeling analysis and its connection to overfitting Explain the pros/cons of higher vs. lower k in k-fold CV in terms of sample size and computing time Implement cross-validation in R using the caret package Slides from today are available here. The caret package (If you have not already installed the caret package, install it with install.packages(\"caret\").) Over this course, we will looking at a broad but linked set of specialized tools applied in statistical machine learning. Specialized tools generally require specialized code. The caret (Classification And REgression Training) package helps us streamline much of this specialized code. We’ll learn some tweaks along the way, but the majority of our caret code will use the train() function, which allows us to build and evaluate various models. It looks like a lot, but remember that we’ll be using this over and over: # Load the package library(caret) # Set the seed for the random number generator set.seed(___) # Run the algorithm my_model &lt;- train( y ~ x, data = ___, method = ___, tuneGrid = data.frame(___), trControl = trainControl(method = ___, number = ___, selectionFunction = ___), metric = ___, na.action = na.omit ) Argument Meaning y ~ x Specifies the outcome and predictor variables (just like lm()). data Sample data method Modeling method to use (e.g., \"lm\", \"knn\") tuneGrid Modeling method’s tuning parameters (parameters which define how it works) trControl Method by which to train and test the model (typically cross-validation). When we build multiple models, selectionFunction describes the process by which to select a final model. metric When we build multiple models, this is the metric by which we’ll evaluate and compare them (e.g., RMSE, MAE). na.action What to do about missing data values. We typically set na.action = na.omit to prevent errors if the data has missing values. The power of caret is that it allows us to streamline the vast world of machine learning techniques into one common syntax. On top of \"lm\", check out the different machine learning methods that we can use in caret. We’ll see several of these throughout the course: names(getModelInfo()) You can find more detailed descriptions of these methods here and in a searchable table here. In the exercises below, you’ll need to adapt the following code to perform the CV procedure on a linear regression model (\"lm\"): # Regression model with CV error my_model &lt;- train( y ~ x, data = ___, method = &quot;lm&quot;, trControl = trainControl(method = &quot;cv&quot;, number = your_k), na.action = na.omit ) Notice how this specifies the general caret code: method = \"lm\" indicates that we want to fit a linear regression model trControl = trainControl(method = \"cv\", number = your_k) indicates that we want to train and test the model using cross validation (\"cv\"). You need to specify the number of folds, your_k. tuneGrid: absent Linear regression doesn’t depend upon any tuning parameters (we’re just minimizing the sum of squared residuals). selectionFunction and metric: absent We’re only building one model, not comparing multiple models, so we don’t need a metric by which to compare models or a method by which to select a model. After building the model, you can check out the results: # Summarize the model summary(my_model) # Get model evaluation metrics for each fold my_model$resample # Get CV evaluation metrics my_model$results Exercises You can download a template RMarkdown file to start from here. Context We’ll be working with a dataset containing physical measurements on 80 adult males. These measurements include body fat percentage estimates as well as body circumference measurements. fatBrozek: Percent body fat using Brozek’s equation: 457/Density - 414.2 fatSiri: Percent body fat using Siri’s equation: 495/Density - 450 density: Density determined from underwater weighing (gm/cm^3). age: Age (years) weight: Weight (lbs) height: Height (inches) neck: Neck circumference (cm) chest: Chest circumference (cm) abdomen: Abdomen circumference (cm) hip: Hip circumference (cm) thigh: Thigh circumference (cm) knee: Knee circumference (cm) ankle: Ankle circumference (cm) biceps: Biceps (extended) circumference (cm) forearm: Forearm circumference (cm) wrist: Wrist circumference (cm) It takes a lot of effort to estimate body fat percentage accurately through underwater weighing. The goal is to build the best predictive model for fatSiri using just circumference measurements, which are more easily attainable. (We won’t use fatBrozek or density as predictors because they’re other outcome variables.) library(readr) library(ggplot2) library(dplyr) library(broom) library(caret) bodyfat_train &lt;- read_csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) # Remove the fatBrozek and density variables bodyfat_train &lt;- bodyfat_train %&gt;% select(-fatBrozek, -density) Exercise 1: 4 models Consider the 4 models below: mod1 &lt;- lm(fatSiri ~ age+weight+neck+abdomen+thigh+forearm, data = bodyfat_train) mod2 &lt;- lm(fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps, data = bodyfat_train) mod3 &lt;- lm(fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps+chest+hip, data = bodyfat_train) mod4 &lt;- lm(fatSiri ~ ., data = bodyfat_train) # The . means all predictors Which model will have the lowest training RMSE, and why? Compute the RMSE for models 1 and 4 to (partially) check your answer for part a. Which model do you think will perform worst on new test data? Why? Exercise 2: Cross-validation with caret Complete the code below to perform 10-fold cross-validation for mod1 to estimate the test RMSE (\\(\\text{CV}_{(10)}\\)). Do we need to use set.seed()? Why or why not? (Is there a number of folds for which we would not need to set the seed?) # Do we need to use set.seed()? mod1_cv &lt;- train( ) STAT 155 review: Look at the summary() of mod1_cv. Contextually interpret the coefficient for the weight predictor. Is anything surprising? Why might this be? Look at mod1_cv$resample, and use this to calculate the 10-fold cross-validated RMSE by hand (the idea is the same as when using MSE). (Note: We haven’t done this together, but how can you adapt code that we’ve used before?) Check your answer to part c by directly printing out the CV metrics: mod1_cv$results. Interpret this metric. Exercise 3: Looking at the evaluation metrics Look at the completed table below of evaluation metrics for the 4 models. Which model performed the best on the training data? Which model performed best on the test set? Explain why there’s a discrepancy between these 2 answers and why CV, in general, can help prevent overfitting. Model Training RMSE \\(\\text{CV}_{(10)}\\) mod1 3.810712 4.389568 mod2 3.766645 4.438637 mod3 3.752362 4.517281 mod4 3.572299 4.543343 Exercise 4: Practical issues: choosing \\(k\\) In terms of sample size, what are the pros/cons of low vs. high \\(k\\)? In terms of computational time, what are the pros/cons of low vs. high \\(k\\)? If possible, it is advisable to choose \\(k\\) to be a divisor of the sample size. Why do you think that is? Digging deeper If you have time, consider these exercises to further explore concepts related to today’s ideas. caret’s trainControl() function also has a \"repeatedcv\" method. Just from the name, how do you think this method differs from \"cv\"? What are the pros/cons of \"repeatedcv\" as compared to \"cv\"? Adapt the train() code to perform leave-one-out-cross-validation (LOOCV). Hint: nrow(dataset) obtains the number of cases in the dataset. Do we need set.seed()? Why or why not? Using the information from your_output$resample (which is a dataset), construct a visualization to examine the variability of RMSE from case to case. What might explain any very large values? What does this highlight about the quality of estimation of the LOOCV process? "],["variable-subset-selection.html", "Topic 4 Variable Subset Selection Learning Goals Exercises", " Topic 4 Variable Subset Selection Learning Goals Clearly describe the forward and backward stepwise selection algorithm and why they are examples of greedy algorithms Compare best subset and stepwise algorithms in terms of optimality of output and computational time Describe how selection algorithms can give a measure of variable importance Exercises You can download a template RMarkdown file to start from here. We’ll continue using the body fat dataset to explore subset selection methods. library(caret) library(ggplot2) library(dplyr) library(readr) bodyfat &lt;- read_csv(&quot;http://www.macalester.edu/~ajohns24/data/bodyfatsub.csv&quot;) # Take out the redundant Density and HeightFt variables bodyfat &lt;- bodyfat %&gt;% select(-Density, -HeightFt) Exercise 1: Backward stepwise selection: by hand In the backward stepwise procedure, we start with the full model, full_model, with all predictors: full_model &lt;- lm(BodyFat ~ Age + Weight + Height + Neck + Chest + Abdomen + Hip + Thigh + Knee + Ankle + Biceps + Forearm + Wrist, data = bodyfat) To practice the backward selection algorithm, step through a few steps of the algorithm using p-values as a selection criterion: Identify which predictor contributes the least to the model. One (problematic) approach is to identify the least significant predictor. Fit a new model which eliminates this predictor. Identify the least significant predictor in this model. Fit a new model which eliminates this predictor. Repeat 1 more time to get the hang of it. (We discussed in the video how the use of p-values for selection is problematic, but for now you’re just getting a handle on the algorithm. You’ll think about the problems with p-values in the next exercise.) Exercise 2: Interpreting the results Examine which predictors remain after the previous exercise. Are you surprised that, for example, Wrist is still in the model but Weight is not? Does this mean that Wrist is a better predictor of body fat percentage than Weight is? What statistical idea is relevant here? Exercise 3: Planning forward selection using CV Using p-values to perform stepwise selection has problems, as was discussed in the video. A better alternative to target predictive accuracy is to evaluate the models using cross-validation. Fully outline the steps required to use cross-validation to perform forward selection. Make sure to provide enough detail such that the stepwise selection and CV algorithms are made clear. Exercise 4: Stepwise selection in caret Run install.packages(\"leaps\") in the Console to install the leaps package before proceeding. Complete the caret code below to perform backward stepwise selection with cross-validation. The following points will help you complete the code: In R model formulas, y ~ . sets y as the outcome and all other predictors in the dataset as predictors. The specific method name for backward selection is \"leapBackward\". The tuneGrid argument is already filled in. It allows us to input tuning parameters into the fitting process. The tuning parameters for subset selection are the number of variables included in the models explored (nvmax). This can vary from 1 to 13 (the maximum number of predictors possible). Use 10-fold CV to estimate test performance of the models. Use \"MAE\" as the evaluation metric to choose how the best of the 1-variable, 2-variable, etc. models will be chosen. (Note: CV is only used to pick among the best 1, 2, 3, …, and 13 variable models. To find the best 1, 2, 3, …, and 13 variable models, training MSE is used. caret uses training MSE because within a subset size, all models have the same number of coefficients, which makes both ordinary R-squared and training MSE ok for comparing models.) set.seed(23) back_step_mod &lt;- train( y ~ x, data = bodyfat, method = ___, tuneGrid = data.frame(nvmax = 1:13), trControl = ___, metric = ___, na.action = na.omit ) Exercise 5: Exploring the results There are a number of ways to examine and use the output of the selection algorithm, which we’ll explore here. (It would be useful to make notes along the way - perhaps on your code note sheet.) Part a Let’s first examine the sequence of models explored. The stars in the table at the bottom indicate the variables included in the 1-variable, 2-variable, etc. models. summary(back_step_mod) Of the 13 models in the sequence, R only prints out the 11 smallest models (since, for reasons we’ll discuss below, it determines the 11 predictor model to be “best”). Which predictor is the last to remain in the model? Second-to-last to remain? How do you think we could use these results to identify which predictors were most/least important in predicting the outcome of body fat percentage? Part b Examine the 10-fold CV MAE for each of the 13 models in the backward stepwise sequence: # Plot metrics for each model in the sequence plot(back_step_mod) # Look at accuracy/error metrics for the different subset sizes back_step_mod$results Which size model has the lowest CV MAE? Which size model would you pick? Why? Part c In our model code, we used selectionFunction = \"best\" by default inside trainControl(). By doing so, we indicated that we wanted to find which model minimizes the CV MAE (i.e., has the “best” MAE). With respect to this criterion: # What tuning parameter gave the best performance? # i.e. What subset size gave the best model? back_step_mod$bestTune # Obtain the coefficients for the best model coef(back_step_mod$finalModel, id = back_step_mod$bestTune$nvmax) # Obtain the coefficients of any size model with at most as many variables as the overall best model (e.g., the 2-predictor model) coef(back_step_mod$finalModel, id = 2) Another sensible choice for the selection function is to not choose the model with the lowest estimated error but to account for the uncertainty in the estimation of that test error by picking the smallest model for which the CV MAE is within one standard error of the minimum CV MAE. What do you think the rationale for this is? (We’ll explore the code for this formally later, or you can try it out below in the Digging Deeper section.) Part d We should end by evaluating our final chosen model. Contextually interpret (with units) the CV MAE for the model. Make residual plots for the chosen model in one of 2 ways: (1) use lm() to fit the model with the chosen predictors or (2) use the following code to create a dataset called back_step_mod_out which contains the original data as well as predicted values and residuals (fitted and resid). back_step_mod_out &lt;- bodyfat %&gt;% mutate( fitted = predict(back_step_mod, newdata = bodyfat), resid = BodyFat - fitted ) Digging deeper As mentioned in Exercise 5c, we have another choice for the selectionFunction used to choose from many possible models. The use of selectionFunction = \"oneSE\" below picks the simplest model for which the CV MAE is within one standard error of the minimum CV MAE. Compare the output you obtain here with your best model from Exercise 5. back_step_mod_1se &lt;- train( BodyFat ~ ., data = bodyfat, method = &quot;leapBackward&quot;, tuneGrid = data.frame(nvmax = 1:13), trControl = trainControl(method = &quot;cv&quot;, number = 10, selectionFunction = &quot;oneSE&quot;), metric = &quot;MAE&quot;, na.action = na.omit ) Forward selection can be implemented in caret with analogous code, except that the method name is \"leapForward\". Try implementing forward selection and comparing your results. "],["lasso-shrinkageregularization.html", "Topic 5 LASSO: Shrinkage/Regularization Learning Goals LASSO models in caret Exercises", " Topic 5 LASSO: Shrinkage/Regularization Learning Goals Explain how ordinary and penalized least squares are similar and different with regard to (1) the form of the objective function and (2) the goal of variable selection Explain why variable scaling is important for the performance of shrinkage methods Explain how the lambda tuning parameter affects model performance and how this is related to overfitting Describe how output from LASSO models can give a measure of variable importance Slides from today are available here. LASSO models in caret To build LASSO models in caret, first load the package and set the seed for the random number generator to ensure reproducible results: library(caret) set.seed(___) # Pick your favorite number to fill in the parentheses Then adapt the following code: # Perform LASSO lasso_mod &lt;- train( y ~ ., data = ___, method = &quot;glmnet&quot;, tuneGrid = data.frame(alpha = 1, lambda = seq(___, ___, length.out = ___)), trControl = trainControl(method = &quot;cv&quot;, number = ___, selectionFunction = ___), metric = &quot;MAE&quot;, na.action = na.omit ) Argument Meaning y ~ . Shorthand for the model of y by all possible predictors x in the data (every other variable in the dataset except y) data Sample data which only includes y and potential predictors x. (Make sure to remove unneeded variables with select(-variable) first.) method \"glmnet\" implements the LASSO and other regularization methods tuneGrid A mini-dataset (data.frame) of tuning parameters. alpha = 1 specifies that we’re interested in the LASSO (not a different regularization method, like ridge regression). lambda = seq(begin, end, length.out = # values in sequence) specifies that LASSO should be run for each tuning parameter value in the specified sequence. trControl Use cross-validation to estimate test performance for each LASSO model fit (corresponding to each tuning parameter in tuneGrid). The process used to pick a final model from among these is indicated by the selectionFunction argument, options including \"best\" and \"oneSE\". The best option will pick the model with the best metric. The oneSE option will pick the simplest model for which the metric is within one standard error of the best metric. metric Evaluate and compare competing LASSO models with respect to their CV-MAE. na.action Set na.action = na.omit to prevent errors if the data has missing values. Note: caret automatically implements variable standardization (scaling variables to have mean 0 and standard deviation 1) when using the \"glmnet\" method. Examining the LASSO model for each \\(\\lambda\\) # Plot coefficient paths as a function of lambda plot(lasso_mod$finalModel, xvar = &quot;lambda&quot;, label = TRUE, col = rainbow(20)) # Codebook for which variables the numbers correspond to rownames(lasso_mod$finalModel$beta) Identifying the “best” LASSO model The “best” model in the sequence of models fit is defined relative to the chosen selectionFunction and metric. # Plot CV-estimated test performance versus the tuning parameter (lambda) plot(lasso_mod) # CV metrics for each LASSO model lasso_mod$results # Identify which tuning parameter (lambda) is &quot;best&quot; lasso_mod$bestTune # Obtain the predictors and coefficients of the &quot;best&quot; model # The .&#39;s indicate that the coefficient is 0 coef(lasso_mod$finalModel, lasso_mod$bestTune$lambda) # Obtain the predictors and coefficients of LASSO model w/ different lambda # e.g., lambda = 1 (roughly) coef(lasso_mod$finalModel, 1) # Get information from all CV iterations for the &quot;best&quot; model lasso_mod$resample Exercises You can download a template RMarkdown file to start from here. We’ll continue using the body fat dataset to explore LASSO modeling. library(caret) library(ggplot2) library(dplyr) library(readr) bodyfat &lt;- read_csv(&quot;http://www.macalester.edu/~ajohns24/data/bodyfatsub.csv&quot;) # Take out the redundant Density and HeightFt variables bodyfat &lt;- bodyfat %&gt;% select(-Density, -HeightFt) Exercise 1: A least squares model Let’s start by building an ordinary (not penalized) least squares model to review important concepts. We’ll fit a model with all possible predictors. ls_mod &lt;- lm(BodyFat ~ ., data = bodyfat) Use caret to perform 10-fold cross-validation to estimate test MAE for this model. How do you think the estimated test error would change with fewer predictors? This model fit with ordinary least squares corresponds to a special case of penalized least squares. What is the value of \\(\\lambda\\) in this special case? As \\(\\lambda\\) increases, what would you expect to happen to the number of predictors that remain in the model? Exercise 2: Fitting a LASSO model in caret Adapt our general LASSO code to fit a set of LASSO models with the following parameters: Use 10-fold CV. Use mean absolute error (MAE) to select a final model. Select the simplest model for which the metric is within one standard error of the best metric. Use a sequence of 100 \\(\\lambda\\) values from 0 to 10. Before running the code, enter install.packages(\"glmnet\") in the Console. We’ll explore output from lasso_mod in the next exercises. # Fit LASSO models for a grid of lambda values set.seed(74) lasso_mod &lt;- train( ) Exercise 3: Examining output: plot of coefficient paths A useful first plot allows us to examine coefficient paths resulting from the fitted LASSO models: coefficient estimates as a function of \\(\\lambda\\). # Plot coefficient paths as a function of lambda plot(lasso_mod$finalModel, xvar = &quot;lambda&quot;, label = TRUE, col = rainbow(20)) # Codebook for which variables the numbers correspond to rownames(lasso_mod$finalModel$beta) # e.g., What are variables 2 and 4? rownames(lasso_mod$finalModel$beta)[c(2,4)] There’s a lot of information in this plot! Each colored line corresponds to a different predictor. The small number to the left of each line indicates a predictor by its position in rownames(lasso_mod$finalModel$beta). The x-axis reflects the range of different \\(\\lambda\\) values (on the log-scale) considered in lasso_mod (in tuneGrid). At each \\(\\lambda\\), the y-axis reflects the coefficient estimates for the predictors in the corresponding LASSO model. At each \\(\\lambda\\), the numbers at the top of the plot indicate how many predictors remain in the corresponding model. Very roughly eyeball the coefficient estimates at the smallest value of \\(\\lambda\\). Do they look like they correspond to the coefficient estimates from ordinary least squares in exercise 2? Why do all of the lines head toward y = 0 on the far right of the plot? We can zoom in on the plot by setting the y-axis limits to go from -0.5 to 1 with ylim as below. Compare the lines for variables 6 and 12. What are variables 6 and 12? Which seems to be a more “important” or “persistent” (persistently present in the model) variable? Does this make sense in context? # Zoom in plot(lasso_mod$finalModel, xvar = &quot;lambda&quot;, label = TRUE, col = rainbow(20), ylim = c(-0.5,1)) Which predictor seems least “persistent”? In general, how might we use these coefficient paths to measure the predictive importance of our predictors? Note: If you’re curious about code to automate this visual inspection of variable importance, look at Digging Deeper Exercise 2. Exercise 4: Tuning \\(\\lambda\\) In order to pick which \\(\\lambda\\) (hence LASSO model) is “best”, we can plot CV error estimates for the different models: # Plot a summary of the performance of the different models plot(lasso_mod) Inspect the shape of the plot. The errors go down at the very beginning then start going back up. Based on this, what are the consequences of picking a \\(\\lambda\\) that is too small or too large? (This is an example of a very important idea that we’ll see shortly: the bias-variance tradeoff.) Based on visual inspection, roughly what value of \\(\\lambda\\) results in the best model? (Remind yourself of the interpretation of “best” as defined by our selectionFunction of \"oneSE\".) Identify the “best” \\(\\lambda\\). # Identify which tuning parameter (lambda) is &quot;best&quot; lasso_mod$bestTune Note: If you’re curious about making plots that show both test error estimates and their uncertainty, look at Digging Deeper Exercise 1. Exercise 5: Examining and evaluating the best LASSO model Take a look at the predictors and coefficients for the “best” LASSO model. Are the predictors that remain in the model sensible? Do the coefficient signs make sense? # Obtain the predictors and coefficients of the &quot;best&quot; model # The .&#39;s indicate that the coefficient is 0 coef(lasso_mod$finalModel, lasso_mod$bestTune$lambda) # Obtain the predictors and coefficients of LASSO model w/ different lambda # In case it&#39;s of interest to look at a slightly different model # e.g., lambda = 1 (roughly) coef(lasso_mod$finalModel, 1) Evaluate the best LASSO model: Contextually interpret (with units) the CV error for the model by inspecting lasso_mod$resample or lasso_mod$results. Make residual plots for the model by creating a dataset called lasso_mod_out which contains the original data as well as predicted values and residuals (fitted and resid). lasso_mod_out &lt;- bodyfat %&gt;% mutate( fitted = predict(lasso_mod, newdata = bodyfat), resid = BodyFat - fitted ) Digging deeper These exercises are recommended for further exploring code useful in an applied analysis. plot(lasso_mod) only shows estimates of test error but doesn’t show the uncertainty in those estimates. Use lasso_mod$results to plot both MAE and its standard deviation (MAESD). Using this ggplot2 cheat sheet might help. In Exercise 3, we used the plot of coefficient paths to evaluate the variable importance of our predictors. The code below does this systematically for each predictor so that we don’t have to eyeball. Step through and work out what each part is doing. It may help to look up function documentation with ?function_name in the Console. # Create a boolean matrix (predictors x lambdas) of variable exclusion bool_predictor_exclude &lt;- lasso_mod$finalModel$beta==0 # Loop over each variable var_imp &lt;- sapply(seq_len(nrow(bool_predictor_exclude)), function(row) { # Extract coefficient path (sorted from highest to lowest lambda) this_coeff_path &lt;- bool_predictor_exclude[row,] # Compute and return the # of lambdas until this variable is out forever ncol(bool_predictor_exclude)-which.min(this_coeff_path)+1 }) # Create a dataset of this information and sort var_imp_data &lt;- tibble( var_name = rownames(bool_predictor_exclude), var_imp = var_imp ) var_imp_data %&gt;% arrange(desc(var_imp)) "],["knn-regression-and-the-bias-variance-tradeoff.html", "Topic 6 KNN Regression and the Bias-Variance Tradeoff Learning Goals KNN models in caret Exercises", " Topic 6 KNN Regression and the Bias-Variance Tradeoff Learning Goals Clearly describe / implement by hand the KNN algorithm for making a regression prediction Explain how the number of neighbors relates to the bias-variance tradeoff Explain the difference between parametric and nonparametric methods Explain how the curse of dimensionality relates to the performance of KNN Slides from today are available here. KNN models in caret To build KNN models in caret, first load the package and set the seed for the random number generator to ensure reproducible results: library(caret) set.seed(___) # Pick your favorite number to fill in the parentheses Then adapt the following code: knn_mod &lt;- train( y ~ ., data = ___, preProcess = &quot;scale&quot;, method = &quot;knn&quot;, tuneGrid = data.frame(k = seq(___, ___, by = ___)), trControl = trainControl(method = &quot;cv&quot;, number = ___, selectionFunction = ___), metric = &quot;MAE&quot;, na.action = na.omit ) Argument Meaning y ~ . Model formula for specifying response and predictors data Sample data preProcess \"scale\" indicates that predictor variables should be scaled to have the same variance (Why might this be important? Should we always do this?) method \"knn\" implements KNN regression (and classification) tuneGrid A mini-dataset (data.frame) of tuning parameters. \\(k\\) is the KNN neighborhood size. Supply a sequence as seq(begin, end, by = size of step). trControl Use cross-validation to estimate test performance for each model fit. The process used to pick a final model from among these is indicated by selectionFunction, with options including \"best\" and \"oneSE\". metric Evaluate and compare competing models with respect to their CV-MAE. na.action Set na.action = na.omit to prevent errors if the data has missing values. Note: When including categorical predictors, caret automatically creates the corresponding indicator variables to allow Euclidean distance to still be used. You’ll think about the pros/cons of this in the exercises. An alternative to creating indicator variables is to use Gower distance. We’ll explore Gower distance more when we talk about clustering. Identifying the “best” KNN model The “best” model in the sequence of models fit is defined relative to the chosen selectionFunction and metric. # Plot CV-estimated test performance versus the tuning parameter plot(knn_mod) # CV metrics for each model knn_mod$results # Identify which tuning parameter is &quot;best&quot; knn_mod$bestTune # Get information from all CV iterations for the &quot;best&quot; model knn_mod$resample # Use the best model to make predictions # newdata should be a data.frame with required predictors predict(knn_mod, newdata = ___) Exercises You can download a template RMarkdown file to start from here. We’ll explore KNN regression using the College dataset in the ISLR package (install it with install.packages(\"caret\") in the Console). You can use ?College in the Console to look at the data codebook. library(caret) library(ggplot2) library(dplyr) library(readr) library(ISLR) data(College) # A little data cleaning college_clean &lt;- College %&gt;% mutate(school = rownames(College)) %&gt;% filter(Grad.Rate &lt;= 100) # Remove one school with grad rate of 118% rownames(college_clean) &lt;- NULL # Remove school names as row names Hello, how are things? We’re about a week and a half into our last module of the year - how are you feeling? What’s on your mind? Exercise 1: Bias-variance tradeoff warmup Think back to the LASSO algorithm which depends upon tuning parameter \\(\\lambda\\). For which values of \\(\\lambda\\) (small or large) will LASSO be the most biased, and why? For which values of \\(\\lambda\\) (small or large) will LASSO be the most variable, and why? The bias-variance tradeoff also comes into play when comparing across algorithms, not just within algorithms. Consider LASSO vs. least squares: Which will tend to be more biased? Which will tend to be more variable? When will LASSO outperform least squares in the bias-variance tradeoff? Exercise 2: Impact of distance metric Consider the 1-nearest neighbor algorithm to predict Grad.Rate on the basis of two predictors: Apps and Private. Let Yes for Private be represented with the value 1 and No with 0. We have a test case whose number of applications is 13,530 and is a private school. Suppose that we have the tiny 2-case training set below. What would the 1-nearest neighbor prediction be using Euclidean distance? college_clean %&gt;% filter(school %in% c(&quot;Princeton University&quot;, &quot;SUNY at Albany&quot;)) %&gt;% select(Apps, Private, Grad.Rate, school) Do you have any concerns about the resulting prediction? Based on this, comment on the impact of the distance metric chosen on KNN performance. How might you change the distance calculation (or correspondingly rescale the data) to generate a more sensible prediction in this situation? Exercise 3: Implementing KNN in caret Adapt our general KNN code to “fit” a set of KNN models with the following specifications: Use the predictors Private, Top10perc (% of new students from top 10% of high school class), and S.F.Ratio (student/faculty ratio). Use 8-fold CV. (Why 8? Take a look at the sample size.) Use mean absolute error (MAE) to select a final model. Select the simplest model for which the metric is within one standard error of the best metric. Use a sequence of \\(K\\) values from 1 to 100 in increments of 5. Should you use preProcess = \"scale\"? After adapting the code (but before inspecting any output), answer the following conceptual questions: Explain your choice for using or not using preProcess = \"scale\". Why is “fit” in quotes? Does KNN actually fit a model as part of training? (This feature of KNN is known as “lazy learning”.) How is test MAE estimated? What are the steps of the KNN algorithm with cross-validation? Draw a picture of how you expect test MAE to vary with \\(K\\). In terms of the bias-variance tradeoff, why do you expect the plot to look this way? set.seed(2021) knn_mod &lt;- train( ) Exercise 4: Inspecting the results Use plot(knn_mod) to verify your expectations about the plot of test MAE vs. \\(K\\). Contextually interpret the test MAE. How else could you evaluate the KNN model? Does your KNN model help you understand which predictors of graduation rate are most important? Why or why not? Exercise 5: Curse of dimensionality Just as with parametric models, we could keep going and add more and more predictors. However, the KNN algorithm is known to suffer from the “curse of dimensionality”. Why? Hint: First do a quick Google search of this new idea. "],["splines.html", "Topic 7 Splines Learning Goals Splines in caret Exercises", " Topic 7 Splines Learning Goals Explain the advantages of splines over global transformations and other types of piecewise polynomials Explain how splines are constructed by drawing connections to variable transformations and least squares Explain how the number of knots relates to the bias-variance tradeoff Slides from today are available here. Splines in caret To build models with splines in caret, we proceed with the same structure for train() as we use for ordinary linear regression models. (Why can we just use least squares?) Refer to code from Topic 3: Overfitting &amp; CV for a refresher. To work with splines, we’ll use the splines package. The ns() function in that package creates the transformations needed to create a spline function for a quantitative predictor. This involves a small update to the formula: # Before: all linear terms ls_mod &lt;- train( y ~ quant_x1 + quant_x2, ... ) # With splines ls_mod &lt;- train( y ~ ns(quant_x1, df = 3) + ns(quant_x2, df = 3), ... ) The df argument in ns() stands for degrees of freedom: df = # knots + 1 The degrees of freedom are the number of coefficients in the transformation functions that are free to vary (essentially the number of underlying parameters behind the transformations). Exercises You can download a template RMarkdown file to start from here. Before proceeding, install the splines package by entering install.packages(\"splines\") in the Console. We’ll continue using the College dataset in the ISLR package to explore splines. You can use ?College in the Console to look at the data codebook. library(caret) library(ggplot2) library(dplyr) library(readr) library(ISLR) library(splines) data(College) # A little data cleaning college_clean &lt;- College %&gt;% mutate(school = rownames(College)) %&gt;% filter(Grad.Rate &lt;= 100) # Remove one school with grad rate of 118% rownames(college_clean) &lt;- NULL # Remove school names as row names Exercise 1: Evaluating a fully linear model We will model Grad.Rate as a function of 4 predictors: Private, Terminal, Expend, and S.F.Ratio. Make scatterplots with 2 different smoothing lines to explore potential nonlinearity. Adding the following to the normal scatterplot code will create a smooth (curved) blue trend line and a red linear trend line. geom_smooth(color = &quot;blue&quot;, se = FALSE) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, se = FALSE) Use caret to fit an ordinary linear regression model (no splines yet) with the following specifications: Use 8-fold CV. Use mean absolute error (MAE) to select a final model. Select the simplest model for which the metric is within one standard error of the best metric. set.seed(___) ls_mod &lt;- train( ) Make plots of the residuals vs. the 3 quantitative predictors to evaluate the appropriateness of linear terms. ls_mod_data &lt;- college_clean %&gt;% mutate( pred = predict(ls_mod, newdata = college_clean), resid = ___ ) ggplot(ls_mod_data, ???) + ___ + ___ + geom_hline(yintercept = 0, color = &quot;red&quot;) Exercise 2: Evaluating a spline model We’ll extend our linear regression model with spline functions of the quantitative predictors (leave Private as is). What tuning parameter is associated with splines? How do high/low values of this parameter relate to bias and variance? Update your code from Exercise 1 to model the 3 quantitative predictors with natural splines that have 2 knots (= 3 degrees of freedom). set.seed(___) spline_mod &lt;- train( ) Make plots of the residuals vs. the 3 quantitative predictors to evaluate if splines improved the model. spline_mod_data &lt;- ___ # Residual plots Extra! Variable scaling What is your intuition about whether variable scaling matters for the performance of splines? Check you intuition by reusing code from Exercise 2, except with preProcess = \"scale\" inside train(). Call this spline_mod2. How do the predictions from spline_mod and spline_mod2 compare? You could use a plot to compare or check out the all.equal() function. "],["local-regression-gams.html", "Topic 8 Local Regression &amp; GAMs Learning Goals GAMs in caret Exercises", " Topic 8 Local Regression &amp; GAMs Learning Goals Clearly describe the local regression algorithm for making a prediction Explain how bandwidth (span) relate to the bias-variance tradeoff Describe some different formulations for a GAM (how the arbitrary functions are represented) Explain how to make a prediction from a GAM Interpret the output from a GAM Slides from today are available here. GAMs in caret To build GAMs in caret, first load the package and set the seed for the random number generator to ensure reproducible results: library(caret) set.seed(___) # Pick your favorite number to fill in the parentheses Then adapt the following code: gam_mod &lt;- train( y ~ x, data = ___, method = &quot;gamLoess&quot;, tuneGrid = data.frame(degree = 1, span = seq(___, ___, by = ___)), trControl = trainControl(method = &quot;cv&quot;, number = ___, selectionFunction = ___), metric = &quot;MAE&quot;, na.action = na.omit ) Argument Meaning y ~ x Model formula for specifying response and predictors data Sample data method \"gamLoess\" builds GAMs with LOESS components tuneGrid A mini-dataset (data.frame) of tuning parameters. degree is the degree of the local polynomial fit (1 = linear is just fine). span is the fraction of data used in the local fit: supply a sequence as seq(begin, end, by = size of step). trControl Use cross-validation to estimate test performance for each model fit. The process used to pick a final model from among these is indicated by selectionFunction, with options including \"best\" and \"oneSE\". metric Evaluate and compare competing models with respect to their CV-MAE. na.action Set na.action = na.omit to prevent errors if the data has missing values. Identifying the “best” GAM The “best” model in the sequence of models fit is defined relative to the chosen selectionFunction and metric. # Plot CV-estimated test performance versus the tuning parameter plot(gam_mod) # CV metrics for each model gam_mod$results # Identify which tuning parameter is &quot;best&quot; gam_mod$bestTune # Get information from all CV iterations for the &quot;best&quot; model gam_mod$resample # Use the best model to make predictions # newdata should be a data.frame with required predictors predict(gam_mod, newdata = ___) Inspecting the “best” GAM # Plot functions for each predictor # Dashed lines are +/- 2 SEs plot(gam_mod$finalModel, se = TRUE) # Plot functions for each predictor in case the functions are splines with plot.Gam library(splines) gam_mod_spline &lt;- lm( Grad.Rate ~ ns(quant_x1,df) + ns(quant_x2,df) + ..., data = ___ ) plot.Gam(gam_mod_spline, se = TRUE) Exercises You can download a template RMarkdown file to start from here. Before proceeding, install the gam package by entering install.packages(\"gam\") in the Console. We’ll continue using the College dataset in the ISLR package to explore splines. You can use ?College in the Console to look at the data codebook. library(caret) library(ggplot2) library(dplyr) library(ISLR) library(splines) library(gam) data(College) # A little data cleaning college_clean &lt;- College %&gt;% mutate(school = rownames(College)) %&gt;% filter(Grad.Rate &lt;= 100) # Remove one school with grad rate of 118% rownames(college_clean) &lt;- NULL # Remove school names as row names Exercise 1: Conceptual warmup Do you think that at GAM with all possible predictors will have better or worse performance than an ordinary (fully linear) least squares model with all possible predictors? Explain your thoughts. How does high/low span relate to bias and variance of a LOESS model? How should we choose predictors to be in a GAM? How could forward and backward stepwise selection and LASSO help with variable selection before a GAM? Exercise 2: Building a GAM in caret Suppose that our initial variable selection investigations lead us to using the predictors indicated below in our GAM. Fit a GAM with the following specifications: Use 8-fold CV. Select the model which has the lowest MAE. (Hint: options are “oneSE” or “best”). Use the sequence of span values: 0.1, 0.2, …, 0.9. What do you expect that the plot of test MAE versus span will look like, and why? set.seed(___) gam_mod &lt;- train( Grad.Rate ~ Private + Apps + Top10perc + Top25perc + P.Undergrad + Outstate + Room.Board + Books + Personal + PhD + perc.alumni, ___ ) Exercise 3: Identifying the “best” GAM The code below has been common to all of our methods below, so it is provided for convenience. Inspect the output to identify the “best” span for our GAM. (Was your prediction from Exercise 2 about the plot correct?) Contextually interpret the CV MAE with units. # Plot CV-estimated test performance versus the tuning parameter plot(gam_mod) # Identify which tuning parameter is &quot;best&quot; gam_mod$bestTune # CV metrics for each model gam_mod$results # CV metrics for just the &quot;best&quot; model gam_mod$results %&gt;% filter(span==gam_mod$bestTune$span) Exercise 4: Interpreting the GAM We can plot the function for each predictor as below. par(mfrow = c(3,4)) # Sets up a grid of plots plot(gam_mod$finalModel, se = TRUE) # Dashed lines are +/- 2 SEs What about these plots indicates that using GAM instead of ordinary linear regression was probably a good choice? Pick 1 or 2 of these plots, and interpret your findings. Anything surprising or interesting? The PrivateYes plot might look odd. Not to worry - the GAM is treating this as a categorical (indicator) variable. What do you learn from this plot? In case you find it useful, you can also build a GAM using spline components with lm() and plot the nonlinear functions for each predictor with plot.Gam() from the gam package. library(splines) gam_mod_spline &lt;- lm( Grad.Rate ~ Private + ns(Apps,3) + ns(Top10perc,3) + ns(Top25perc,3) + ns(P.Undergrad,3) + ns(Outstate,3) + ns(Room.Board,3) + ns(Books,3) + ns(Personal,3) + ns(PhD,3) + ns(perc.alumni,3), data = college_clean ) par(mfrow = c(3,4)) plot.Gam(gam_mod_spline, se = TRUE) Exercise 5: Comparison of methods Brainstorm the pros/cons of the different methods that we’ve explored. You may find it helpful to refer to the portfolio themes for each method. (Soon, as part of the Portfolio, you’ll be doing a similar synthesis of our regression unit, so this brainstorming session might help!) Just for fun! In case you want a (silly!) take on the curse of dimensionality, check out this video. (“Relevant” parts are from 0:28 - 4:16.) "],["logistic-regression.html", "Topic 9 Logistic Regression Learning Goals Logistic regression in caret Exercises", " Topic 9 Logistic Regression Learning Goals Use a logistic regression model to make hard (class) and soft (probability) predictions Interpret non-intercept coefficients from logistic regression models in the data context Slides from today are available here. Logistic regression in caret To build logistic regression models in caret, first load the package and set the seed for the random number generator to ensure reproducible results: library(caret) set.seed(___) # Pick your favorite number to fill in the parentheses Then adapt the following code: logistic_mod &lt;- train( y ~ x, data = ___, method = &quot;glm&quot;, family = &quot;binomial&quot;, trControl = trainControl(method = &quot;cv&quot;, number = ___), metric = &quot;Accuracy&quot;, na.action = na.omit ) Argument Meaning y ~ x y must be a character or factor data Sample data method &amp; family The glm method implements various “generalized” linear models. When we specify family = \"binomial\", glm performs logistic regression. (BTW: glm with family = \"gaussian\" is equivalent to using lm!) trControl Use cross-validation to estimate test performance for each model fit. metric Evaluate and compare competing models with respect to their CV-Accuracy. na.action Set na.action = na.omit to prevent errors if the data has missing values. Examining the logistic model # Model summary table summary(logistic_mod) # Coefficients coef(logistic_mod$finalModel) # Exponentiated coefficients coef(logistic_mod$finalModel) %&gt;% exp() # CV accuracy metrics logistic_mod$results logistic_mod$resample Making predictions from the logistic model # Make soft (probability) predictions predict(logistic_mod, newdata = ___, type = &quot;prob&quot;) # Make hard (class) predictions (using a default 0.5 probability threshold) predict(logistic_mod, newdata = ___, type = &quot;raw&quot;) Exercises You can download a template RMarkdown file to start from here. Context Before proceeding, install the e1071 package (utilities for evaluating classification models) by entering install.packages(\"e1071\") in the Console. We’ll be working with a spam dataset that contains information on different features of emails and whether or not the email was spam. The variables are as follows: spam: Either spam or not spam word_freq_WORD: percentage of words in the e-mail that match WORD (0-100) char_freq_CHAR: percentage of characters in the e-mail that match CHAR (e.g., exclamation points, dollar signs) capital_run_length_average: average length of uninterrupted sequences of capital letters capital_run_length_longest: length of longest uninterrupted sequence of capital letters capital_run_length_total: sum of length of uninterrupted sequences of capital letters Our goal will be to use email features to predict whether or not an email is spam - essentially, to build a spam filter! library(readr) library(ggplot2) library(dplyr) library(caret) spam &lt;- read_csv(&quot;https://www.dropbox.com/s/leurr6a30f4l32a/spambase.csv?dl=1&quot;) Hello, how are things? We’re nearing the halfway point of the module - how are you holding up? Exercise 1: Visualization warmup Let’s take a look at the frequency of the word “George” (the email recipient’s name is George) (word_freq_george) and the frequency of exclamation points (char_freq_exclam). Create appropriate visualizations to assess the predictive ability of these two predictors. # If you want to adjust the axis limits, you can add the following to your plot: # + coord_cartesian(ylim = c(0,1)) # + coord_cartesian(xlim = c(0,1)) ggplot(spam, aes(x = ___, y = ___)) + geom_???() Exercise 2: Implementing logistic regression in caret Our goal is to fit a logistic regression model with word_freq_george and char_freq_exclam as predictors. Write down the corresponding logistic regression model formula using general notation. Use caret to fit this logistic regression model. Use 10-fold CV to estimate test accuracy. (We’ll focus more on interpreting model evaluation metrics next time.) Note: If you get warning (not error) messages like fitted probabilities numerically 0 or 1 occurred, this means that in some of the CV iterations, one or more of the predictors perfectly predicted spam classification. set.seed(___) logistic_mod &lt;- train( ) Exercise 3: Interpreting the model Take a look at the log-scale coefficients with summary(logistic_mod). Do the signs of the coefficients for the 2 predictors agree with your visual inspection from Exercise 1? Display the exponentiated coefficients, and provide contextual interpretations for them (not the intercept). Exercise 4: Making predictions Consider a new email where the frequency of “George” is 0.25% and the frequency of exclamation points is 1%. Use the model summary to make both a soft (probability) and hard (class) prediction for this test case by hand. Use a default probability threshold of 0.5. (You can use math expressions to use R as a calculator. The exp() function exponentiates a number.) Check your work from part a by using predict(). predict(___, newdata = data.frame(word_freq_george = 0.25, char_freq_exclam = 1), ___) "],["evaluating-classification-models.html", "Topic 10 Evaluating Classification Models Learning Goals LASSO for logistic regression in caret Exercises", " Topic 10 Evaluating Classification Models Learning Goals Calculate (by hand from confusion matrices) and contextually interpret overall accuracy, sensitivity, and specificity Construct and interpret plots of predicted probabilities across classes Explain how a ROC curve is constructed and the rationale behind AUC as an evaluation metric Appropriately use and interpret the no-information rate to evaluate accuracy metrics Slides from today are available here. LASSO for logistic regression in caret To build LASSO models for logistic regression in caret, first load the package and set the seed for the random number generator to ensure reproducible results: library(caret) set.seed(___) # Pick your favorite number to fill in the parentheses If we would like to use estimates of test (overall) accuracy to choose our final model (based on a probability threshold of 0.5), we can adapt the following: lasso_logistic_mod &lt;- train( y ~ x, data = ___, method = &quot;glmnet&quot;, family = &quot;binomial&quot;, tuneGrid = data.frame(alpha = 1, lambda = seq(___, ___, length.out = ___)), trControl = trainControl(method = &quot;cv&quot;, number = ___, selectionFunction = ___), metric = &quot;Accuracy&quot;, na.action = na.omit ) Argument Meaning y ~ x y must be a character or factor data Sample data method &amp; family The glm method implements various “generalized” linear models. When we specify family = \"binomial\", glm performs logistic regression. trControl Use cross-validation to estimate test performance for each model fit. selectionFunction can be \"best\" or \"oneSE\" as before. tuneGrid Tuning parameters for LASSO: alpha = 1 indicates LASSO (as opposed to another regularization method). Specify a sequence of lambda values for the penalty. metric Evaluate and compare competing models with respect to their CV-Accuracy. (Uses a default probability threshold of 0.5 to make hard predictions.) na.action Set na.action = na.omit to prevent errors if the data has missing values. If we would like to choose our final model based on estimates of test sensitivity, specificity, or AUC, we can adapt the following: lasso_logistic_mod &lt;- train( y ~ x, data = ___, method = &quot;glmnet&quot;, family = &quot;binomial&quot;, tuneGrid = data.frame(alpha = 1, lambda = seq(___, ___, length.out = ___)), trControl = trainControl(method = &quot;cv&quot;, number = ___, selectionFunction = ___, classProbs = TRUE, summaryFunction = twoClassSummaryCustom), metric = &quot;AUC&quot;, na.action = na.omit ) The two new arguments to trainControl() are as follows: classProbs: Set to true if soft (probability) predictions should be computed summaryFunction: Use twoClassSummaryCustom in order to compute overall accuracy, sensitivity, and specificity (based on a threshold of 0.5) and to compute AUC The metric now has 4 options: \"AUC\": Compute AUC \"Sens\": Compute sensitivity \"Spec\": Compute specificity \"Accuracy\": Compute overall accuracy You’ll need to run the code below to create the twoClassSummaryCustom function (don’t worry about how this is written): twoClassSummaryCustom &lt;- function (data, lev = NULL, model = NULL) { if (length(lev) &gt; 2) { stop(paste(&quot;Your outcome has&quot;, length(lev), &quot;levels. The twoClassSummary() function isn&#39;t appropriate.&quot;)) } caret:::requireNamespaceQuietStop(&quot;pROC&quot;) if (!all(levels(data[, &quot;pred&quot;]) == lev)) { stop(&quot;levels of observed and predicted data do not match&quot;) } rocObject &lt;- try(pROC::roc(data$obs, data[, lev[1]], direction = &quot;&gt;&quot;, quiet = TRUE), silent = TRUE) rocAUC &lt;- if (inherits(rocObject, &quot;try-error&quot;)) NA else rocObject$auc out &lt;- c(rocAUC, sensitivity(data[, &quot;pred&quot;], data[, &quot;obs&quot;], lev[1]), specificity(data[, &quot;pred&quot;], data[, &quot;obs&quot;], lev[2])) out2 &lt;- postResample(data[, &quot;pred&quot;], data[, &quot;obs&quot;]) out &lt;- c(out, out2[1]) names(out) &lt;- c(&quot;AUC&quot;, &quot;Sens&quot;, &quot;Spec&quot;, &quot;Accuracy&quot;) out } Exercises You can download a template RMarkdown file to start from here. Context Before proceeding, install the pROC package (utilities for evaluating classification models with ROC curves) by entering install.packages(\"pROC\") in the Console. We’ll continue working with the spam dataset from last time. spam: Either spam or not spam (outcome) word_freq_WORD: percentage of words in the e-mail that match WORD (0-100) char_freq_CHAR: percentage of characters in the e-mail that match CHAR (e.g., exclamation points, dollar signs) capital_run_length_average: average length of uninterrupted sequences of capital letters capital_run_length_longest: length of longest uninterrupted sequence of capital letters capital_run_length_total: sum of length of uninterrupted sequences of capital letters Our goal will be to use email features to predict whether or not an email is spam - essentially, to build a spam filter! library(readr) library(ggplot2) library(dplyr) library(caret) spam &lt;- read_csv(&quot;https://www.dropbox.com/s/leurr6a30f4l32a/spambase.csv?dl=1&quot;) # A little data cleaning to remove the space in &quot;not spam&quot; spam &lt;- spam %&gt;% mutate(spam = ifelse(spam==&quot;spam&quot;, &quot;spam&quot;, &quot;not_spam&quot;)) Exercise 1: Conceptual warmup LASSO for the logistic regression setting works analogously to the regression setting. How would you expect a plot of test accuracy vs. \\(\\lambda\\) to look, and why? (Draw it!) Exercise 2: Implementing LASSO logistic regression in caret Fit a LASSO logistic regression model for the spam outcome, and allow all possible predictors to be considered (~ . in the model formula). Use 10-fold CV. Choose a final model whose test AUC is within one standard error of the overall best metric. Initially try a sequence of 100 \\(\\lambda\\)’s from 0 to 10. Diagnose whether this sequence should be updated by looking at the plot of test AUC vs. \\(\\lambda\\) (plot(lasso_logistic_mod)). If needed, adjust the max value in the sequence up or down by a factor of 10. (You’ll be able to determine from the plot whether to adjust up or down.) set.seed(___) lasso_logistic_mod &lt;- train( ) Exercise 3: Inspecting the model Inspect the $bestTune part of your fitted lasso_logistic_mod in conjunction with the plot of test AUC vs. \\(\\lambda\\). Is anything surprising about the results relative to your expectations from Exercise 1? Brainstorm some possible explanations in consideration of the data context. Exercise 4: Interpreting evaluation metrics Inspect the overall CV results for the “best” \\(\\lambda\\), and compute the no-information rate (NIR): # CV results for &quot;best lambda&quot; lasso_logistic_mod$results %&gt;% filter(lambda==lasso_logistic_mod$bestTune$lambda) # Count up number of spam and not_spam emails in the training data spam %&gt;% count(spam) # Name of the outcome variable goes inside count() # Compute the NIR Interpret the estimates of test sensitivity and specificity - what do these numbers mean? Do you think higher sensitivity or specificity would be more important in designing a spam filter? Interpret overall accuracy - does this seem high? How can the no-information rate (NIR) help us interpret the overall accuracy? Why is an AUC of 1 the best possible value for this metric? How does the AUC for our spam model look relative to this best value? Exercise 5: Algorithmic understanding for evaluation metrics Inspect the iteration specific information from CV for the “best” \\(\\lambda\\): lasso_logistic_mod$resample How is one row of information computed? Carefully describe the CV process for a single iteration to estimate each of AUC, Sens, Spec, and Accuracy (overall accuracy). Use a generic confusion matrix (filled with variables instead of hard numbers) to illustrate the underlying computations. "],["trees-part-1.html", "Topic 11 Trees (Part 1) Learning Goals Trees in caret Exercises", " Topic 11 Trees (Part 1) Learning Goals Clearly describe the recursive binary splitting algorithm for tree building for both regression and classification Compute the weighted average Gini index to measure the quality of a classification tree split Compute the sum of squared residuals to measure the quality of a regression tree split Explain how recursive binary splitting is a greedy algorithm Explain how different tree parameters relate to the bias-variance tradeoff Slides from today are available here. Trees in caret To build tree models in caret, first load the package and set the seed for the random number generator to ensure reproducible results: library(caret) set.seed(___) # Pick your favorite number to fill in the parentheses If we would like to use estimates of test (overall) accuracy to choose our final model (based on a hard predictions resulting from majority classification), we can adapt the following: tree_mod &lt;- train( y ~ x, data = ___, method = &quot;rpart&quot;, tuneGrid = data.frame(cp = seq(___, ___, length.out = ___)), trControl = trainControl(method = &quot;cv&quot;, number = ___, selectionFunction = ___), metric = &quot;Accuracy&quot;, na.action = na.omit ) Argument Meaning y ~ x y must be a character or factor data Sample data method \"rpart\" builds trees (through recursive partitioning) trControl Use cross-validation to estimate test performance for each model fit. selectionFunction can be \"best\" or \"oneSE\", as before. tuneGrid The cp “complexity” tuning parameter indicates the degree to which a new split must improve purity. We can stop splitting branches if they don’t improve purity by at least cp. metric Evaluate and compare competing models with respect to their CV-Accuracy. na.action Set na.action = na.omit to prevent errors if the data has missing values. If we would like to choose our final model based on estimates of test sensitivity, specificity, or AUC, we can amend the arguments of trainControl() as described in Evaluating Classification Models. Visualizing and interpreting the “best” tree # Plot the tree (make sure to load the rpart.plot package first) rpart.plot(tree_mod$finalModel) # Get variable importance metrics tree_mod$finalModel$variable.importance Exercises You can download a template RMarkdown file to start from here. Context Before proceeding, install the rpart and rpart.plot packages (for building and plotting decision trees) by entering install.packages(c(\"rpart\", \"rpart.plot\")) in the Console. Our goal will be to classify types of urban land cover in small subregions within a high resolution aerial image of a land region. Data from the UCI Machine Learning Repository include the observed type of land cover (determined by human eye) and “spectral, size, shape, and texture information” computed from the image. See this page for the data codebook. Source: https://ncap.org.uk/sites/default/files/EK_land_use_0.jpg library(readr) library(ggplot2) library(dplyr) library(caret) library(rpart.plot) # Read in the data land &lt;- read_csv(&quot;https://www.macalester.edu/~ajohns24/data/land_cover.csv&quot;) # There are 9 land types, but we&#39;ll focus on 3 of them land &lt;- land %&gt;% filter(class %in% c(&quot;asphalt&quot;, &quot;grass&quot;, &quot;tree&quot;)) Exercise 1: Core theme: parametric/nonparametric What does it mean for a method to be nonparametric? In general, when might we prefer nonparametric to parametric methods? When might we not? Where do you think trees fall on the parametric/nonparametric spectrum? Exercise 2: Core theme: Tuning parameters and the BVT The key feature governing complexity of a tree model is the number of splits used in the tree. How is the number of splits related to model complexity, bias, and variance? In practice, the number of splits is controlled indirectly through the following tuning parameters. For each, discuss how low/high parameter settings would affect the number of tree splits. minsplit: the minimum number of observations that must exist in a node in order for a split to be attempted. minbucket: the minimum number of observations in any leaf node. cp: complexity parameter. Any split that does not increase node purity by cp is not attempted. maxdepth: Set the maximum depth of any node of the final tree. The depth of a node is the number of branches that need to be followed to get to a given node from the root node. (The root node has depth 0.) Exercise 3: Building trees in caret Fit a tree model for the class outcome (land type), and allow all possible predictors to be considered (~ . in the model formula). Use 10-fold CV. Choose a final model whose test overall accuracy is within one standard error of the overall best metric. The Gini index impurity measure can be a minimum of zero and has an upper bound of 1. Thus, we’ll try a sequence of 50 cp values from 0 to 0.5. Make a plot of test performance versus the cp tuning parameter. Does it look as you expected? set.seed(___) tree_mod &lt;- train( ) Exercise 4: Visualizing trees Let’s visualize the difference between the trees learned under cp parameters. The code below fits a tree for a lower than optimal cp value (tree_mod_lowcp) and a higher than optimal cp (tree_mod_highcp). We then plot these trees (1st and 3rd) along with our best tree (2nd). Look at page 3 of the rpart.plot package vignette (an example-heavy manual) to understand what the plot components mean. tree_mod_lowcp &lt;- train( class ~ ., data = land, method = &quot;rpart&quot;, tuneGrid = data.frame(cp = 0), trControl = trainControl(method = &quot;cv&quot;, number = 10), metric = &quot;Accuracy&quot;, na.action = na.omit ) tree_mod_highcp &lt;- train( class ~ ., data = land, method = &quot;rpart&quot;, tuneGrid = data.frame(cp = 0.1), trControl = trainControl(method = &quot;cv&quot;, number = 10), metric = &quot;Accuracy&quot;, na.action = na.omit ) # Plot all 3 trees in a row par(mfrow = c(1,3)) rpart.plot(tree_mod_lowcp$finalModel) rpart.plot(tree_mod$finalModel) rpart.plot(tree_mod_highcp$finalModel) Verify for a couple of splits the idea of increasing node purity/homogeneity in tree-building. (How is this idea reflected in the numbers in the plot output?) Tuning classification trees (like with the cp parameter) is also referred to as “pruning”. Why does this make sense? NOTE: If “pruning” is a new word to you, first Google it. "],["trees-part-2.html", "Topic 12 Trees (Part 2) Exercises", " Topic 12 Trees (Part 2) Exercises You can download a template RMarkdown file to start from here. Context Our goal will be to classify types of urban land cover in small subregions within a high resolution aerial image of a land region. Data from the UCI Machine Learning Repository include the observed type of land cover (determined by human eye) and “spectral, size, shape, and texture information” computed from the image. See this page for the data codebook. Source: https://ncap.org.uk/sites/default/files/EK_land_use_0.jpg library(readr) library(ggplot2) library(dplyr) library(caret) library(rpart.plot) # Read in the data land &lt;- read_csv(&quot;https://www.macalester.edu/~ajohns24/data/land_cover.csv&quot;) # There are 9 land types, but we&#39;ll focus on 3 of them land &lt;- land %&gt;% filter(class %in% c(&quot;asphalt&quot;, &quot;grass&quot;, &quot;tree&quot;)) Exercise 1: Predictions from trees Last time, we built a classification tree to predict land type of an image patch (class) from the spectral, size, shape, and texture predictors in the dataset. Looking at the plot of the fitted tree, manually make a soft (probability) and hard (class) prediction for the case shown below. (See page 3 of the rpart.plot package vignette for a refresher on what the plot shows.) set.seed(186) tree_mod &lt;- train( class ~ ., data = land, method = &quot;rpart&quot;, tuneGrid = data.frame(cp = seq(0, 0.5, length.out = 50)), trControl = trainControl(method = &quot;cv&quot;, number = 10, selectionFunction = &quot;oneSE&quot;), metric = &quot;Accuracy&quot;, na.action = na.omit ) rpart.plot(tree_mod$finalModel) # Pick out training case 2 to make a prediction test_case &lt;- land[2,] # Show only the needed predictors test_case %&gt;% select(NDVI, Bright_100, SD_NIR) Verify your predictions with the predict() function. (Note: we introduced this code in Logistic Regression, but this type of code applies to any classification model fit in caret). # Soft (probability) prediction predict(tree_mod, newdata = test_case, type = &quot;prob&quot;) # Hard (class) prediction predict(tree_mod, newdata = test_case, type = &quot;raw&quot;) Exercise 2: Reinforcing the BVT Last time, we looked at a number of different tuning parameters that impact the number of splits in a tree. Let’s focus on minbucket described below. minbucket: the minimum number of observations in any leaf node. How would you expect a plot of test accuracy (how is test accuracy estimated again?) vs. minbucket to look, and why? (Draw it!) What part of the plot corresponds to overfitting? To underfitting? How would you expect a plot of training accuracy vs. minbucket to look, and why? (Draw it!) What part of the plot corresponds to overfitting? To underfitting? Exercise 3: Variable importance in trees We can obtain numerical variable importance measures from trees. These measure, roughly, “the total decrease in node impurities from splitting on the variable” (even if the variable isn’t ultimately used in the split). What are the 3 most important predictors by this measure? Does this agree with you might have expected based on the plot of the fitted tree in Exercise 1? What might greedy behavior have to do with this? tree_mod$finalModel$variable.importance Exercise 4: Regression trees As discussed in the video, trees can also be used for regression! Let’s work through a step of building a regression tree by hand. For the two possible splits below, determine the better split for the tree by computing the sum of squared residuals as the measure of node impurity. (The numbers following Yes: and No: indicate the outcome value of the cases in the left (Yes) and right (No) regions.) Split 1: x1 &lt; 3 - Yes: 1, 1, 2, 4 - No: 2, 2, 4, 4 Split 2: x1 &lt; 4 - Yes: 1, 1, 2 - No: 2, 2, 4, 4, 4 Extra! In case you want to explore building regression trees in R, try out the following exercises using the College data from the ISLR package. Our goal was to predict graduation rate (Grad.Rate) as a function of other predictors. You can look at the data codebook with ?College in the Console. library(ISLR) data(College) # A little data cleaning college_clean &lt;- College %&gt;% mutate(school = rownames(College)) %&gt;% filter(Grad.Rate &lt;= 100) # Remove one school with grad rate of 118% rownames(college_clean) &lt;- NULL # Remove school names as row names Adapt our general decision tree code for the regression setting by adapting the metric used to pick the final model. (Note how other parts stay the same!) Note about tuning the cp parameter: rpart uses the R-squared metric to prune branches. (The R-squared metric must increase by cp at each step for a split to be considered.) set.seed(132) tree_mod_college &lt;- train( Grad.Rate ~ ., data = college_clean, method = &quot;rpart&quot;, tuneGrid = data.frame(cp = seq(0, 0.2, length = 50)), trControl = trainControl(method = &quot;cv&quot;, number = 10, selectionFunction = &quot;oneSE&quot;), metric = ___, na.action = na.omit ) Plot test performance as a function of cp, and comment on the shape of the plot. Plot the “best” tree. (See page 3 of the rpart.plot package vignette for a refresher on what the plot shows.) Do the sequence of splits and outcomes in the leaf nodes make sense? Look at the variable importance metrics from the best tree. Do the most important variables align with your intuition? "],["bagging-and-random-forests.html", "Topic 13 Bagging and Random Forests Learning Goals Exercises", " Topic 13 Bagging and Random Forests Learning Goals Explain the rationale for bagging Explain the rationale for selecting a random subset of predictors at each split (random forests) Explain how the size of the random subset of predictors at each split relates to the bias-variance tradeoff Explain the rationale for and implement out-of-bag error estimation for both regression and classification Explain the rationale behind the random forest variable importance measure and why it is biased towards quantitative predictors Slides from today are available here. Exercises You can download a template RMarkdown file to start from here. Before proceeding, install the randomForest package by entering install.packages(\"randomForest\") in the Console. Our goal will be to classify types of urban land cover in small subregions within a high resolution aerial image of a land region. Data from the UCI Machine Learning Repository include the observed type of land cover (determined by human eye) and “spectral, size, shape, and texture information” computed from the image. See this page for the data codebook. Source: https://ncap.org.uk/sites/default/files/EK_land_use_0.jpg library(readr) library(ggplot2) library(dplyr) library(caret) library(rpart.plot) library(randomForest) # Read in the data land &lt;- read_csv(&quot;https://www.macalester.edu/~ajohns24/data/land_cover.csv&quot;) # There are 9 land types, but we&#39;ll focus on 3 of them land &lt;- land %&gt;% filter(class %in% c(&quot;asphalt&quot;, &quot;grass&quot;, &quot;tree&quot;)) Hello, how are things? It’s been a hard week. How are all of you doing? Please check in with each other. Exercise 1: Preparing to build a random forest You’ll eventually use the train() function from caret package to build a random forest for the classification model of class ~ .. In this exercise, you’ll take some preliminary steps. Visit caret’s (extensive) manual and search for “random forest” in the top right search bar. There will be many results. Find the entry that only has randomForest listed in the “Libraries” column, and inspect the information in that row. (The instructor is not familiar with the other packages.) What method will we use for random forests (e.g., for single trees, we used method = \"rpart\")? What is the tuning parameter called, and what does it represent? To investigate this, pull up the help file for the randomForest() function in the randomForest package by entering ?randomForest::randomForest in the Console. Exercise 2: More preparation to build a random forest Suppose we wanted to evaluate the performance of a random forest which uses 500 classification trees. Describe the 10-fold CV approach to evaluating the random forest. In this process, how many total trees would we need to construct? The out-of-bag (OOB) error rate provides an alternative approach to evaluating forests. Unlike CV, OOB summarizes misclassification rates when applying each of the 500 trees to the “test” cases that were not used to build the tree. How many total trees would we need to construct in order to calculate the OOB error estimate? Moving forward, we’ll use OOB and not CV to evaluate forest performance. Explain why. Look at the trainControl() documentation by entering ?caret::trainControl in the Console. What is the name of the method to perform OOB error estimation? Exercise 3: Building the random forest We can now put together our work from the previous 2 exercises to train our random forest model. Using train() code for previous methods as a guide, build a set of random forest models with the following specifications: Set the seed to 253. Run the algorithm with the following number of randomly sampled predictors at each split: 2, 12 (roughly \\(\\sqrt{147}\\)), 74 (roughly 147/2), and all 147 predictors You can generate a sequence of numbers with c(). e.g., c(2,3). Use \"oob\" instead of \"cv\" for model evaluation. Hint: The number argument is not necessary. (Why?) Select the model with the overall best value of estimated test overall accuracy. Note: By default, 500 trees are built. rf_mod &lt;- train( ) Exercise 4: Preliminary interpretation Plot estimated test performance vs. the tuning parameter with plot(rf_mod). What tuning parameter would you choose? Describe the bias-variance tradeoff in tuning this forest. For what values of the tuning parameter will forests be the most biased? The most variable? Exercise 5: Evaluating the forest The code below prints information pertaining to the “best” forest model. rf_mod$finalModel Report and interpret the OOB estimate of error rate. (How does this match up with the plot from the previous exercise?) The output includes an OOB test confusion matrix (as opposed to a training confusion matrix). Rows are true classes, and columns are predicted classes. How do you think this is constructed? Why is the test confusion matrix preferable to a training confusion matrix? Further inspecting the test confusion matrix, which type of land use is most accurately classified by our forest? Which type of land use is least accurately classified by our forest? Why do you think this is? In our previous activities, our best tree had a cross-validated accuracy rate of around 85%. How does the forest performance compare? Exercise 6: Variable importance measures Because bagging and random forests use tons of trees, the nice interpretability of single decision trees is lost. However, we can still get a measure of how important the different predictors were in this classification task. For each of the 147 predictors, the code below gives the “total decrease in node impurities (as measured by the Gini index) from splitting on the variable, averaged over all trees” (package documentation). var_imp_rf &lt;- randomForest::importance(rf_mod$finalModel) # Sort by importance with dplyr&#39;s arrange() var_imp_rf &lt;- data.frame( predictor = rownames(var_imp_rf), MeanDecreaseGini = var_imp_rf[,&quot;MeanDecreaseGini&quot;] ) %&gt;% arrange(desc(MeanDecreaseGini)) # Top 20 head(var_imp_rf, 20) # Bottom 10 tail(var_imp_rf, 10) Check out the codebook for these variables here. The descriptions of the variables aren’t the greatest, but does this ranking make some contextual sense? Construct some visualizations of the 1 most and 1 least important predictors that support your conclusion in a. It has been found that this random forest measure of variable importance can tend to favor predictors with a lot of unique values. Explain briefly why it makes sense that this can happen by thinking about the recursive binary splitting algorithm for a single tree. (Note: similar cautions arise for variable importance in single trees.) "],["homework-1.html", "Homework 1 Project Work Portfolio Work Course Engagement", " Homework 1 Due Friday, March 26 at midnight CST on Moodle Please turn in a single PDF document containing (1) your responses for the Project Work and Course Engagement sections and (2) a LINK to the Google Doc with your responses for the Portfolio Work section. Project Work Goal: Find a dataset (or datasets) to use for your final project, and start to get to know the data. Details: Your dataset(s) should allow you to perform a (1) regression, (2) classification, and (3) unsupervised learning analysis. The following resources are good places to start looking for data: Google Dataset Search Kaggle UCI Machine Learning Repository Harvard Dataverse Even if you end up working with a partner on the project (which isn’t required - working alone is fine), please complete this initial work individually. It’s fine if you and a potential/future partner end up using the same dataset and collaborate on the finding of data, but complete the short bit of writing (below) individually. Check in with the instructor early if you need help. Deliverables: Write 1-2 paragraphs (no more than 350 words) summarizing: The information in the dataset(s) and the context behind the data. Use the prompts below to guide your thoughts. (Note: in some situations, there may be incomplete information on the data context. That’s fine. Just do your best to summarize what information is available, and acknowledge the lack of information where relevant.) What are the cases? Broadly describe the variables contained in the data. Who collected the data? When, why, and how? 3 research questions 1 that can be investigated in a regression setting 1 that can be investigated in a classification setting 1 that can be investigated in an unsupervised learning setting Also make sure that you can read the data into R. You don’t need to do any analysis in R yet, but making sure that you can read the data will make the next steps go more smoothly. Portfolio Work Page maximum: 2 pages of text (pictures don’t count) Organization: Your choice! Use titles and section headings that make sense to you. (It probably makes sense to have a separate section for each method.) Deliverables: Put your responses for this part in a Google Doc, and update the link sharing so that anyone with the link at Macalester College can edit. Include the URL for the Google Doc in your submission. Note: Some prompts below may seem very open-ended. This is intentional. Crafting good responses requires looking back through our material to organize the concepts in a coherent, thematic way, which is extremely useful for your learning. Concepts to address: Overfitting: The video used the analogy of a cat picture model to explain overfitting. Come up with your own analogy to explain overfitting. Evaluating regression models: Describe how residuals are central to the evaluation of regression models. Explain how they arise in quantitative evaluation metrics and how they are used in evaluation plots. Include examples of plots that show desirable and undesirable model behavior (feel free to draw them by hand if you wish) and what steps can be taken to address that undesirable behavior. Cross-validation: In your own words, explain the rationale for cross-validation in relation to overfitting and model evaluation. Describe the algorithm in your own words in at most 2 sentences. Subset selection: Algorithmic understanding: Look at Conceptual exercise 1, parts (a) and (b) in ISLR Section 6.8. What are the aspects of the subset selection algorithm(s) that are essential to answering these questions, and why? (Note: you’ll have to try to answer the ISLR questions to respond to this prompt, but the focus of your writing should be on the question in bold here.) Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? Computational time: What computational time considerations are relevant for this method (how long the algorithms take to run)? Interpretation of output: What parts of the algorithm output have useful interpretations, and what are those interpretations? Focus on output that allows us to measure variable importance. How do the algorithms/output allow us to learn about variable importance? LASSO: Algorithmic understanding: Come up with your own analogy for explaining how the penalized least squares criterion works. Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? Computational time: What computational time considerations are relevant for this method (how long the algorithms take to run)? Interpretation of output: What parts of the algorithm output have useful interpretations, and what are those interpretations? Focus on output that allows us to measure variable importance. How do the algorithms/output allow us to learn about variable importance? Course Engagement The Ethics component below is the only required piece. The others are optional depending on the modes of engagement you wish to pursue consistently throughout the module. Ethics: (REQUIRED) Read the article Amazon scraps secret AI recruiting tool that showed bias against women. Write a short (roughly 250 words), thoughtful response about the themes and cautions that the article brings forth. Reflection: Write a short, thoughtful reflection about how things went this week. Feel free to use whichever prompts below resonate most with you, but don’t feel limited to these prompts. How is your understanding of the material? What ideas/topics have stuck out for you? How is group work going? Any strategies for improving collaboration that you want to try out next week? How is your work/life balance going? Any new activities or strategies that you want to try out for next week? Note-taking: Share link(s) to file(s) where you keep your notes on videos/readings and on code from class. Q &amp; A: In one short paragraph, summarize your engagement in at least 2 of the 3 following areas: (1) preceptor / instructor office hours, (2) on Slack, (3) in small groups during synchronous class sessions. "],["homework-2.html", "Homework 2 Project Work Portfolio Work Course Engagement", " Homework 2 Due Friday, April 2 at midnight CST on Moodle Deliverables: Please use this template to knit an HTML document. Convert this HTML document to a PDF by opening the HTML document in your web browser. Print the document (Ctrl/Cmd-P) and change the destination to “Save as PDF”. Submit this one PDF to Moodle. Alternatively, you may knit your Rmd directly to PDF if you have LaTeX installed. If you don’t have time to work on Project Work, work on it for HW3. I won’t include a Project Work section on HW3 (only Portfolio + CE). Project Work Goal: Begin an analysis of your dataset to answer your regression research question. Collaboration: If you have already formed a team (of at most 3 members) for the project, this part can be done as a team. Only one team member should submit a Project Work section. Grading: Completing this Project Work section is a required part of the final project. Rather than receiving a grade for the analyses below, you will get qualitative feedback from the instructor. It is the synthesis of the analyses across homework assignments that will determine the final project grade. Data cleaning: If your dataset requires any cleaning (e.g., merging datasets, creation of new variables), first consult the R Resources page to see if your questions are answered there. If not, post on the #content-questions channel in our Slack workspace to ask for help. Please ask for help early and regularly to avoid stressful workloads. Required Analyses: Initial investigation: ignoring nonlinearity (for now) Use ordinary least squares (OLS) regression, forward and/or backward selection, and LASSO to build initial models for your quantitative outcome as a function of the predictors of interest. (As part of data cleaning, exclude any variables that you don’t want to consider as predictors.) These models should not include any transformations to deal with nonlinearity. You’ll explore this in the next investigation. Note: If you have highly collinear/redundant variables, you might see the message “Reordering variables and trying again” and associated warning()s about linear dependencies being found. Sometimes stepwise selection is able to handle the collinearity/redundancy by modifying the order of the variables tried. If collinearity/redundancy cannot be handled and causes an error, try reducing nvmax. Estimate test performance of the models from these different methods. Report and interpret (with units) these estimates along with a measure of uncertainty in the estimate (SD is most readily available from caret). Compare estimated test performance across methods. Which method(s) might you prefer? Use residual plots to evaluate whether some quantitative predictors might be better modeled with nonlinear relationships. Compare insights from variable importance analyses from the different methods (stepwise and LASSO, but not OLS). Are there variables for which the methods reach consensus? What insights are expected? Surprising? Note that if some (but not all) of the indicator terms for a categorical predictor are selected in the final models, the whole predictor should be treated as selected. Accounting for nonlinearity Update your stepwise selection model(s) and LASSO model to use natural splines for the quantitative predictors. You’ll need to update the model formula from y ~ . to something like y ~ cat_var1 + ns(quant_var1, df) + .... It’s recommended to use few knots (e.g., 2 knots = 3 degrees of freedom). Note that ns(x,3) replaces x with 3 transformations of x. Keep this in mind when setting nvmax in stepwise selection. Compare insights from variable importance analyses here and the corresponding results from Investigation 1. Now after having accounted for nonlinearity, have the most relevant predictors changed? Note that if some (but not all) of the spline terms are selected in the final models, the whole predictor should be treated as selected. Fit a GAM using LOESS terms using the set of variables deemed to be most relevant based on your investigations so far. How does test performance of the GAM compare to other models you explored? Do you gain any insights from the GAM output plots for each predictor? Don’t worry about KNN for now. Summarize investigations Decide on an overall best model based on your investigations so far. To do this, make clear your analysis goals. Predictive accuracy? Interpretability? A combination of both? Societal impact Are there any harms that may come from your analyses and/or how the data were collected? What cautions do you want to keep in mind when communicating your work? Portfolio Work Length requirements: Detailed for each section below. Organization: To help the instructor and preceptors grade, please organize your document as shown in this example. Clear section headers and new pages for each method help a lot. Thank you! Deliverables: Continue writing your responses in the same Google Doc that you set up for Homework 1. Include the URL for the Google Doc in your submission. Note: Some prompts below may seem very open-ended. This is intentional. Crafting good responses requires looking back through our material to organize the concepts in a coherent, thematic way, which is extremely useful for your learning. Revisions: Make any revisions desired to previous concepts. Important formatting note: Please use a comment to mark the text that you want to be reread. (Highlight each span of text you want to be reread, and mark it with the comment “REVISION”.) Rubrics to past homeworks will be available on Moodle (under the Solutions section). Look at these rubrics to guide your revisions. You can always ask for guidance in office hours as well. New concepts to address: The following prompts are shared for all methods: Bias-variance tradeoff: What tuning parameters control the performance of the method? How do low/high values of the tuning parameters relate to bias and variance of the learned model? (3 sentences max.) Parametric / nonparametric: Where (roughly) does this method fall on the parametric-nonparametric spectrum, and why? (3 sentences max.) Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? (3 sentences max.) Subset selection: Bias-variance tradeoff Parametric / nonparametric LASSO: Bias-variance tradeoff Parametric / nonparametric KNN: Algorithmic understanding: Draw and annotate pictures that show how the KNN (K = 2) regression algorithm would work for a test case in a 2 quantitative predictor setting. Also explain how the curse of dimensionality affects KNN performance. (5 sentences max.) Bias-variance tradeoff Parametric / nonparametric Scaling of variables Computational time: The KNN algorithm is often called a “lazy” learner. Discuss how this relates to the model training process and the computations that must be performed when predicting on a new test case. (3 sentences max.) Interpretation of output: The “lazy” learner feature of KNN in relation to model training affects the interpretability of output. How? (3 sentences max.) Splines: Algorithmic understanding: Explain the advantages of natural cubic splines over global transformations and piecewise polynomials. Also explain the connection between splines and the ordinary (least squares) regression framework. (5 sentences max.) Bias-variance tradeoff Parametric / nonparametric Scaling of variables Computational time: When using splines, how does computation time compare to fitting ordinary (least squares) regression models? (1 sentence) Interpretation of output: SKIP - will be covered in the GAMs section Local regression: Algorithmic understanding: Consider the R functions lm(), predict(), dist(), and dplyr::filter(). (Look up the documentation for unfamiliar functions in the Help pane of RStudio.) In what order would these functions need to be used in order to make a local regression prediction for a supplied test case? Explain. (5 sentences max.) Bias-variance tradeoff Parametric / nonparametric Scaling of variables Computational time: In general, local regression is very fast, but how would you expect computation time to vary with span? Explain. (3 sentences max.) Interpretation of output: SKIP - will be covered in the GAMs section GAMs: Algorithmic understanding: How do linear regression, splines, and local regression each relate to GAMs? Why would we want to model with GAMs? (5 sentences max.) Bias-variance tradeoff Parametric / nonparametric Scaling of variables Computational time: How a GAM is specified affects the time required to fit the model - why? (3 sentences max.) Interpretation of output: How does the interpretation of ordinary regression coefficients compare to the interpretation of GAM output? (3 sentences max.) Course Engagement The Ethics component below is the only required piece. The others are optional depending on the modes of engagement you wish to pursue consistently throughout the module. Ethics: (REQUIRED) Read the article Automated background checks are deciding who’s fit for a home. Write a short (roughly 250 words), thoughtful response about the ideas that the article brings forth. What themes recur from last week’s article (on an old Amazon recruiting tool)? What aspects are more particular to the context of equity in housing access? Reflection: Write a short, thoughtful reflection about how things went this week. Feel free to use whichever prompts below resonate most with you, but don’t feel limited to these prompts. How are class-related things going? Is there anything that you need from the instructor? What new strategies for watching videos, reading, reviewing, gaining insights from class work have you tried or would like to try? How is group work going? Did you try out any new collaboration strategies with your new group? How did they go? How is your work/life balance going? Did you try out any new activities or strategies for staying well? How did they go? Note-taking: Share link(s) to file(s) where you keep your notes on videos/readings and on code from class. Q &amp; A: In one short paragraph, summarize your engagement in at least 2 of the 3 following areas: (1) preceptor / instructor office hours, (2) on Slack, (3) in small groups during synchronous class sessions. "],["homework-3.html", "Homework 3 Project Work Portfolio Work Course Engagement", " Homework 3 Due Friday, April 9 at midnight CST on Moodle Deliverables: Please use this template to knit an HTML document. Convert this HTML document to a PDF by opening the HTML document in your web browser. Print the document (Ctrl/Cmd-P) and change the destination to “Save as PDF”. Submit this one PDF to Moodle. Alternatively, you may knit your Rmd directly to PDF if you have LaTeX installed. Project Work (Note: This is a repeat of the Homework 2 investigations.) Goal: Begin an analysis of your dataset to answer your regression research question. Collaboration: If you have already formed a team (of at most 3 members) for the project, this part can be done as a team. Only one team member should submit a Project Work section. Grading: Completing this Project Work section is a required part of the final project. Rather than receiving a grade for the analyses below, you will get qualitative feedback from the instructor. It is the synthesis of the analyses across homework assignments that will determine the final project grade. Data cleaning: If your dataset requires any cleaning (e.g., merging datasets, creation of new variables), first consult the R Resources page to see if your questions are answered there. If not, post on the #rcode-questions channel in our Slack workspace to ask for help. Please ask for help early and regularly to avoid stressful workloads. Required Analyses: Initial investigation: ignoring nonlinearity (for now) Use ordinary least squares (OLS) regression, forward and/or backward selection, and LASSO to build initial models for your quantitative outcome as a function of the predictors of interest. (As part of data cleaning, exclude any variables that you don’t want to consider as predictors.) These models should not include any transformations to deal with nonlinearity. You’ll explore this in the next investigation. Note: If you have highly collinear/redundant variables, you might see the message “Reordering variables and trying again” and associated warning()s about linear dependencies being found. Sometimes stepwise selection is able to handle the collinearity/redundancy by modifying the order of the variables tried. If collinearity/redundancy cannot be handled and causes an error, try reducing nvmax. Estimate test performance of the models from these different methods. Report and interpret (with units) these estimates along with a measure of uncertainty in the estimate (SD is most readily available from caret). Compare estimated test performance across methods. Which method(s) might you prefer? Use residual plots to evaluate whether some quantitative predictors might be better modeled with nonlinear relationships. Compare insights from variable importance analyses from the different methods (stepwise and LASSO, but not OLS). Are there variables for which the methods reach consensus? What insights are expected? Surprising? Note that if some (but not all) of the indicator terms for a categorical predictor are selected in the final models, the whole predictor should be treated as selected. Accounting for nonlinearity Update your stepwise selection model(s) and LASSO model to use natural splines for the quantitative predictors. You’ll need to update the model formula from y ~ . to something like y ~ cat_var1 + ns(quant_var1, df) + .... It’s recommended to use few knots (e.g., 2 knots = 3 degrees of freedom). Note that ns(x,3) replaces x with 3 transformations of x. Keep this in mind when setting nvmax in stepwise selection. Compare insights from variable importance analyses here and the corresponding results from Investigation 1. Now after having accounted for nonlinearity, have the most relevant predictors changed? Note that if some (but not all) of the spline terms are selected in the final models, the whole predictor should be treated as selected. Fit a GAM using LOESS terms using the set of variables deemed to be most relevant based on your investigations so far. How does test performance of the GAM compare to other models you explored? Do you gain any insights from the GAM output plots for each predictor? Don’t worry about KNN for now. Summarize investigations Decide on an overall best model based on your investigations so far. To do this, make clear your analysis goals. Predictive accuracy? Interpretability? A combination of both? Societal impact Are there any harms that may come from your analyses and/or how the data were collected? What cautions do you want to keep in mind when communicating your work? Portfolio Work Length requirements: Detailed for each section below. Organization: To help the instructor and preceptors grade, please organize your document as shown in this example. Clear section headers and new pages for each method help a lot. Thank you! Deliverables: Continue writing your responses in the same Google Doc that you set up for Homework 1. We have recorded the URL from past HWs - you don’t need to supply it again. Note: Some prompts below may seem very open-ended. This is intentional. Crafting good responses requires looking back through our material to organize the concepts in a coherent, thematic way, which is extremely useful for your learning. Revisions: Make any revisions desired to previous concepts. Important formatting note: Please use a comment to mark the text that you want to be reread. (Highlight each span of text you want to be reread, and mark it with the comment “REVISION”.) Rubrics to past homeworks will be available on Moodle (under the Solutions section). Look at these rubrics to guide your revisions. You can always ask for guidance in office hours as well. New concepts to address: Evaluating classification models: Consider this xkcd comic. Write a paragraph (around 250 words) that addresses the following questions. Craft this pargraph so it flows nicely and does not read like a disconnected list of answers. (Include transitions between sentences.) What is the classification model here? How do the ideas in this comic emphasize comparisons between overall accuracy and class-specific accuracy measures? What are the names of the relevant class-specific accuracy measures here, and what are there values? How does this comic connect to the no-information rate? Logistic regression: Algorithmic understanding: Write your own example of a logistic regression model formula. (Don’t use the example from the video.) Using this example, show how to use the model to make both a soft and a hard prediction. Bias-variance tradeoff: What tuning parameters control the performance of the method? How do low/high values of the tuning parameters relate to bias and variance of the learned model? (3 sentences max.) Parametric / nonparametric: Where (roughly) does this method fall on the parametric-nonparametric spectrum, and why? (3 sentences max.) Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? (3 sentences max.) Computational time: SKIP Interpretation of output: In general, how can the coefficient for a quantitative predictor be interpreted? How can the coefficient for a categorical predictor (an indicator variable) be interpreted? Course Engagement The Ethics component below is the only required piece. The others are optional depending on the modes of engagement you wish to pursue consistently throughout the module. Ethics: (REQUIRED) Read the article Getting Past Identity to What You Really Want. Write a short (roughly 250 words), thoughtful response about the ideas that the article brings forth. What skills do you think are essential for the leaders and data analysts of organizations to have to handle these issues with care? Reflection: Write a short, thoughtful reflection about how things went this week. Feel free to use whichever prompts below resonate most with you, but don’t feel limited to these prompts. What’s going well? In school, work, other areas? What do you think would help sustain the things that are going well? What’s not going well? In school, work, other areas? What do you think would help improve the things that aren’t going as well? Anything that the instructor can do? Note-taking: Share link(s) to file(s) where you keep your notes on videos/readings and on code from class. Q &amp; A: In one short paragraph, summarize your engagement in at least 2 of the 3 following areas: (1) preceptor / instructor office hours, (2) on Slack, (3) in small groups during synchronous class sessions. "],["homework-4.html", "Homework 4 Project Work Portfolio Work Course Engagement", " Homework 4 Due Friday, April 16 at midnight CST on Moodle Deliverables: Submit a single PDF containing your responses for Course Engagement. Project Work No new project work this week. Portfolio Work Length requirements: Detailed for each section below. Organization: To help the instructor and preceptors grade, please organize your document as shown in this example. Clear section headers and new pages for each method help a lot. Thank you! Deliverables: Continue writing your responses in the same Google Doc that you set up for Homework 1. We have recorded the URL from past HWs - you don’t need to supply it again. Note: Some prompts below may seem very open-ended. This is intentional. Crafting good responses requires looking back through our material to organize the concepts in a coherent, thematic way, which is extremely useful for your learning. Revisions: Make any revisions desired to previous concepts. Important formatting note: Please use a comment to mark the text that you want to be reread. (Highlight each span of text you want to be reread, and mark it with the comment “REVISION”.) Rubrics to past homeworks will be available on Moodle (under the Solutions section). Look at these rubrics to guide your revisions. You can always ask for guidance in office hours as well. New concepts to address: Decision trees: Algorithmic understanding: Consider a dataset with two predictors: x1 is categorical with levels A, B, or C. x2 is quantitative with integer values from 1 to 100. How many different splits must be considered when recursive binary splitting attempts to make a split? Explain. (2 sentences max.) Explain the “recursive”, “binary”, and “splitting” parts of the recursive binary splitting algorithm. Make sure to discuss the concept of node (im)purity and how it is measured for classification and regression trees. Bias-variance tradeoff: What tuning parameters control the performance of the method? How do low/high values of the tuning parameters relate to bias and variance of the learned model? (3 sentences max.) Note: There are several specific named tuning parameters as part of the rpart method. Don’t focus on these. Just discuss the broader feature of trees that these specific parameters affect. Parametric / nonparametric: Where (roughly) does this method fall on the parametric-nonparametric spectrum, and why? (3 sentences max.) Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? (3 sentences max.) Computational time: Recursive binary splitting does not find the overall optimal sequence of splits for a tree. What type of behavior is this? What method have we seen before that also exhibits this type of behavior? Briefly explain the parallels between these methods and what implications this have for computational time. (5 sentences max.) Interpretation of output: Explain the rationale behind the variable importance measures that decision trees provide. (4 sentences max.) Bagging &amp; Random Forests: Algorithmic understanding: Explain the rationale for extending single decision trees to bagging models and then to random forest models. What specific improvements to predictive performance are being sought? (5 sentences max.) Bias-variance tradeoff: What tuning parameters control the performance of the method? How do low/high values of the tuning parameters relate to bias and variance of the learned model? (3 sentences max.) Parametric / nonparametric: Where (roughly) does this method fall on the parametric-nonparametric spectrum, and why? (3 sentences max.) Scaling of variables: Does the scale on which variables are measured matter for the performance of this algorithm? Why or why not? If scale does matter, how should this be addressed when using this method? (3 sentences max.) Computational time: Explain why cross-validation is computationally intensive for many-tree algorithms. What method do we have to reduce this computational burden, and why is it faster? (5 sentences max.) Interpretation of output: Explain the rationale behind the variable importance measures that random forest models provide. (4 sentences max.) Course Engagement The Ethics component below is the only required piece. The others are optional depending on the modes of engagement you wish to pursue consistently throughout the module. Ethics: (REQUIRED) Read the article How to Support your Data Interpretations. Write a short (roughly 250 words), thoughtful response about the ideas that the article brings forth. Which pillar(s) do you think is/are hardest to do well for groups that rely on data analytics, and why? Reflection: Write a short, thoughtful reflection about how things went this week. Feel free to use whichever prompts below resonate most with you, but don’t feel limited to these prompts. How are class-related things going? Is there anything that you need from the instructor? How is your work/life balance going? What do you want to make time for this next week? Note-taking: Share link(s) to file(s) where you keep your notes on videos/readings and on code from class. Q &amp; A: In one short paragraph, summarize your engagement in at least 2 of the 3 following areas: (1) preceptor / instructor office hours, (2) on Slack, (3) in small groups during synchronous class sessions. "],["final-project.html", "Final Project Requirements", " Final Project Requirements You will be analyzing a dataset using a regression, classification, and unsupervised learning analysis. You must complete the Project Work parts of the 4 weekly homework assignments. These are incremental investigations that build from our applied class exercises. You will synthesize these investigations at the end of the module. Collaboration: You may work in teams of up to 3 members. Individual work is fine. The weekly homework assignments will note whether work for that week should be submitted individually or if just one team member should submit work. There will be a required synthesis of the weekly homework investigations at the end of the course (e.g., a final report and/or presentation). If working on a team, this should be done in groups, rather than individually. Still being determined. Updates will appear on this page later in the course. "],["r-resources.html", "R Resources Outside resources Example code", " R Resources Outside resources Lisa Lendway’s COMP/STAT 112 website (with code examples and videos) RStudio cheat sheets ggplot2 reference R Programming Wikibook Debugging in R Article Video Colors in R Data import tutorial Free online textbooks R for Data Science Exploratory Data Analysis with R Example code Creating new variables case_when() from the dplyr package is a very versatile function for creating new variables based on existing variables. This can be useful for creating categorical or quantitative variables and for creating indices from multiple variables. # Turn quant_var into a Low/Med/High version data &lt;- data %&gt;% mutate(cat_var = case_when( quant_var &lt; 10 ~ &quot;Low&quot;, quant_var &gt;= 10 &amp; quant_var &lt;= 20 ~ &quot;Med&quot;, quant_var &gt; 20 ~ &quot;High&quot; ) ) # Turn cat_var (A, B, C categories) into another categorical variable # (collapse A and B into one category) data &lt;- data %&gt;% mutate(new_cat_var = case_when( cat_var %in% c(&quot;A&quot;, &quot;B&quot;) ~ &quot;A or B&quot; cat_var==&quot;C&quot; ~ &quot;C&quot; ) ) # Turn a categorical variable (x1) encoded as a numerical 0/1/2 variable into a different quantitative variable # Doing this for multiple variables allows you to create an index data &lt;- data %&gt;% mutate(x1_score = case_when( x1==0 ~ 10, x1==1 ~ 20, x1==2 ~ 50 ) ) # Add together multiple variables with mutate data &lt;- data %&gt;% mutate(index = x1_score + x2_score + x3_score) "]]
